{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbrMqMPyncTC"
   },
   "source": [
    "# **Tarea 1 - CC6205 Natural Language Processing üìö**\n",
    "\n",
    "**Integrantes:**\n",
    "\n",
    "**Fecha l√≠mite de entrega üìÜ:** Lunes 29 de Marzo.\n",
    "\n",
    "**Tiempo estimado de dedicaci√≥n:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1HFX-9PpxF9"
   },
   "source": [
    "` ` \n",
    "\n",
    "\n",
    "Bienvenid@s a la primera tarea en el curso de Natural Language Processing (NLP). Esta tarea tiene como objetivo evaluar los contenidos te√≥ricos de las primeras semanas de clases, enfocado principalmente en ***Information Retrieval (IR)*** y ***Vector Space Models***. Si a√∫n no han visto las clases, se recomienda visitar los links de las referencias.\n",
    "\n",
    "La tarea consta de una parte te√≥rica que busca evaluar conceptos vistos en clases. Seguido por una parte pr√°ctica con el f√≠n de introducirlos a la programaci√≥n en Python enfocada en NLP. \n",
    "\n",
    "` ` \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Instrucciones:**\n",
    "- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
    "- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n",
    "- El formato de entrega es este mismo Jupyter Notebook.\n",
    "- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n. \n",
    "- Est√° **PROHIBIDO** usar cualquier librer√≠a que implemente los algoritmos pedidos (Spacy, scikit, etc). S√≥lo se podr√°n utilizar las librer√≠as importadas al inicio de la secci√≥n de pr√°ctica.\n",
    "- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav√©s del canal de Discord del curso. \n",
    "\n",
    "\n",
    "\n",
    "Ahora s√≠, empecemos! üòÑüöÄ\n",
    "\n",
    "` ` \n",
    "\n",
    "**Referencias:**\n",
    "\n",
    "\n",
    "Slides:\n",
    "    \n",
    "- [Introducci√≥n al curso](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-introduction.pdf)\n",
    "- [Vector Space Model / Information Retrieval](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-IR.pdf)    \n",
    "\n",
    "Videos: \n",
    "\n",
    "- [CC6205 - Procesamiento de Lenguaje Natural: CC6205 - Procesamiento de Lenguaje Natural: Introducci√≥n parte I](https://www.youtube.com/watch?v=HEKTNOttGvU)\n",
    "- [CC6205 - Procesamiento de Lenguaje Natural: CC6205 - Procesamiento de Lenguaje Natural: Introducci√≥n parte II](https://www.youtube.com/watch?v=P8cwnI-f-Kg)\n",
    "- [CC6205 - Procesamiento de Lenguaje Natural: Vector Space Model and Information Retrieval parte 1](https://youtu.be/FXIVClF370w)\n",
    "- [CC6205 - Procesamiento de Lenguaje Natural: Vector Space Model and Information Retrieval parte 2](https://youtu.be/f8nG1EMmPZk)\n",
    "\n",
    "` ` \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJshpe1yrKJr"
   },
   "source": [
    "## **1 - Preguntas te√≥ricas üìï (2 puntos).** ##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBEDKXBPoA7w"
   },
   "source": [
    "Las siguientes celdas contienen preguntas acerca del contenido visto en clases y en el material del curso.  Contestar cada pregunta en su celda correspondiente y **no extenderse m√°s de 5 lineas** . üôè\n",
    "` ` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNJPR1kMrw9R"
   },
   "source": [
    "**Pregunta 1 (0.2 puntos): ¬øPor qu√© el an√°lisis del lenguaje humano es una tarea compleja? Mencione dos razones.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlTBYHEptdde"
   },
   "source": [
    "> El an√°lisis del lenguaje humano es complejo porque el lenguaje humano es **ambiguo**: por ejemplo, las oraciones *com√≠ empanadas con pebre* y *com√≠ empanadas con Luis* son formalmente muy similares, ambas solo se diferencian en las palabras *pebre* y *Luis*, sin embargo, tienen significados completamente distintos; es decir, dichas oraciones son sint√°cticamente muy similares pero sem√°nticamente distintas. Lo anterior conlleva a que analizar el lenguaje humano es m√°s complejo que solamente estudiar *formalmente* las palabras y oraciones . Asimismo, relacionado con lo anterior, el an√°lisis del lenguaje humano tambi√©n es complejo porque el lenguaje **depende del contexto**: la misma palabra puede tener un significado completamente distinto dependiendo del lugar, tiempo y/o grupo humano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zv8yjG94r6FC"
   },
   "source": [
    "**Pregunta 2 (0.3 puntos): ¬øCu√°l es la diferencia entre Natural Language Processing (NLP) y Computational Linguistics?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vmC3uYHtctH"
   },
   "source": [
    "> Natural Language Processing es una disciplina que se encarga de resolver tareas particulares que involucran el lenguaje humano y, en ese sentido, en NLP se buscan m√©todos y algoritmos que *solucionen* problemas que involucren lenguaje humano. Por el otro lado, Computational Linguistics es una disciplina que estudia la *linguistica* mediante la ayuda de herramientas computacionales y, en ese sentido, en la disciplina Computational Linguistics el objeto de estudio principal es el lenguaje en s√≠ mismo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-0imOaFsRK5"
   },
   "source": [
    "**Pregunta 3 (0.3 puntos): ¬øQu√© es el Machine Learning supersivado? Donde podr√≠a aplicar este tipo de Aprendizaje**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kY6mfCE7tb1x"
   },
   "source": [
    "> Machine Learning supervisado es sub-√°rea de la disciplina de Machine Learning que se define por utilizar datasets *etiquetados* para entrenar algoritmos para clasificar data o predecir *outputs* de manera precisa. Por ejemplo, en NLP se pueden clasificar textos utilizando textos pre-etiquetados: sup√≥ngase que se tiene un conjunto de textos etiquetados en relaci√≥n al tipo de texto (textos pol√≠ticos, textos deportivos y textos cient√≠ficos); dado dichos textos, se puede utilizar alg√∫n algoritmo de Machine Learning supervisado para que, dado un texto nuevo no etiquteado, se etiquete dicho texto en alguna de las categor√≠as de los textos iniciales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVMXilrYsiSZ"
   },
   "source": [
    "**Pregunta 4 (0.3 puntos): ¬øCu√°les son las diferencias entre usar Deep learning y Machine Learning cl√°sico (empirismo) para un problema de NLP? Ejemplifique con alguna task.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QXWMzpTtBZL"
   },
   "source": [
    "> Para utlizar Machine Learning cl√°sico en NLP se necesita establecer una representaci√≥n particular para la data, es decir, dado un texto, se necesita predefinir la *mejor* forma de representar dicho texto. Por el otro lado, en Deep learning no se necesita establecer una representaci√≥n particular para la data, el algoritmo encuentra por s√≠ solo la mejor representaci√≥n para los datos. Por ejemplo, en el caso de clasificar tweets seg√∫n sentimiento, en un enfoque cl√°sico por Machine Learning se tiene que establecer de qu√© forma se va a representar el tweet. Se puede crear un vector que contenga informaci√≥n relevante sobre un el tweet: una columna puede ser el n√∫mero de negaciones individuales del tweet, otra columna puede ser el n√∫mero de palabras en may√∫sculas, etc√©tera. Not√©se que existen muchas representaciones y definir cu√°l es la mejor es una tarea compleja. En cambio, en el caso de Deep learning, el algoritmo mismo ser√° el que escoga qu√© elementos del tweet son los m√°s relevante para definir su *sentimiento*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXc4XuVG7Loa"
   },
   "source": [
    " **Pregunta 5 (0.3 puntos) Seg√∫n las primeras clases, ¬øQu√© m√©todo cl√°sico nos permite rankear las similitudes existentes entre documentos?, ¬øC√≥mo son las representaciones que genera y problemas que podr√≠an experimentar estas soluciones simples?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtaQ_aVE8LhH"
   },
   "source": [
    "> tf-idf scoring model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKNDNy047tcY"
   },
   "source": [
    "**Pregunta 6 (0.3 puntos) Usted se encuentra realizando un modelo de clasificaci√≥n de sentimientos con texto, su jefe le se√±ala que debe eliminar las palabras mas comunes para obtener una mejor clasificaci√≥n. ¬øQu√© palabras le se√±ala que elimine su jefe?, ¬øes acaso esto una buena idea?.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRe4QaGS8MqY"
   },
   "source": [
    "> La idea del jefe podr√≠a ser correcta ya que, en general, en un texto las palabras m√°s comunes (palabras como de, y, el, la, que, un, por, para) no dicen tanto sobre la sem√°ntica de dicho texto (y, en este caso, se quiere capturar la sem√°ntica del texto porque se quiere encontrar los *sentimientos* de dicho texto). Por ejemplo, en las oraciones \"*de anoche que estoy enojado*\" y \"*es inevitable que est√© feliz si paso de estar nublado a soleado*\" es claro que las palabras *enojado* y *feliz* son las palabr√°s que capturan los sentimientos de dichos textos (y las palabras *de* y *que* no entregan informaci√≥n alguna). Sin embargo, en otros casos, eliminar palabras podr√≠a ser una mala idea. Por ejemplo, en las oraciones \"*la fila del supermercado es muy peque√±a*\" y \"*el sueldo que me pagaron termin√≥ siendo bastante peque√±o*\" el sentimiento del texto no est√° tanto en una o en un par de palabras sino en el contexto del texto: la palabra *peque√±a* puede ser positiva o negativa. En dichos casos, eliminar palabras no necesariamente es una buena opci√≥n para obtener una mejor clasificaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uO-79Xth8Ajd"
   },
   "source": [
    "**Pregunta 7 (0.3 puntos) Nombre las ventajas y desventajas de utilizar un modelo Bag of Words y TFIDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PROa1Csq8OAR"
   },
   "source": [
    "> Algunos ventajas de utilizar un modelo Bag of Words y TFIDF son las siguientes:\n",
    "> - Es un modelo simple tanto en t√©rminos de espacio (el vocabulario generado es a lo m√°s del tama√±o del vocabulario de un lenguaje humano y la representaci√≥n vectorial de los textos permite almacenar dichos vectores de forma *sparse*) como en t√©rminos de complejidad (crear el diccionario es proporcionalmente lineal al tama√±o de todos los textos y las operaciones sobre las representaciones vectoriales de los textos son proporcionales al tama√±o del vocabulario).\n",
    "> - Permite representar de forma simple los textos y, por tanto, las *operaciones* entre dos textos es simple. Por ejemplo, dada dos representaciones de textos, encontrar qu√© t√©rminos se hayan en ambos textos es una operaci√≥n sumamente simple.\n",
    "> - Relacionado con el punto anterior, el modelo permite buscar t√©rminos en textos de forma simple, es decir, responder a la pregunta si un t√©rmino se encuentra o no en un texto en particular es una tarea f√°cil.\n",
    ">\n",
    ">\n",
    "> Por el otro lado, algunas desventajas:\n",
    "> - Cuando se representan un texto como vector y se almacena la informaci√≥n respecto a qu√© palabras se hayan en dicho texto, se pierde la informaci√≥n *espacial* de dicho texto, es decir,la representaci√≥n vectorial no contiene informaci√≥n sobre *d√≥nde* se encontraban las palabras presentes en el texto.\n",
    "> Es *explotable* en el sentido de que se puede enga√±ar al modelo  usando t√©rminos que se refieren a una palabra pero no de forma expl√≠cita. Por ejemplo, si se quisiera que la palabra *pol√≠tica* no sea identifica en el texto, se podr√≠a cambiar dicha palabra por las \"P O L I T I C A\", \"p0l1t1c4\", \"politika\", entre otras infinitas variaciones de dicha palabra. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fpsz2pQt8x5"
   },
   "source": [
    "## **2 - Preguntas pr√°cticas üíª (4 puntos).** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YB92kQXspvbR"
   },
   "source": [
    "Esta segunda secci√≥n incluye ejercicios de programaci√≥n ü§ô. Leer atentamente las instrucciones entregadas a continuaci√≥n para facilitar el proceso de revisi√≥n de sus trabajos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PosWgWgRxHKp"
   },
   "source": [
    "` ` \n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "\n",
    "\n",
    "- Escribe tu c√≥digo entre las lineas de comentarios **### Aqu√≠ inicia tu c√≥digo ###** y **### Aqu√≠ termina tu c√≥digo ###**.\n",
    "- Cuando el ejercicio incluya un bloque llamado ***Test***, comprueba que el resultado de la ejecuci√≥n coincida con el resultado esperado.\n",
    "- Recuerde siempre mantener buenas pr√°cticas de c√≥digo.\n",
    "- Est√° permitido s√≥lo utilizar las librer√≠as importadas antes del Ejercicio 1.\n",
    "- **Recordar** que: *Documento = Oraci√≥n. Dataset = Corpus. Vocabulario = Typos*.\n",
    "- El **orden de los resultados** pueden variar dependiendo de su m√°quina, pero los valores de los resultados son los mismos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBrmFHXhqUww"
   },
   "source": [
    "**Ejemplo:** Implemente una funci√≥n **`hello_world()`** que imprima en pantalla `\"Hello World\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tu7cIsawyJHx"
   },
   "outputs": [],
   "source": [
    "def hello_world():\n",
    "  ### Aqu√≠ inicia tu c√≥digo ###\n",
    "  print(\"Hello World\")\n",
    "  ### Aqu√≠ termina tu c√≥digo ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fz6klw12lwbW"
   },
   "source": [
    "***Test:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ac-WMk2dyQbp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "hello_world()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIoiAMxtyUjQ"
   },
   "source": [
    "***Resultado esperado***: \n",
    "<table>\n",
    "    <tr> \n",
    "        <td> Hello World </td> \n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "``\n",
    "``\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlPyrPXiH0l4"
   },
   "source": [
    "Estas son las librer√≠as permitidas. Si quieren utilizar alguna librer√≠a adicional, pueden realizar la consulta a trav√©s de Discord. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CtP6Emjo1kF0"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj9Do0LdC9w2"
   },
   "source": [
    "En caso de desarrollar la tarea desde colab, utilizar el siguiente c√≥digo para cargar los archivos desde el drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9EXq1VDeC-ml"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2601773408.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [4]\u001b[0;36m\u001b[0m\n\u001b[0;31m    path = # Rellenar con la ruta del drive donde alojan la tarea\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "    path = # Rellenar con la ruta del drive donde alojan la tarea\n",
    "except: \n",
    "    print('Ignorando conexi√≥n drive-colab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSN4bBoY2Td4"
   },
   "source": [
    "` ` \n",
    "\n",
    "**Ejercicio 1 - *Tokenizaci√≥n* (0.5 puntos).** \n",
    "` `  \n",
    "` ` \n",
    "\n",
    "En el primer ejercicio veremos la dificultad üò® de tokenizar textos no estructurados, destacando la importancia de tener librer√≠as que realicen este trabajo. \n",
    "\n",
    "El archivo adjunto al enunciado de la tarea contiene la letra de una canci√≥n del marcianeke üëΩ. Utilice este texto para realizar su primera tokenizaci√≥n y ver que tan bien funciona su funci√≥n. \n",
    "\n",
    "Ejecute el c√≥digo a continuaci√≥n para cargar el ejemplo. Recuerde realizar la modificaci√≥n al directorio en caso que el archivo no se encuentre en el mismo directorio del Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1647388936045,
     "user": {
      "displayName": "Gabriel Iturra",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghe-mqbWbrqQ1gVhaBhwEAK5Uu5cfHEnENzwLJUGA=s64",
      "userId": "02319919045117626989"
     },
     "user_tz": 180
    },
    "id": "FnNUYlWo21g4",
    "outputId": "66ceaf30-0eed-487e-af02-d76b04123978"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brr\r\n",
      "Marcianeke\r\n",
      "Vamo' a estar con Pailita\r\n",
      "Dimelo m√°\r\n",
      "Ando en busca de una criminal (ah, ah)\r\n",
      "Esa que el gatillo le gusta jalar (rata-ta)\r\n",
      "Que le guste flotar y fumar (brr)\r\n",
      "Tussi, keta quiere' mezclar\r\n",
      "Dimelo m√°\r\n",
      "\r\n",
      "Ando en busca de una criminal (ah, ah)\r\n",
      "Esa que el gatillo le gusta jalar (rata-ta)\r\n",
      "Que le guste flotar y fumar\r\n",
      "Tussi, keta pura quiere' mezclar\r\n",
      "Di-dimelo m√°\r\n",
      "Di-dimelo m√°\r\n",
      "Di-dimelo m√°\r\n",
      "Di-dimelo m√°\r\n",
      "Di-dimelo m√°\r\n",
      "Di-dimelo m√°\r\n",
      "Di-dimelo m√°\r\n",
      "\r\n",
      "Esperame que ahora entro yo\r\n",
      "Y lo que pide yo lo traje\r\n",
      "No visto de traje\r\n",
      "Puro corte calle, no de maquillaje\r\n",
      "Pronto coronamos y nos vamo' de viaje\r\n",
      "Tanto hit que hago que lo' culo bajen\r\n",
      "Ella se va de shopping\r\n",
      "Sale positivo si se hace el doping\r\n",
      "Baila twerk con un poco de popping\r\n",
      "Los fardos en el bot√≠n\r\n",
      "\r\n",
      "Si quieren letra llamen pa' mi booking\r\n",
      "Generando, sigo en la m√≠a lowkey\r\n",
      "Cooking en el estudio con tu woman\r\n",
      "Tanto whisky, pisco que hasta lo' vecinos toman\r\n",
      "Si se tiran pa' aca puede que la arena coman\r\n",
      "Ja, en el chanteo titulado sin diploma\r\n",
      "Di-dimelo m√°\r\n",
      "Di-dimelo m√°\r\n",
      "Di-dimelo m√°\r\n",
      "Di-dimelo m√°\r\n",
      "Di-dimelo m√°\r\n",
      "Di-dimelo m√°\r\n",
      "Di-dimelo m√°\r\n",
      "\r\n",
      "Ah, pe-peligrosa\r\n",
      "Quiero ver como perreando me acosa\r\n",
      "Eso de atra' con el Gucci me lo roza\r\n",
      "Tengo tussi del naranjo me aburrio el rosa\r\n",
      "Capaz que tosa con el blunt\r\n",
      "Sprite con Flemibron\r\n",
      "Louis Vuitton, le quito la polera Champion\r\n",
      "A tu pretendiente con la fory lo espanto\r\n",
      "Puro perro, le doy de comer Champion Dog\r\n",
      "Ese toto lo corono yo\r\n",
      "La movie en play no hay stop\r\n",
      "Flow de sobra no hay stock\r\n",
      "Me la topo en la disco queda en shock\r\n",
      "My love, rica en las redes y en persona\r\n",
      "No usa Photoshop\r\n",
      "La llevo a comprar blone' al growshop\r\n",
      "En ropa interior los do'\r\n",
      "Me roza su vicky con mi boxer Top\r\n",
      "Dimelo m√°\r\n",
      "Ando en busca de una cri\r\n",
      "minal (ah, ah)\r\n",
      "Esa que el gatillo le gusta jalar (rata-ta)\r\n",
      "Que le guste flotar y fumar\r\n",
      "Tussi, keta quiere' mezclar\r\n",
      "Dimelo m√°\r\n",
      "\r\n",
      "Ando en busca de una criminal (ah, ah)\r\n",
      "Esa que el gatillo le gusta jalar (rata-ta)\r\n",
      "Que le guste flotar y fumar\r\n",
      "Tussi, keta pura quiere' mezclar\r\n",
      "Marcianeke, Pailita\r\n",
      "Young Varas\n"
     ]
    }
   ],
   "source": [
    "text = codecs.open('marcianeke.txt', 'r', 'UTF-8').read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIaOYzMk3v1X"
   },
   "source": [
    "Implementen una funci√≥n **`get_tokens()`** que reciba un texto y entregue una lista con sus tokens. Son libres de elegir la forma de tokenizar mientras no utilicen librer√≠as con tokenizadores ya implementados. Pueden utilizar la librer√≠a **re** importada para trabajar s√≠mbolos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dl42-hgIhqB"
   },
   "source": [
    "Ejemplo de uso:\n",
    "\n",
    "`get_tokens('Este es un ejemplo de prueba.')` \n",
    "\n",
    "Nos entregar√≠a:\n",
    "\n",
    "`['Este', 'es', 'un', 'ejemplo', 'de', 'prueba', '.']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "IF1RcIwq4G2x"
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def get_tokens(text):\n",
    "    tokens = text.split('\\n')\n",
    "    tokens = list(map(lambda x: x.strip().split(' '), tokens))\n",
    "    tokens = reduce(lambda x, y: x + y, tokens)\n",
    "    tokens = reduce(lambda l, el: l + re.split(r\"(\\w+)\", el), tokens, [])\n",
    "    tokens = list(filter(lambda x: x != '', tokens))\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "KqlSpefv4_EH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Brr',\n",
       " 'Marcianeke',\n",
       " 'Vamo',\n",
       " \"'\",\n",
       " 'a',\n",
       " 'estar',\n",
       " 'con',\n",
       " 'Pailita',\n",
       " 'Dimelo',\n",
       " 'm√°',\n",
       " 'Ando',\n",
       " 'en',\n",
       " 'busca',\n",
       " 'de',\n",
       " 'una',\n",
       " 'criminal',\n",
       " '(',\n",
       " 'ah',\n",
       " ',',\n",
       " 'ah',\n",
       " ')',\n",
       " 'Esa',\n",
       " 'que',\n",
       " 'el',\n",
       " 'gatillo',\n",
       " 'le',\n",
       " 'gusta',\n",
       " 'jalar',\n",
       " '(',\n",
       " 'rata',\n",
       " '-',\n",
       " 'ta',\n",
       " ')',\n",
       " 'Que',\n",
       " 'le',\n",
       " 'guste',\n",
       " 'flotar',\n",
       " 'y',\n",
       " 'fumar',\n",
       " '(',\n",
       " 'brr',\n",
       " ')',\n",
       " 'Tussi',\n",
       " ',',\n",
       " 'keta',\n",
       " 'quiere',\n",
       " \"'\",\n",
       " 'mezclar',\n",
       " 'Dimelo',\n",
       " 'm√°',\n",
       " 'Ando',\n",
       " 'en',\n",
       " 'busca',\n",
       " 'de',\n",
       " 'una',\n",
       " 'criminal',\n",
       " '(',\n",
       " 'ah',\n",
       " ',',\n",
       " 'ah',\n",
       " ')',\n",
       " 'Esa',\n",
       " 'que',\n",
       " 'el',\n",
       " 'gatillo',\n",
       " 'le',\n",
       " 'gusta',\n",
       " 'jalar',\n",
       " '(',\n",
       " 'rata',\n",
       " '-',\n",
       " 'ta',\n",
       " ')',\n",
       " 'Que',\n",
       " 'le',\n",
       " 'guste',\n",
       " 'flotar',\n",
       " 'y',\n",
       " 'fumar',\n",
       " 'Tussi',\n",
       " ',',\n",
       " 'keta',\n",
       " 'pura',\n",
       " 'quiere',\n",
       " \"'\",\n",
       " 'mezclar',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Esperame',\n",
       " 'que',\n",
       " 'ahora',\n",
       " 'entro',\n",
       " 'yo',\n",
       " 'Y',\n",
       " 'lo',\n",
       " 'que',\n",
       " 'pide',\n",
       " 'yo',\n",
       " 'lo',\n",
       " 'traje',\n",
       " 'No',\n",
       " 'visto',\n",
       " 'de',\n",
       " 'traje',\n",
       " 'Puro',\n",
       " 'corte',\n",
       " 'calle',\n",
       " ',',\n",
       " 'no',\n",
       " 'de',\n",
       " 'maquillaje',\n",
       " 'Pronto',\n",
       " 'coronamos',\n",
       " 'y',\n",
       " 'nos',\n",
       " 'vamo',\n",
       " \"'\",\n",
       " 'de',\n",
       " 'viaje',\n",
       " 'Tanto',\n",
       " 'hit',\n",
       " 'que',\n",
       " 'hago',\n",
       " 'que',\n",
       " 'lo',\n",
       " \"'\",\n",
       " 'culo',\n",
       " 'bajen',\n",
       " 'Ella',\n",
       " 'se',\n",
       " 'va',\n",
       " 'de',\n",
       " 'shopping',\n",
       " 'Sale',\n",
       " 'positivo',\n",
       " 'si',\n",
       " 'se',\n",
       " 'hace',\n",
       " 'el',\n",
       " 'doping',\n",
       " 'Baila',\n",
       " 'twerk',\n",
       " 'con',\n",
       " 'un',\n",
       " 'poco',\n",
       " 'de',\n",
       " 'popping',\n",
       " 'Los',\n",
       " 'fardos',\n",
       " 'en',\n",
       " 'el',\n",
       " 'bot√≠n',\n",
       " 'Si',\n",
       " 'quieren',\n",
       " 'letra',\n",
       " 'llamen',\n",
       " 'pa',\n",
       " \"'\",\n",
       " 'mi',\n",
       " 'booking',\n",
       " 'Generando',\n",
       " ',',\n",
       " 'sigo',\n",
       " 'en',\n",
       " 'la',\n",
       " 'm√≠a',\n",
       " 'lowkey',\n",
       " 'Cooking',\n",
       " 'en',\n",
       " 'el',\n",
       " 'estudio',\n",
       " 'con',\n",
       " 'tu',\n",
       " 'woman',\n",
       " 'Tanto',\n",
       " 'whisky',\n",
       " ',',\n",
       " 'pisco',\n",
       " 'que',\n",
       " 'hasta',\n",
       " 'lo',\n",
       " \"'\",\n",
       " 'vecinos',\n",
       " 'toman',\n",
       " 'Si',\n",
       " 'se',\n",
       " 'tiran',\n",
       " 'pa',\n",
       " \"'\",\n",
       " 'aca',\n",
       " 'puede',\n",
       " 'que',\n",
       " 'la',\n",
       " 'arena',\n",
       " 'coman',\n",
       " 'Ja',\n",
       " ',',\n",
       " 'en',\n",
       " 'el',\n",
       " 'chanteo',\n",
       " 'titulado',\n",
       " 'sin',\n",
       " 'diploma',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Ah',\n",
       " ',',\n",
       " 'pe',\n",
       " '-',\n",
       " 'peligrosa',\n",
       " 'Quiero',\n",
       " 'ver',\n",
       " 'como',\n",
       " 'perreando',\n",
       " 'me',\n",
       " 'acosa',\n",
       " 'Eso',\n",
       " 'de',\n",
       " 'atra',\n",
       " \"'\",\n",
       " 'con',\n",
       " 'el',\n",
       " 'Gucci',\n",
       " 'me',\n",
       " 'lo',\n",
       " 'roza',\n",
       " 'Tengo',\n",
       " 'tussi',\n",
       " 'del',\n",
       " 'naranjo',\n",
       " 'me',\n",
       " 'aburrio',\n",
       " 'el',\n",
       " 'rosa',\n",
       " 'Capaz',\n",
       " 'que',\n",
       " 'tosa',\n",
       " 'con',\n",
       " 'el',\n",
       " 'blunt',\n",
       " 'Sprite',\n",
       " 'con',\n",
       " 'Flemibron',\n",
       " 'Louis',\n",
       " 'Vuitton',\n",
       " ',',\n",
       " 'le',\n",
       " 'quito',\n",
       " 'la',\n",
       " 'polera',\n",
       " 'Champion',\n",
       " 'A',\n",
       " 'tu',\n",
       " 'pretendiente',\n",
       " 'con',\n",
       " 'la',\n",
       " 'fory',\n",
       " 'lo',\n",
       " 'espanto',\n",
       " 'Puro',\n",
       " 'perro',\n",
       " ',',\n",
       " 'le',\n",
       " 'doy',\n",
       " 'de',\n",
       " 'comer',\n",
       " 'Champion',\n",
       " 'Dog',\n",
       " 'Ese',\n",
       " 'toto',\n",
       " 'lo',\n",
       " 'corono',\n",
       " 'yo',\n",
       " 'La',\n",
       " 'movie',\n",
       " 'en',\n",
       " 'play',\n",
       " 'no',\n",
       " 'hay',\n",
       " 'stop',\n",
       " 'Flow',\n",
       " 'de',\n",
       " 'sobra',\n",
       " 'no',\n",
       " 'hay',\n",
       " 'stock',\n",
       " 'Me',\n",
       " 'la',\n",
       " 'topo',\n",
       " 'en',\n",
       " 'la',\n",
       " 'disco',\n",
       " 'queda',\n",
       " 'en',\n",
       " 'shock',\n",
       " 'My',\n",
       " 'love',\n",
       " ',',\n",
       " 'rica',\n",
       " 'en',\n",
       " 'las',\n",
       " 'redes',\n",
       " 'y',\n",
       " 'en',\n",
       " 'persona',\n",
       " 'No',\n",
       " 'usa',\n",
       " 'Photoshop',\n",
       " 'La',\n",
       " 'llevo',\n",
       " 'a',\n",
       " 'comprar',\n",
       " 'blone',\n",
       " \"'\",\n",
       " 'al',\n",
       " 'growshop',\n",
       " 'En',\n",
       " 'ropa',\n",
       " 'interior',\n",
       " 'los',\n",
       " 'do',\n",
       " \"'\",\n",
       " 'Me',\n",
       " 'roza',\n",
       " 'su',\n",
       " 'vicky',\n",
       " 'con',\n",
       " 'mi',\n",
       " 'boxer',\n",
       " 'Top',\n",
       " 'Dimelo',\n",
       " 'm√°',\n",
       " 'Ando',\n",
       " 'en',\n",
       " 'busca',\n",
       " 'de',\n",
       " 'una',\n",
       " 'cri',\n",
       " 'minal',\n",
       " '(',\n",
       " 'ah',\n",
       " ',',\n",
       " 'ah',\n",
       " ')',\n",
       " 'Esa',\n",
       " 'que',\n",
       " 'el',\n",
       " 'gatillo',\n",
       " 'le',\n",
       " 'gusta',\n",
       " 'jalar',\n",
       " '(',\n",
       " 'rata',\n",
       " '-',\n",
       " 'ta',\n",
       " ')',\n",
       " 'Que',\n",
       " 'le',\n",
       " 'guste',\n",
       " 'flotar',\n",
       " 'y',\n",
       " 'fumar',\n",
       " 'Tussi',\n",
       " ',',\n",
       " 'keta',\n",
       " 'quiere',\n",
       " \"'\",\n",
       " 'mezclar',\n",
       " 'Dimelo',\n",
       " 'm√°',\n",
       " 'Ando',\n",
       " 'en',\n",
       " 'busca',\n",
       " 'de',\n",
       " 'una',\n",
       " 'criminal',\n",
       " '(',\n",
       " 'ah',\n",
       " ',',\n",
       " 'ah',\n",
       " ')',\n",
       " 'Esa',\n",
       " 'que',\n",
       " 'el',\n",
       " 'gatillo',\n",
       " 'le',\n",
       " 'gusta',\n",
       " 'jalar',\n",
       " '(',\n",
       " 'rata',\n",
       " '-',\n",
       " 'ta',\n",
       " ')',\n",
       " 'Que',\n",
       " 'le',\n",
       " 'guste',\n",
       " 'flotar',\n",
       " 'y',\n",
       " 'fumar',\n",
       " 'Tussi',\n",
       " ',',\n",
       " 'keta',\n",
       " 'pura',\n",
       " 'quiere',\n",
       " \"'\",\n",
       " 'mezclar',\n",
       " 'Marcianeke',\n",
       " ',',\n",
       " 'Pailita',\n",
       " 'Young',\n",
       " 'Varas']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = get_tokens(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPbgTvAW-stF"
   },
   "source": [
    "**Describa cu√°les fueron sus supuestos para realizar la tokenizaci√≥n y compare sus tokens con los entregados por la librer√≠a nltk en el bloque de c√≥digo de m√°s abajo.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGQ7CJy3-3aH"
   },
   "source": [
    " > Se supuso que un token es una letra, una palabra o un s√≠mbolo. En particular, se intent√≥ seguir a misma forma de *tokenizar* que el ejemplo dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "YYtmAXTr9KXK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Brr',\n",
       " 'Marcianeke',\n",
       " 'Vamo',\n",
       " \"'\",\n",
       " 'a',\n",
       " 'estar',\n",
       " 'con',\n",
       " 'Pailita',\n",
       " 'Dimelo',\n",
       " 'm√°',\n",
       " 'Ando',\n",
       " 'en',\n",
       " 'busca',\n",
       " 'de',\n",
       " 'una',\n",
       " 'criminal',\n",
       " '(',\n",
       " 'ah',\n",
       " ',',\n",
       " 'ah',\n",
       " ')',\n",
       " 'Esa',\n",
       " 'que',\n",
       " 'el',\n",
       " 'gatillo',\n",
       " 'le',\n",
       " 'gusta',\n",
       " 'jalar',\n",
       " '(',\n",
       " 'rata',\n",
       " '-',\n",
       " 'ta',\n",
       " ')',\n",
       " 'Que',\n",
       " 'le',\n",
       " 'guste',\n",
       " 'flotar',\n",
       " 'y',\n",
       " 'fumar',\n",
       " '(',\n",
       " 'brr',\n",
       " ')',\n",
       " 'Tussi',\n",
       " ',',\n",
       " 'keta',\n",
       " 'quiere',\n",
       " \"'\",\n",
       " 'mezclar',\n",
       " 'Dimelo',\n",
       " 'm√°',\n",
       " 'Ando',\n",
       " 'en',\n",
       " 'busca',\n",
       " 'de',\n",
       " 'una',\n",
       " 'criminal',\n",
       " '(',\n",
       " 'ah',\n",
       " ',',\n",
       " 'ah',\n",
       " ')',\n",
       " 'Esa',\n",
       " 'que',\n",
       " 'el',\n",
       " 'gatillo',\n",
       " 'le',\n",
       " 'gusta',\n",
       " 'jalar',\n",
       " '(',\n",
       " 'rata',\n",
       " '-',\n",
       " 'ta',\n",
       " ')',\n",
       " 'Que',\n",
       " 'le',\n",
       " 'guste',\n",
       " 'flotar',\n",
       " 'y',\n",
       " 'fumar',\n",
       " 'Tussi',\n",
       " ',',\n",
       " 'keta',\n",
       " 'pura',\n",
       " 'quiere',\n",
       " \"'\",\n",
       " 'mezclar',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Esperame',\n",
       " 'que',\n",
       " 'ahora',\n",
       " 'entro',\n",
       " 'yo',\n",
       " 'Y',\n",
       " 'lo',\n",
       " 'que',\n",
       " 'pide',\n",
       " 'yo',\n",
       " 'lo',\n",
       " 'traje',\n",
       " 'No',\n",
       " 'visto',\n",
       " 'de',\n",
       " 'traje',\n",
       " 'Puro',\n",
       " 'corte',\n",
       " 'calle',\n",
       " ',',\n",
       " 'no',\n",
       " 'de',\n",
       " 'maquillaje',\n",
       " 'Pronto',\n",
       " 'coronamos',\n",
       " 'y',\n",
       " 'nos',\n",
       " 'vamo',\n",
       " \"'\",\n",
       " 'de',\n",
       " 'viaje',\n",
       " 'Tanto',\n",
       " 'hit',\n",
       " 'que',\n",
       " 'hago',\n",
       " 'que',\n",
       " 'lo',\n",
       " \"'\",\n",
       " 'culo',\n",
       " 'bajen',\n",
       " 'Ella',\n",
       " 'se',\n",
       " 'va',\n",
       " 'de',\n",
       " 'shopping',\n",
       " 'Sale',\n",
       " 'positivo',\n",
       " 'si',\n",
       " 'se',\n",
       " 'hace',\n",
       " 'el',\n",
       " 'doping',\n",
       " 'Baila',\n",
       " 'twerk',\n",
       " 'con',\n",
       " 'un',\n",
       " 'poco',\n",
       " 'de',\n",
       " 'popping',\n",
       " 'Los',\n",
       " 'fardos',\n",
       " 'en',\n",
       " 'el',\n",
       " 'bot√≠n',\n",
       " 'Si',\n",
       " 'quieren',\n",
       " 'letra',\n",
       " 'llamen',\n",
       " 'pa',\n",
       " \"'\",\n",
       " 'mi',\n",
       " 'booking',\n",
       " 'Generando',\n",
       " ',',\n",
       " 'sigo',\n",
       " 'en',\n",
       " 'la',\n",
       " 'm√≠a',\n",
       " 'lowkey',\n",
       " 'Cooking',\n",
       " 'en',\n",
       " 'el',\n",
       " 'estudio',\n",
       " 'con',\n",
       " 'tu',\n",
       " 'woman',\n",
       " 'Tanto',\n",
       " 'whisky',\n",
       " ',',\n",
       " 'pisco',\n",
       " 'que',\n",
       " 'hasta',\n",
       " 'lo',\n",
       " \"'\",\n",
       " 'vecinos',\n",
       " 'toman',\n",
       " 'Si',\n",
       " 'se',\n",
       " 'tiran',\n",
       " 'pa',\n",
       " \"'\",\n",
       " 'aca',\n",
       " 'puede',\n",
       " 'que',\n",
       " 'la',\n",
       " 'arena',\n",
       " 'coman',\n",
       " 'Ja',\n",
       " ',',\n",
       " 'en',\n",
       " 'el',\n",
       " 'chanteo',\n",
       " 'titulado',\n",
       " 'sin',\n",
       " 'diploma',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Di',\n",
       " '-',\n",
       " 'dimelo',\n",
       " 'm√°',\n",
       " 'Ah',\n",
       " ',',\n",
       " 'pe',\n",
       " '-',\n",
       " 'peligrosa',\n",
       " 'Quiero',\n",
       " 'ver',\n",
       " 'como',\n",
       " 'perreando',\n",
       " 'me',\n",
       " 'acosa',\n",
       " 'Eso',\n",
       " 'de',\n",
       " 'atra',\n",
       " \"'\",\n",
       " 'con',\n",
       " 'el',\n",
       " 'Gucci',\n",
       " 'me',\n",
       " 'lo',\n",
       " 'roza',\n",
       " 'Tengo',\n",
       " 'tussi',\n",
       " 'del',\n",
       " 'naranjo',\n",
       " 'me',\n",
       " 'aburrio',\n",
       " 'el',\n",
       " 'rosa',\n",
       " 'Capaz',\n",
       " 'que',\n",
       " 'tosa',\n",
       " 'con',\n",
       " 'el',\n",
       " 'blunt',\n",
       " 'Sprite',\n",
       " 'con',\n",
       " 'Flemibron',\n",
       " 'Louis',\n",
       " 'Vuitton',\n",
       " ',',\n",
       " 'le',\n",
       " 'quito',\n",
       " 'la',\n",
       " 'polera',\n",
       " 'Champion',\n",
       " 'A',\n",
       " 'tu',\n",
       " 'pretendiente',\n",
       " 'con',\n",
       " 'la',\n",
       " 'fory',\n",
       " 'lo',\n",
       " 'espanto',\n",
       " 'Puro',\n",
       " 'perro',\n",
       " ',',\n",
       " 'le',\n",
       " 'doy',\n",
       " 'de',\n",
       " 'comer',\n",
       " 'Champion',\n",
       " 'Dog',\n",
       " 'Ese',\n",
       " 'toto',\n",
       " 'lo',\n",
       " 'corono',\n",
       " 'yo',\n",
       " 'La',\n",
       " 'movie',\n",
       " 'en',\n",
       " 'play',\n",
       " 'no',\n",
       " 'hay',\n",
       " 'stop',\n",
       " 'Flow',\n",
       " 'de',\n",
       " 'sobra',\n",
       " 'no',\n",
       " 'hay',\n",
       " 'stock',\n",
       " 'Me',\n",
       " 'la',\n",
       " 'topo',\n",
       " 'en',\n",
       " 'la',\n",
       " 'disco',\n",
       " 'queda',\n",
       " 'en',\n",
       " 'shock',\n",
       " 'My',\n",
       " 'love',\n",
       " ',',\n",
       " 'rica',\n",
       " 'en',\n",
       " 'las',\n",
       " 'redes',\n",
       " 'y',\n",
       " 'en',\n",
       " 'persona',\n",
       " 'No',\n",
       " 'usa',\n",
       " 'Photoshop',\n",
       " 'La',\n",
       " 'llevo',\n",
       " 'a',\n",
       " 'comprar',\n",
       " 'blone',\n",
       " \"'\",\n",
       " 'al',\n",
       " 'growshop',\n",
       " 'En',\n",
       " 'ropa',\n",
       " 'interior',\n",
       " 'los',\n",
       " 'do',\n",
       " \"'\",\n",
       " 'Me',\n",
       " 'roza',\n",
       " 'su',\n",
       " 'vicky',\n",
       " 'con',\n",
       " 'mi',\n",
       " 'boxer',\n",
       " 'Top',\n",
       " 'Dimelo',\n",
       " 'm√°',\n",
       " 'Ando',\n",
       " 'en',\n",
       " 'busca',\n",
       " 'de',\n",
       " 'una',\n",
       " 'cri',\n",
       " 'minal',\n",
       " '(',\n",
       " 'ah',\n",
       " ',',\n",
       " 'ah',\n",
       " ')',\n",
       " 'Esa',\n",
       " 'que',\n",
       " 'el',\n",
       " 'gatillo',\n",
       " 'le',\n",
       " 'gusta',\n",
       " 'jalar',\n",
       " '(',\n",
       " 'rata',\n",
       " '-',\n",
       " 'ta',\n",
       " ')',\n",
       " 'Que',\n",
       " 'le',\n",
       " 'guste',\n",
       " 'flotar',\n",
       " 'y',\n",
       " 'fumar',\n",
       " 'Tussi',\n",
       " ',',\n",
       " 'keta',\n",
       " 'quiere',\n",
       " \"'\",\n",
       " 'mezclar',\n",
       " 'Dimelo',\n",
       " 'm√°',\n",
       " 'Ando',\n",
       " 'en',\n",
       " 'busca',\n",
       " 'de',\n",
       " 'una',\n",
       " 'criminal',\n",
       " '(',\n",
       " 'ah',\n",
       " ',',\n",
       " 'ah',\n",
       " ')',\n",
       " 'Esa',\n",
       " 'que',\n",
       " 'el',\n",
       " 'gatillo',\n",
       " 'le',\n",
       " 'gusta',\n",
       " 'jalar',\n",
       " '(',\n",
       " 'rata',\n",
       " '-',\n",
       " 'ta',\n",
       " ')',\n",
       " 'Que',\n",
       " 'le',\n",
       " 'guste',\n",
       " 'flotar',\n",
       " 'y',\n",
       " 'fumar',\n",
       " 'Tussi',\n",
       " ',',\n",
       " 'keta',\n",
       " 'pura',\n",
       " 'quiere',\n",
       " \"'\",\n",
       " 'mezclar',\n",
       " 'Marcianeke',\n",
       " ',',\n",
       " 'Pailita',\n",
       " 'Young',\n",
       " 'Varas']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize \n",
    "nltk_tokens = wordpunct_tokenize(text)\n",
    "nltk_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N√≥tese que se obtuvo el mismo resultado que el ejemplo. La siguiente l√≠nea de c√≥digo evidencia lo mencionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(nltk_tokens == tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5QKlXAZwN1L"
   },
   "source": [
    "` `  \n",
    "` ` \n",
    "\n",
    "**Ejercicio 2 - *Stopwords y Stemming* (1 punto).** \n",
    "` `  \n",
    "` ` \n",
    "\n",
    "Considere el siguiente corpus:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "sEp83zESwb2j"
   },
   "outputs": [],
   "source": [
    "dataset = [\"I like human languages\", \"I like programming languages\", \"Spanish is my favorite language\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmjdlJWuyS2E"
   },
   "source": [
    "Dise√±e una funci√≥n **`get_vocab()`** que extraiga los typos de este corpus solamente tokenizando. Puede utilizar la funci√≥n del Ejercicio 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "UudC-b6TzZgw"
   },
   "outputs": [],
   "source": [
    "def get_vocab(dataset):\n",
    "    typos = list(map(get_tokens, dataset))\n",
    "    typos = reduce(lambda x, y: x + y, typos, [])\n",
    "    typos = set(typos)\n",
    "    return list(typos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-m32IoNmSwM"
   },
   "source": [
    "***Test:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BNzPKiAx0Aa1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['languages',\n",
       " 'programming',\n",
       " 'my',\n",
       " 'favorite',\n",
       " 'like',\n",
       " 'Spanish',\n",
       " 'language',\n",
       " 'human',\n",
       " 'is',\n",
       " 'I']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = get_vocab(dataset)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZLV2hWf9FN7"
   },
   "source": [
    "``\n",
    "``\n",
    "\n",
    "***Resultado esperado***: \n",
    "<table>\n",
    "    <tr> \n",
    "        <td>['favorite',\n",
    " 'Spanish',\n",
    " 'language',\n",
    " 'I',\n",
    " 'like',\n",
    " 'programming',\n",
    " 'languages',\n",
    " 'my',\n",
    " 'human',\n",
    " 'is'] </td> \n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "``\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3KB0fL2zk2v"
   },
   "source": [
    "Ahora dise√±e reglas que usted estime convenientes tanto de **Stemming** como de **Stopwords**. Implemente una funci√≥n que reciba una lista con los elementos del vocabulario, le aplique sus reglas y devuelva el vocabulario actualizado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY7g67Ml0aby"
   },
   "source": [
    "> Para las reglas asociadas a Stemming se utiliz√≥ el algoritmo de Porter con las siguientes reglas:\n",
    "> - SSES $\\rightarrow$ SS\n",
    "> - IES $\\rightarrow$ I\n",
    "> - SS $\\rightarrow$ SS\n",
    "> - S $\\rightarrow$\n",
    ">\n",
    "> Para reglas asociadas a Stopwords se utiliz√≥ la siguiente lista de stopwords: <a href=\"https://gist.github.com/sebleier/554280\">NLTK's list of english stopwords</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "lrKDtM_H0XlE"
   },
   "outputs": [],
   "source": [
    "def pre_processing(vocab):\n",
    "    \n",
    "    def porter(token):\n",
    "        if token.endswith(\"sses\"):\n",
    "            token = re.sub(\"(sses)$\", \"ss\", token)\n",
    "        elif token.endswith(\"ies\"):\n",
    "            token = re.sub(\"(ies)$\", \"i\", token)\n",
    "        elif token.endswith(\"ss\"):\n",
    "            token = re.sub(\"(ss)$\", \"ss\", token)\n",
    "        elif token.endswith(\"s\"):\n",
    "            token = re.sub(\"(s)$\", \"\", token)\n",
    "        return token\n",
    "    \n",
    "    def is_stopword(token):\n",
    "        stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "        return token in stopwords\n",
    "    \n",
    "    vocab = map(lambda tok: tok.lower(), vocab) # lower\n",
    "    vocab = map(porter, vocab) # porter stemming\n",
    "    vocab = filter(lambda x: not is_stopword(x), vocab) # filter stopwords\n",
    "    vocab = set(vocab) # remove non-unique words\n",
    "    return list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "onOSuS-_mL2F"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['programming', 'favorite', 'like', 'human', 'spanish', 'language']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = pre_processing(vocab)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65IwHx11uA75"
   },
   "source": [
    "\n",
    "**Ejercicio 3 - *Bag of Words* üê∂üêà(0.5 puntos).** \n",
    " \n",
    "\n",
    "\n",
    "Considere el siguiente corpus, donde cada elemento del arreglo representa un documento:\n",
    "\n",
    "**disclaimer: El orden de los resultados pueden variar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Rh-utMuozhsK"
   },
   "outputs": [],
   "source": [
    "d0 = 'El perro se come la comida y despu√©s se duerme'\n",
    "d1 = 'El perro se despierta y despu√©s empieza a ladrar'\n",
    "d2 = 'El perro ladra y despu√©s se come la comida'\n",
    "d3 = 'El gato se come la comida y despu√©s se duerme'\n",
    "d4 = 'El gato se despierta y despu√©s empieza a maullar'\n",
    "d5 = 'El gato maulla y despu√©s se come la comida'\n",
    "dataset = [d0, d1, d2, d3, d4, d5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WH7ne8C6ltvE"
   },
   "source": [
    "El objetivo de este ejercicio es determinar cu√°les de  los documentos entregados son los m√°s similares entre s√≠. Para ello utilizaremos la t√©cnica TF-IDF. \n",
    "\n",
    "Como los algoritmos de Machine Learning no comprenden el texto en lenguaje natural, estos documentos deben ser convertidos a vectores num√©ricos. La representaci√≥n m√°s simple vista en clases es el **Bag of Words**, m√©todo mediante el cu√°l se cuentan las apariciones de cada palabra en cada uno de los documentos entregados.\n",
    "\n",
    "Implemente la funci√≥n **`bag_of_words()`**, que reciba como input un arreglo de documentos y devuelva un pandas dataframe con la representaci√≥n Bag of Words de los documentos entregados. En esta representaci√≥n las columnas son el vocabulario y las filas representa las apariciones de cada una de las palabras en los documentos. En otras palabras, cada fila representa el bow de un determinado documento.\n",
    "\n",
    "\n",
    "Por ejemplo para el siguiente dataset: \n",
    "\n",
    "```\n",
    "dataset = ['El perro ladra', 'El perro come']\n",
    "```\n",
    "\n",
    "Debiese entregarnos lo siguiente:\n",
    "\n",
    "\n",
    "|   | el | perro | ladra | come |\n",
    "|---|----|-------|------|-------|\n",
    "| 0 | 1  | 1     | 1    | 0     |\n",
    "| 1 | 1  | 1     | 0    | 1     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "kXDMAyFmnq5j"
   },
   "outputs": [],
   "source": [
    "def bag_of_words(dataset):\n",
    "    vocab = get_vocab(dataset)\n",
    "    \n",
    "    def freq_list(document):\n",
    "        freqs = {key: 0 for key in vocab}\n",
    "        for token in get_tokens(document):\n",
    "            freqs[token] += 1\n",
    "        return [freqs[word] for word in vocab]\n",
    "    \n",
    "    frequencies = list(map(freq_list, dataset))\n",
    "    dataset_names = map(lambda x: f\"d{x}\", range(len(dataset)))\n",
    "    frame = pd.DataFrame(data=frequencies, index=dataset_names, columns=vocab)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okfo-nEQmW1R"
   },
   "source": [
    "***Test:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "T_Kk9GwCoDW8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maulla</th>\n",
       "      <th>ladra</th>\n",
       "      <th>la</th>\n",
       "      <th>El</th>\n",
       "      <th>empieza</th>\n",
       "      <th>ladrar</th>\n",
       "      <th>gato</th>\n",
       "      <th>comida</th>\n",
       "      <th>maullar</th>\n",
       "      <th>se</th>\n",
       "      <th>a</th>\n",
       "      <th>y</th>\n",
       "      <th>perro</th>\n",
       "      <th>despierta</th>\n",
       "      <th>duerme</th>\n",
       "      <th>despu√©s</th>\n",
       "      <th>come</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    maulla  ladra  la  El  empieza  ladrar  gato  comida  maullar  se  a  y  \\\n",
       "d0       0      0   1   1        0       0     0       1        0   2  0  1   \n",
       "d1       0      0   0   1        1       1     0       0        0   1  1  1   \n",
       "d2       0      1   1   1        0       0     0       1        0   1  0  1   \n",
       "d3       0      0   1   1        0       0     1       1        0   2  0  1   \n",
       "d4       0      0   0   1        1       0     1       0        1   1  1  1   \n",
       "d5       1      0   1   1        0       0     1       1        0   1  0  1   \n",
       "\n",
       "    perro  despierta  duerme  despu√©s  come  \n",
       "d0      1          0       1        1     1  \n",
       "d1      1          1       0        1     0  \n",
       "d2      1          0       0        1     1  \n",
       "d3      0          0       1        1     1  \n",
       "d4      0          1       0        1     0  \n",
       "d5      0          0       0        1     1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_bow = bag_of_words(dataset)\n",
    "dataset_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeR5ADGz-MPa"
   },
   "source": [
    "``\n",
    "``\n",
    "\n",
    "***Resultado esperado***: \n",
    "\n",
    "|    | El | perro | se | come | la | comida | y | despu√©s | duerme | despierta | empieza | a | ladrar | ladra | gato | maullar | maulla |\n",
    "|----|---:|------:|---:|-----:|---:|-------:|--:|--------:|-------:|----------:|--------:|--:|-------:|------:|-----:|--------:|-------:|\n",
    "| d0 |  1 |     1 |  2 |    1 |  1 |      1 | 1 |       1 |      1 |         0 |       0 | 0 |      0 |     0 |    0 |       0 |      0 |\n",
    "| d1 |  1 |     1 |  1 |    0 |  0 |      0 | 1 |       1 |      0 |         1 |       1 | 1 |      1 |     0 |    0 |       0 |      0 |\n",
    "| d2 |  1 |     1 |  1 |    1 |  1 |      1 | 1 |       1 |      0 |         0 |       0 | 0 |      0 |     1 |    0 |       0 |      0 |\n",
    "| d3 |  1 |     0 |  2 |    1 |  1 |      1 | 1 |       1 |      1 |         0 |       0 | 0 |      0 |     0 |    1 |       0 |      0 |\n",
    "| d4 |  1 |     0 |  1 |    0 |  0 |      0 | 1 |       1 |      0 |         1 |       1 | 1 |      0 |     0 |    1 |       1 |      0 |\n",
    "| d5 |  1 |     0 |  1 |    1 |  1 |      1 | 1 |       1 |      0 |         0 |       0 | 0 |      0 |     0 |    1 |       0 |      1 |\n",
    "\n",
    "``\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4OXMz7opWcd"
   },
   "source": [
    "` `  \n",
    "` ` \n",
    "\n",
    "**Ejercicio 4 - *Calcular TF* (0.5 puntos):** Ahora debemos usar el dataframe del ejercicio anterior para calcular la matriz de TF normalizada por la m√°xima frecuencia ${max_i({\\text{tf}_{i,j})}}$, donde\n",
    "i corresponde al √≠ndice de las filas (bow) y j al de las columnas (palabras). Es decir, dividir cada bow en la cantidad de veces de la palabra que aparezca m√°s veces en ese vector. \n",
    "\n",
    "\n",
    "$$\\text{nft}_{i,j} = \\frac{\\text{tf}_{i,j}}{max_i({\\text{tf}_{i,j})}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "xWE16xhBpswc"
   },
   "outputs": [],
   "source": [
    "def calc_tf(dataset_bow):\n",
    "    maxs = dataset_bow.apply(max, 1)\n",
    "    data = dataset_bow.apply(lambda vec: list(map(lambda x: x[0] / x[1],zip(vec, maxs))), 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZQPZe3JmYqy"
   },
   "source": [
    "***Test:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "YQ2h8jEYp4nZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maulla</th>\n",
       "      <th>ladra</th>\n",
       "      <th>la</th>\n",
       "      <th>El</th>\n",
       "      <th>empieza</th>\n",
       "      <th>ladrar</th>\n",
       "      <th>gato</th>\n",
       "      <th>comida</th>\n",
       "      <th>maullar</th>\n",
       "      <th>se</th>\n",
       "      <th>a</th>\n",
       "      <th>y</th>\n",
       "      <th>perro</th>\n",
       "      <th>despierta</th>\n",
       "      <th>duerme</th>\n",
       "      <th>despu√©s</th>\n",
       "      <th>come</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    maulla  ladra   la   El  empieza  ladrar  gato  comida  maullar   se    a  \\\n",
       "d0     0.0    0.0  0.5  0.5      0.0     0.0   0.0     0.5      0.0  1.0  0.0   \n",
       "d1     0.0    0.0  0.0  1.0      1.0     1.0   0.0     0.0      0.0  1.0  1.0   \n",
       "d2     0.0    1.0  1.0  1.0      0.0     0.0   0.0     1.0      0.0  1.0  0.0   \n",
       "d3     0.0    0.0  0.5  0.5      0.0     0.0   0.5     0.5      0.0  1.0  0.0   \n",
       "d4     0.0    0.0  0.0  1.0      1.0     0.0   1.0     0.0      1.0  1.0  1.0   \n",
       "d5     1.0    0.0  1.0  1.0      0.0     0.0   1.0     1.0      0.0  1.0  0.0   \n",
       "\n",
       "      y  perro  despierta  duerme  despu√©s  come  \n",
       "d0  0.5    0.5        0.0     0.5      0.5   0.5  \n",
       "d1  1.0    1.0        1.0     0.0      1.0   0.0  \n",
       "d2  1.0    1.0        0.0     0.0      1.0   1.0  \n",
       "d3  0.5    0.0        0.0     0.5      0.5   0.5  \n",
       "d4  1.0    0.0        1.0     0.0      1.0   0.0  \n",
       "d5  1.0    0.0        0.0     0.0      1.0   1.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = calc_tf(dataset_bow)\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOzdRwx9_UMM"
   },
   "source": [
    "``\n",
    "``\n",
    "\n",
    "***Resultado esperado***: \n",
    "\n",
    "|    |  El | perro |  se | come |  la | comida |   y | despu√©s | duerme | despierta | empieza |   a | ladrar | ladra | gato | maullar | maulla |\n",
    "|----|----:|------:|----:|-----:|----:|-------:|----:|--------:|-------:|----------:|--------:|----:|-------:|------:|-----:|--------:|-------:|\n",
    "| d0 | 0.5 |   0.5 | 1.0 |  0.5 | 0.5 |    0.5 | 0.5 |     0.5 |    0.5 |       0.0 |     0.0 | 0.0 |    0.0 |   0.0 |  0.0 |     0.0 |    0.0 |\n",
    "| d1 | 1.0 |   1.0 | 1.0 |  0.0 | 0.0 |    0.0 | 1.0 |     1.0 |    0.0 |       1.0 |     1.0 | 1.0 |    1.0 |   0.0 |  0.0 |     0.0 |    0.0 |\n",
    "| d2 | 1.0 |   1.0 | 1.0 |  1.0 | 1.0 |    1.0 | 1.0 |     1.0 |    0.0 |       0.0 |     0.0 | 0.0 |    0.0 |   1.0 |  0.0 |     0.0 |    0.0 |\n",
    "| d3 | 0.5 |   0.0 | 1.0 |  0.5 | 0.5 |    0.5 | 0.5 |     0.5 |    0.5 |       0.0 |     0.0 | 0.0 |    0.0 |   0.0 |  0.5 |     0.0 |    0.0 |\n",
    "| d4 | 1.0 |   0.0 | 1.0 |  0.0 | 0.0 |    0.0 | 1.0 |     1.0 |    0.0 |       1.0 |     1.0 | 1.0 |    0.0 |   0.0 |  1.0 |     1.0 |    0.0 |\n",
    "| d5 | 1.0 |   0.0 | 1.0 |  1.0 | 1.0 |    1.0 | 1.0 |     1.0 |    0.0 |       0.0 |     0.0 | 0.0 |    0.0 |   0.0 |  1.0 |     0.0 |    1.0 |\n",
    "\n",
    "``\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqgW4Ni4t0xC"
   },
   "source": [
    "` `  \n",
    "` ` \n",
    "\n",
    "**Ejercicio 5 - *Calcular IDF* (0.5 punto):**\n",
    "\n",
    "\n",
    "Implementar `calc_idf(dataset_bow)`. Este debe retornar un diccionario en donde las llaves sean las palabras y los valores sean el calculo de cada idf por palabra.\n",
    "\n",
    "Recordar que $idf_{t_i} = log_{10}\\frac{N}{n_i}$ con $N = $ n√∫mero de documentos y $n_i = $ N√∫mero de documentos que contienen la palabra $t_i$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "thhDY1-Ht6T5"
   },
   "outputs": [],
   "source": [
    "def calc_idf(dataset_bow):\n",
    "    N = len(dataset_bow.index)\n",
    "    n = dataset_bow.apply(lambda x: reduce(lambda x, y: x + (1 if y >= 1 else 0), x, 0), 0)\n",
    "    idf = n.map(lambda n_i: np.log10(N/n_i))\n",
    "    return dict(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yR_j3pYemcAc"
   },
   "source": [
    "***Test:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "ro-OMGpduC0R"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'maulla': 0.7781512503836436,\n",
       " 'ladra': 0.7781512503836436,\n",
       " 'la': 0.17609125905568124,\n",
       " 'El': 0.0,\n",
       " 'empieza': 0.47712125471966244,\n",
       " 'ladrar': 0.7781512503836436,\n",
       " 'gato': 0.3010299956639812,\n",
       " 'comida': 0.17609125905568124,\n",
       " 'maullar': 0.7781512503836436,\n",
       " 'se': 0.0,\n",
       " 'a': 0.47712125471966244,\n",
       " 'y': 0.0,\n",
       " 'perro': 0.3010299956639812,\n",
       " 'despierta': 0.47712125471966244,\n",
       " 'duerme': 0.47712125471966244,\n",
       " 'despu√©s': 0.0,\n",
       " 'come': 0.17609125905568124}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = calc_idf(dataset_bow)\n",
    "idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ioy_HicQDr-a"
   },
   "source": [
    "***Resultado esperado***: \n",
    "\n",
    "```python\n",
    "{'El': 0.0, \n",
    " 'a': 0.47712125471966244,\n",
    " 'come': 0.17609125905568124,\n",
    " 'comida': 0.17609125905568124,\n",
    " 'despierta': 0.47712125471966244,\n",
    " 'despu√©s': 0.0,\n",
    " 'duerme': 0.47712125471966244,\n",
    " 'empieza': 0.47712125471966244,\n",
    " 'gato': 0.3010299956639812,\n",
    " 'la': 0.17609125905568124,\n",
    " 'ladra': 0.7781512503836436,\n",
    " 'ladrar': 0.7781512503836436,\n",
    " 'maulla': 0.7781512503836436,\n",
    " 'maullar': 0.7781512503836436,\n",
    " 'perro': 0.3010299956639812,\n",
    " 'se': 0.0,\n",
    " 'y': 0.0}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzKAzJtSJ7gx"
   },
   "source": [
    "Puede notar el bajo puntaje otorgado a las palabras que m√°s se repiten! üòÆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D17lm6l9uJPo"
   },
   "source": [
    "**Ejercicio 6 - *Calcular TF-IDF & concluir similitud de documentos.* (1 punto)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc7FTQ19Kcwo"
   },
   "source": [
    "Programe la funci√≥n `calc_tf_idf(tf, idf)` que entrega el dataframe TF-IDF asociado al dataset que estamos analizando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "9knMl0KguMwo"
   },
   "outputs": [],
   "source": [
    "def calc_tf_idf(tf, idf):\n",
    "    tfidf = tf.copy()\n",
    "    for word in tf.columns:\n",
    "        tfidf[word] = tfidf[word].map(lambda x: idf[word] * x)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRzIr1nQmepp"
   },
   "source": [
    "***Test.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "H8z6jaq2uPEo"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maulla</th>\n",
       "      <th>ladra</th>\n",
       "      <th>la</th>\n",
       "      <th>El</th>\n",
       "      <th>empieza</th>\n",
       "      <th>ladrar</th>\n",
       "      <th>gato</th>\n",
       "      <th>comida</th>\n",
       "      <th>maullar</th>\n",
       "      <th>se</th>\n",
       "      <th>a</th>\n",
       "      <th>y</th>\n",
       "      <th>perro</th>\n",
       "      <th>despierta</th>\n",
       "      <th>duerme</th>\n",
       "      <th>despu√©s</th>\n",
       "      <th>come</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>0.778151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.778151</td>\n",
       "      <td>0.176091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150515</td>\n",
       "      <td>0.088046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.778151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d5</th>\n",
       "      <td>0.778151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.176091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      maulla     ladra        la   El   empieza    ladrar      gato    comida  \\\n",
       "d0  0.000000  0.000000  0.088046  0.0  0.000000  0.000000  0.000000  0.088046   \n",
       "d1  0.000000  0.000000  0.000000  0.0  0.477121  0.778151  0.000000  0.000000   \n",
       "d2  0.000000  0.778151  0.176091  0.0  0.000000  0.000000  0.000000  0.176091   \n",
       "d3  0.000000  0.000000  0.088046  0.0  0.000000  0.000000  0.150515  0.088046   \n",
       "d4  0.000000  0.000000  0.000000  0.0  0.477121  0.000000  0.301030  0.000000   \n",
       "d5  0.778151  0.000000  0.176091  0.0  0.000000  0.000000  0.301030  0.176091   \n",
       "\n",
       "     maullar   se         a    y     perro  despierta    duerme  despu√©s  \\\n",
       "d0  0.000000  0.0  0.000000  0.0  0.150515   0.000000  0.238561      0.0   \n",
       "d1  0.000000  0.0  0.477121  0.0  0.301030   0.477121  0.000000      0.0   \n",
       "d2  0.000000  0.0  0.000000  0.0  0.301030   0.000000  0.000000      0.0   \n",
       "d3  0.000000  0.0  0.000000  0.0  0.000000   0.000000  0.238561      0.0   \n",
       "d4  0.778151  0.0  0.477121  0.0  0.000000   0.477121  0.000000      0.0   \n",
       "d5  0.000000  0.0  0.000000  0.0  0.000000   0.000000  0.000000      0.0   \n",
       "\n",
       "        come  \n",
       "d0  0.088046  \n",
       "d1  0.000000  \n",
       "d2  0.176091  \n",
       "d3  0.088046  \n",
       "d4  0.000000  \n",
       "d5  0.176091  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = calc_tf_idf(tf, idf)\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBG2qfwv_6HK"
   },
   "source": [
    "``\n",
    "``\n",
    "\n",
    "***Resultado esperado***: \n",
    "\n",
    "|    |  El |    perro |  se |     come |       la |   comida |   y | despu√©s |   duerme | despierta |  empieza |        a |   ladrar |    ladra |     gato |  maullar |   maulla |\n",
    "|----|----:|---------:|----:|---------:|---------:|---------:|----:|--------:|---------:|----------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|\n",
    "| d0 | 0.0 | 0.150515 | 0.0 | 0.088046 | 0.088046 | 0.088046 | 0.0 |     0.0 | 0.238561 |  0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 |\n",
    "| d1 | 0.0 | 0.301030 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 |     0.0 | 0.000000 |  0.477121 | 0.477121 | 0.477121 | 0.778151 | 0.000000 | 0.000000 | 0.000000 | 0.000000 |\n",
    "| d2 | 0.0 | 0.301030 | 0.0 | 0.176091 | 0.176091 | 0.176091 | 0.0 |     0.0 | 0.000000 |  0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.778151 | 0.000000 | 0.000000 | 0.000000 |\n",
    "| d3 | 0.0 | 0.000000 | 0.0 | 0.088046 | 0.088046 | 0.088046 | 0.0 |     0.0 | 0.238561 |  0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.150515 | 0.000000 | 0.000000 |\n",
    "| d4 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 |     0.0 | 0.000000 |  0.477121 | 0.477121 | 0.477121 | 0.000000 | 0.000000 | 0.301030 | 0.778151 | 0.000000 |\n",
    "| d5 | 0.0 | 0.000000 | 0.0 | 0.176091 | 0.176091 | 0.176091 | 0.0 |     0.0 | 0.000000 |  0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.301030 | 0.000000 | 0.778151 |\n",
    "\n",
    "\n",
    "``\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmVlbpzMp5NU"
   },
   "source": [
    "Ahora que tenemos el dataframe de TF-IDF, nos queda calcular la similitud coseno entre todos los vectores. Notar que la matriz resultante ser√° una matriz sim√©trica. Implemente la funci√≥n *cosine_similarity(v1, v2)* que recibe dos vectores (v1 y v2) y calcula la similitud coseno entre ambos vectores. Concluya cu√°les son los dos documentos m√°s similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "HEgUtgBkQAae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.12032418 0.3223436  0.77967014 0.         0.1632828 ]\n",
      " [0.12032418 1.         0.08686457 0.         0.4952126  0.        ]\n",
      " [0.3223436  0.08686457 1.         0.1632828  0.         0.11787732]\n",
      " [0.77967014 0.         0.1632828  1.         0.12032418 0.3223436 ]\n",
      " [0.         0.4952126  0.         0.12032418 1.         0.08686457]\n",
      " [0.1632828  0.         0.11787732 0.3223436  0.08686457 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "similarity_matrix = np.zeros((6,6))\n",
    "for i, v1 in enumerate(tf_idf.index.values):\n",
    "  for j, v2 in enumerate(tf_idf.index.values):\n",
    "      similarity = cosine_similarity(tf_idf.loc[v1].values, tf_idf.loc[v2].values)\n",
    "      similarity_matrix[i][j] = similarity\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9FlltqIqRxf"
   },
   "source": [
    "> De la matriz de similitudes, se puede concuir que los documentos m√°s similares son el documento *d0* (\"El perro se come la comida y despu√©s se duerme\") y el documento *d3* (\"El gato se come la comida y despu√©s se duerme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUAc1zX0Lg16"
   },
   "source": [
    "![gato](https://www.elagoradiario.com/wp-content/uploads/2020/05/Gato-mascarilla.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1A95IaXLHaB"
   },
   "source": [
    "**Cualquier recomendaci√≥n que nos quisieran dar para una futura tarea es bienvenid@!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tarea_1_CC6205_Natural_Language_Processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
