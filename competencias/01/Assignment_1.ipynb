{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:49:08.174519Z",
     "start_time": "2020-03-31T13:49:08.165989Z"
    },
    "id": "gpbvNOH0zvIi"
   },
   "source": [
    "# **Competencia 1 - CC6205 Natural Language Processing 游닄**\n",
    "\n",
    "**Integrantes:** Nicolas Lemu침ir y Mat칤as Seda\n",
    "\n",
    "**Usuario del equipo en CodaLab:**\n",
    "\n",
    "**Fecha l칤mite de entrega 游늱:** Jueves 14 de Abril.\n",
    "\n",
    "**Tiempo estimado de dedicaci칩n:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxlNrNf_p0ZY"
   },
   "source": [
    "## **Objetivo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrtvsKf2p3A4"
   },
   "source": [
    "En esta tarea grupal participar치n de una competencia estilo [Kaggle](https://www.kaggle.com/) pero utilizando la p치gina [CodaLab](https://codalab.org/). El objetivo es trabajar en la clasificaci칩n de tweets  seg칰n su intensidad de emoci칩n, esto corresponde a la task de clasificaci칩n de texto. \n",
    "\n",
    "Tendr치n a su disposici칩n 4 datasets de tweets con distintas emociones: `anger`, `fear`, `sadness` y `joy`. Deber치n crear un clasificador para cada uno de estos datasets que indique la intensidad de dicha emoci칩n en sus tweets (`low`, `medium`, `high`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6lhhfl2zvIk"
   },
   "source": [
    "## **Instrucciones**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T14:34:38.796217Z",
     "start_time": "2020-04-07T14:34:38.782255Z"
    },
    "id": "7wTyult1zvIl"
   },
   "source": [
    "- La competencia consiste en resolver 4 problemas de clasificaci칩n **distintos**, cada uno con tres clases posibles. Por cada problema deber치n crear un clasificador distinto. La evaluaci칩n de la competencia se realiza en base a 3 m칠tricas: `AUC`, `Kappa` y `Accuracy`. Los mejores puntajes en cada una de estas m칠tricas ser치n quienes ganen la competencia.\n",
    "\n",
    "- Para comenzar se les entregar치 en este notebook la estructura del informe a entregar y el c칩digo de un baseline con el cu치l pueden comparar los resultados de sus experimentos. Este baseline consiste en la creaci칩n de features y una clasificaci칩n simple siguiendo el paradigma de Machine Learning emp칤rico. Los puntajes obtenidos por el baseline los pueden visualizar en el link de CodaLab buscando al usuario **cc6205**. Esperamos que los superen f치cilmente 游땔\n",
    "\n",
    "- Para participar, deben registrarse en CodaLab y luego ingresar a la competencia usando el siguiente [link](https://competitions.codalab.org/competitions/30330?secret_key=b2ed0be7-bf23-42a1-9400-f85fa1b7bae7). \n",
    "\n",
    "- Est치 permitido hacer grupos de m치ximo 4 alumnos. Cada grupo debe tener un nombre de equipo, para ello en CodaLab pueden dirigirse a settings y luego cambiar el Team Name. S칩lo una persona debe administrar la cuenta del grupo y se verificar치 que no se hayan creado m칰ltiples cuentas por grupo.\n",
    "\n",
    "- En total pueden hacer un **m치ximo de 4 submissions**, hagan muchos experimentos probando en el conjunto de test antes de realizar el env칤o.\n",
    "\n",
    "- Es importante que hagan varios experimentos incorporando t칠cnicas como [cross-validation](https://es.wikipedia.org/wiki/Validaci%C3%B3n_cruzada#:~:text=La%20validaci%C3%B3n%20cruzada%20o%20cross,datos%20de%20entrenamiento%20y%20prueba.) o [random sampling](https://towardsdatascience.com/the-5-sampling-algorithms-every-data-scientist-need-to-know-43c7bc11d17c) antes de enviar sus predicciones a CodaLab, ya que les puede dar un mejor indicio del nivel de generalizaci칩n de sus modelos. \n",
    "\n",
    "- Aseg칰rense que la distribuci칩n de las clases sea balanceada en las particiones de training y testing debido a que existe un desbalanceo. \n",
    "\n",
    "- Verificar que el formato de la submission coincida con el de la competencia. De lo contrario, se les ser치 evaluado incorrectamente ya que el Script de evaluaci칩n espera como input dicho formato. En el c칩digo de las m칠tricas pueden verificar c칩mo son los inputs.\n",
    "\n",
    "- No se limiten a los contenidos vistos ni a scikit ni a este baseline. No tienen restricciones entre utilizar Deep Learning o Machine Learning emp칤rico. Si reutilizan gran cantidad de c칩digo de alguna p치gina por favor mostrar la referencia en su c칩digo. 춰Usen todo su conocimiento e ingenio en mejorar sus sistemas para poder ganar la competencia! \n",
    "\n",
    "- **Es requisito entregar el reporte con el c칩digo y haber participado en la competencia para ser evaluado. Un c칩digo sin reporte o un reporte sin c칩digo ser치n evaluados con la nota m칤nima.**\n",
    "\n",
    "- Todas las dudas escr칤banlas en el canal de Discord de competencias. Los emails que lleguen al equipo docente ser치n remitidos a ese medio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:25:19.677190Z",
     "start_time": "2020-04-07T15:25:19.671206Z"
    },
    "id": "jiDISxa-zvIn"
   },
   "source": [
    "**Importante**: Recuerden poner su nombre y el de su usuario o de equipo (en caso de que aplique) en el reporte. NO ser치n evaluados Notebooks sin nombre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igf7TBfSzvIo"
   },
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6moqxkEwCe-"
   },
   "source": [
    "## **Reporte a entregar**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo7vfXV4wD8s"
   },
   "source": [
    "Uno de los puntos claves en la evaluaci칩n de esta competencia es elaborar un informe claro y preciso, argumentando las decisiones tomadas al momento de crear sus modelos para que el cuerpo docente pueda entenderlos. La estructura que debe contener es la siguiente:\n",
    "\n",
    "1.\t**Introducci칩n**: Presentar brevemente el problema a resolver, incluyendo la formalizaci칩n de la task (c칩mo son los inputs y outputs del problema) y los desaf칤os que ven al analizar el corpus entregado. (**0.5 puntos**)\n",
    "2.\t**Representaciones**: Describir los atributos y representaciones usadas como entrada de los clasificadores, **recordar** que para entrenar modelos el input debe tener su representaci칩n num칠rica. Si bien, con Bag of Words (**baseline**) ya se comienzan a percibir buenos resultados, pueden mejorar su evaluaci칩n agregando m치s atributos y representaciones dise침adas a mano, sean lo m치s creativos posible. M치s abajo encontrar치n una lista de estos posibles atributos que les podr치 ser de utilidad. (**1 punto**)\n",
    "3.\t**Algoritmos**: Describir **brevemente** los algoritmos de clasificaci칩n usados, tanto si fueron algoritmos ya vistos en clases o bien arquitecturas de Deep Learning. (**0.5 puntos**)\n",
    "4.\t**M칠tricas de evaluaci칩n**: Describir brevemente las m칠tricas utilizadas en la evaluaci칩n, indicando qu칠 miden y su interpretaci칩n. (**0.5 puntos**)\n",
    "\n",
    "5.  **Dise침o experimental**: Esta es una de las secciones m치s importantes del reporte. Deben describir minuciosamente los experimentos que realizar치n en la siguiente secci칩n. Describir las variables de control que manejar치n, algunos ejemplos pueden ser: Los par치metros de los clasificadores, par치metros en las funciones con que procesan los textos y los transforman, par치metros para el cross-validation, particiones de datos utilizadas, etc. En caso que utilicen redes neuronales, ser claros con el conjunto de hiperpar치metros que probar치n, la decisi칩n en las funciones de optimizaci칩n, funci칩n de p칠rdida,  regulaci칩n, etc. B치sicamente explicar qu칠 es lo que veremos en la siguiente secci칩n.\n",
    "(**1 punto**)\n",
    "\n",
    "6.\t**Experimentos**: Incluyan todo el c칩digo de sus experimentos aqu칤. 춰Es vital haber realizado varios experimentos para sacar una buena nota! (**1.5 puntos**)\n",
    "\n",
    "7. **Resultados**: Comparar los resultados obtenidos utilizando diferentes algoritmos y representaciones.  Pueden mostrar los resultados sobre la partici칩n de validaci칩n en caso que la generen o sobre los resultados del conjunto de testing. Mostrar los resultados en alguna tabla, pueden poner aqu칤 tambi칠n los resultados obtenidos al realizar la submission. (**0.5 puntos**)\n",
    "\n",
    "8. **Conclusiones**:  Discutir resultados, proponer trabajo futuro. (**0.5 puntos**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMSn_tDYwOb1"
   },
   "source": [
    "## **Descripci칩n Baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:18:43.301002Z",
     "start_time": "2019-08-21T19:18:43.298037Z"
    },
    "id": "30fPWG5pzvIm"
   },
   "source": [
    "` `\n",
    "El baseline de la secci칩n **Experimentos** contiene un c칩digo b치sico que resuelve la task y es el modelo subido por **cc6205-baseline** a la competencia. Pueden modificar el c칩digo como quieran siempre y cuando no cambien las funciones de las m칠tricas y el formato en que se suben los archivos a la competencia, ya que ese es el formato en que realizamos la evaluaci칩n. En concretro, el baseline hace lo siguiente:\n",
    "` `\n",
    "\n",
    "- Obtiene los datasets desde el repositorio del curso.\n",
    "- Divide los datasets en train (datos que est치n etiquetados) y target set (datos no etiquetados para la competencia). Adem치s, por cada dataset en train, se divide en un conjunto de entrenamiento y uno de prueba. Aqu칤 la mejor pr치ctica ser칤a cambiar el c칩digo para obtener un conjunto de entrenamiento, validaci칩n y prueba.\n",
    "\n",
    "- Crea un Pipeline que: \n",
    "    - Crea features personalizados para la representaci칩n num칠rica.\n",
    "    - Transforma los dataset a bag of words (BoW).  \n",
    "    - Entrena un clasificador usando cada train set.\n",
    "- Clasifica y evalua el sistema creado usando el test set.\n",
    "- Clasifica el target set.\n",
    "- Genera una submission con el target en formato zip en el directorio en donde se est치 ejecutando el notebook. \n",
    "\n",
    "` `\n",
    "\n",
    "Algunas pistas sobre como mejorar el rendimiento de los sistemas que creen. (Esto tendr치 mas sentido cuando vean el c칩digo)\n",
    "\n",
    "- **Vectorizador**: Investigar los modulos de `nltk`, en particular, `TweetTokenizer`, `mark_negation` para reemplazar los tokenizadores. Tambi칠n, el par치metro `ngram_range` (Ojo que el clf naive bayes no deber칤a usarse con n-gramas, ya que rompe el supuesto de independencia). Adem치s, implementar los atributos que crean 칰tiles desde el listado del enunciado. Investigar tambi칠n el vectorizador tf-idf.\n",
    "\n",
    "- **Clasificador**: Investigar otros clasificadores m치s efectivos que naive bayes. Estos deben poder retornar la probabilidad de pertenecia de las clases (ie: implementar la funci칩n `predict_proba`).\n",
    "\n",
    "- **Features**: Recuerden que pueden implementar todas las features que se les ocurra! Aqu칤 les adjuntamos algunos ejemplos:\n",
    "    -\tWord n-grams.\n",
    "    -\tCharacter n-grams. \n",
    "    -\tPart-of-speech tags.\n",
    "    -\tSentiment Lexicons (Lexicon = A set of words with a label or associated value.).\n",
    "        - Count the number of positive and negative words within a sentence.\n",
    "        - If the lexicon has associated intensity of feeling (for example in a decimal), then take the average of the intensity of the sentence according to the feeling, the sum, etc.\n",
    "        -\tA good lexicon of sentiment: [Bing Liu](http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar) \n",
    "        - A reference with a lot of [sentiment lexicons](https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c). \n",
    "    -\tThe number of elongated words (words with one character repeated more than two times).\n",
    "    -\tThe number of words with all characters in uppercase.\n",
    "    -\tThe presence and the number of positive or negative emoticons.\n",
    "    -\tThe number of individual negations.\n",
    "    -\tThe number of contiguous sequences of dots, question marks and exclamation marks.\n",
    "    -\tWord Embeddings: Here are some good ideas on how to use them.\n",
    "    https://stats.stackexchange.com/questions/221715/apply-word-embeddings-to-entire-document-to-get-a-feature-vector\n",
    "\n",
    "- **Reducci칩n de dimensionalidad**: Tambi칠n puede serles de ayuda. Referencias [aqu칤](https://scikit-learn.org/stable/modules/unsupervised_reduction.html).\n",
    "\n",
    "- Por 칰ltimo, pueden encontrar mas referencias de c칩mo mejorar sus features, el vectorizador y el clasificador [aqu칤](https://affectivetweets.cms.waikato.ac.nz/benchmark/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IT7ZpVRuzGAF"
   },
   "source": [
    "# **Entregable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:34:25.683540Z",
     "start_time": "2020-03-31T13:34:25.673430Z"
    },
    "id": "E29LEMZ9zvIo"
   },
   "source": [
    "## **1. Introducci칩n**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W20NnoduzvIo"
   },
   "source": [
    "    Escriba su introducci칩n aqu칤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:13.474238Z",
     "start_time": "2020-03-31T13:47:13.454068Z"
    },
    "id": "OTAIEnSJzvIp"
   },
   "source": [
    "## **2. Representaciones**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:17.719268Z",
     "start_time": "2020-03-31T13:47:17.709207Z"
    },
    "id": "EV1qBv-MzvIp"
   },
   "source": [
    "Los tweets son representandos como la uni칩n de distintos features. Todos los tweets tienen una base de representaci칩n que consiste en un vector td-idf de 1-grama o 2-grama generado a partir del vocabulario de lo. Luego, a cada tweet uno o algunos de los siguientes features:\n",
    "\n",
    "- *Lexicon* : Dado un lexico que define palabras positivas y palabras negativas, se cuenta las palabras positivas y negativas que posee un tweet.\n",
    "- *Emoji Lexicon*: Dado un lexico que define emojis positivos y emojis negativos, se cuenta las emojis positivas y negativos.\n",
    "- *CharsCount* : Para cada de los s칤mbolos \\!, 춰, \\#, \\@, \\? y , se cuenta la cantidad de dicho s칤mbolo.\n",
    "- *UperCounts* : Se cuenta la cantidad de palabras en may칰sculas presentes.\n",
    "- *ElongatedWords* : Se cuenta la cantidad de palabras con elongaciones. Por ejemplo, la palabra *hoooooola* es la palabra *hola* con una elongaci칩n en la letra *o*.\n",
    "- *Polarity Scores* : Se calcula la *polaridad* en el texto. N칩tese que este feature es el 칰nico que utiliza una *herramienta* externa.\n",
    "\n",
    "La representaci칩n escogida depende del pipeline: los cuatros pipelines tienen distintas representaciones para los tweets. Lo anteriormente mencionado se realiza de dicha forma ya que como no se conoce la mejor representaci칩n, se buca generar cuatro representaciones distintas y escoger la que entregue mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMOjYSQezvIq"
   },
   "source": [
    "## **3. Algoritmos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bPiFs33zilS"
   },
   "source": [
    "Para clasificar los tweets dada las representaciones descritas en la secci칩n anterior (secci칩n 2), se utilizaron los siguientes algoritmos:\n",
    "\n",
    "- *Random Forest*: Es un algoritmo de clasificaci칩n que consiste en crear variados *Decision trees*, clasificar el input en cada 치rbol de decisi칩n y, dado los resultados obtenidos a partir de las distintas clasifaciones con los 치rboles de decisiones creados, elegir la clasificaci칩n m치s *com칰n*, es decir, se escoge la clase m치s predecida por los 치rboles de decisi칩n.\n",
    "- *Logistic Regression*: M칠todo estad칤stico para clasificar objetos. M치s en particular, es un m칠todo de clasificaci칩n lineal que genera un resultado binario (el objeto pertenece o no a la clase). Sin embargo, permite generar predicciones para problemas multiclase.\n",
    "\n",
    "Del mismo modo que las representaciones, el algoritmo escogido depende del pipeline: los cuatros pipelines tienen distintos algoritmos para los tweets. Lo anteriormente mencionado se realiza de dicha forma ya que como no se conoce el mejor algoritmo para la clasificaciones, se buca generar cuatro combinaciones de representaciones y algoritmos de representaci칩n y escoger la que entregue mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:52.064631Z",
     "start_time": "2020-03-31T13:47:52.044451Z"
    },
    "id": "ECjkdgdwzvIq"
   },
   "source": [
    "## **4. M칠tricas de Evaluaci칩n**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6eHJdHBzvIr"
   },
   "source": [
    "- AUC: ...\n",
    "- Kappa: ...\n",
    "- Accuracy: ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJyTrr2onLOo"
   },
   "source": [
    "## **5. Dise침o experimental**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnhEwjh_nP9M"
   },
   "source": [
    "El dise침o experimental sigue el baseline dado en la competencia. Los pasos realizados son los siguientes:\n",
    "- Se importan las librerias necesarias. Se incluyeron librer칤as y m칠todos que no estaban presente en el baseline com\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OX5Ib_pCzvIr"
   },
   "source": [
    "## **6. Experimentos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:31:40.023344Z",
     "start_time": "2020-03-31T13:31:40.003541Z"
    },
    "id": "aK24MJ8jzvIr"
   },
   "source": [
    "### Importar librer칤as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:20.587160Z",
     "start_time": "2020-04-07T15:44:19.319386Z"
    },
    "id": "FukgFUTUzvIs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /home/tridimensional/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/tridimensional/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/tridimensional/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "nltk.download('opinion_lexicon')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import opinion_lexicon, stopwords\n",
    "from nltk import TweetTokenizer, download\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FevBPus0zvIs"
   },
   "source": [
    "### Definir m칠todos de evaluaci칩n (**NO tocar este c칩digo**)\n",
    "\n",
    "Estas funciones est치n a cargo de evaluar los resultados de la tarea. No deber칤an cambiarlas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:20.604066Z",
     "start_time": "2020-04-07T15:44:20.589106Z"
    },
    "id": "9wlllV7PzvIs"
   },
   "outputs": [],
   "source": [
    "def auc_score(test_set, predicted_set):\n",
    "    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
    "    medium_predicted = np.array(\n",
    "        [prediction[1] for prediction in predicted_set])\n",
    "    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
    "    high_test = np.where(test_set == 'high', 1.0, 0.0)\n",
    "    medium_test = np.where(test_set == 'medium', 1.0, 0.0)\n",
    "    low_test = np.where(test_set == 'low', 1.0, 0.0)\n",
    "    auc_high = roc_auc_score(high_test, high_predicted)\n",
    "    auc_med = roc_auc_score(medium_test, medium_predicted)\n",
    "    auc_low = roc_auc_score(low_test, low_predicted)\n",
    "    auc_w = (low_test.sum() * auc_low + medium_test.sum() * auc_med +\n",
    "             high_test.sum() * auc_high) / (\n",
    "                 low_test.sum() + medium_test.sum() + high_test.sum())\n",
    "    return auc_w\n",
    "\n",
    "\n",
    "def evaluate(predicted_probabilities, y_test, labels, dataset_name):\n",
    "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
    "    # entregar el arreglo de clases aprendido por el clasificador.\n",
    "    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n",
    "    predicted_labels = [\n",
    "        labels[np.argmax(item)] for item in predicted_probabilities\n",
    "    ]\n",
    "\n",
    "    print('Confusion Matrix for {}:\\n'.format(dataset_name))\n",
    "    print(\n",
    "        confusion_matrix(y_test,\n",
    "                         predicted_labels,\n",
    "                         labels=['low', 'medium', 'high']))\n",
    "\n",
    "    print('\\nClassification Report:\\n')\n",
    "    print(\n",
    "        classification_report(y_test,\n",
    "                              predicted_labels,\n",
    "                              labels=['low', 'medium', 'high']))\n",
    "    # Reorder predicted probabilities array.\n",
    "    labels = labels.tolist()\n",
    "    \n",
    "    predicted_probabilities = predicted_probabilities[:, [\n",
    "        labels.index('low'),\n",
    "        labels.index('medium'),\n",
    "        labels.index('high')\n",
    "    ]]\n",
    "    \n",
    "    \n",
    "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
    "    print(\"Scores:\\n\\nAUC: \", auc, end='\\t')\n",
    "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
    "    print(\"Kappa:\", kappa, end='\\t')\n",
    "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print('------------------------------------------------------\\n')\n",
    "    return np.array([auc, kappa, accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkOP6ugwzvIt"
   },
   "source": [
    "### Datos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.068137Z",
     "start_time": "2020-04-07T15:44:20.606061Z"
    },
    "id": "D1XhFPhrzvIt"
   },
   "outputs": [],
   "source": [
    "# Datasets de entrenamiento.\n",
    "train = {\n",
    "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/anger-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/fear-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/joy-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/sadness-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'])\n",
    "}\n",
    "# Datasets que deber치n predecir para la competencia.\n",
    "target = {\n",
    "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/anger-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/fear-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/joy-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/sadness-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.088707Z",
     "start_time": "2020-04-07T15:44:21.069757Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1647890199751,
     "user": {
      "displayName": "Gabriel Iturra",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghe-mqbWbrqQ1gVhaBhwEAK5Uu5cfHEnENzwLJUGA=s64",
      "userId": "02319919045117626989"
     },
     "user_tz": 180
    },
    "id": "flg2Zw2mzvIt",
    "outputId": "1444867d-5d1f-4ba8-de05-6e13a5c1e113"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>sentiment_intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>10109</td>\n",
       "      <td>I swear if @devincameron23 blocks me I'm going...</td>\n",
       "      <td>anger</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10018</td>\n",
       "      <td>why are people so offended by kendall he ends ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>10470</td>\n",
       "      <td>British humour should offend and challenge mai...</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>10522</td>\n",
       "      <td>Since the 'update' my @iPhone loses power near...</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>@DPD_UK I asked for my parcel to be delivered ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet  class  \\\n",
       "109  10109  I swear if @devincameron23 blocks me I'm going...  anger   \n",
       "18   10018  why are people so offended by kendall he ends ...  anger   \n",
       "470  10470  British humour should offend and challenge mai...  anger   \n",
       "522  10522  Since the 'update' my @iPhone loses power near...  anger   \n",
       "2    10002  @DPD_UK I asked for my parcel to be delivered ...  anger   \n",
       "\n",
       "    sentiment_intensity  \n",
       "109                high  \n",
       "18                 high  \n",
       "470              medium  \n",
       "522              medium  \n",
       "2                  high  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de algunas filas aleatorias del dataset etiquetado:\n",
    "train['anger'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1647890201421,
     "user": {
      "displayName": "Gabriel Iturra",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghe-mqbWbrqQ1gVhaBhwEAK5Uu5cfHEnENzwLJUGA=s64",
      "userId": "02319919045117626989"
     },
     "user_tz": 180
    },
    "id": "XB7hb7KH2DFK",
    "outputId": "4c3397ce-471d-45d8-a944-fcde1abdf2cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>sentiment_intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>11683</td>\n",
       "      <td>Like he really just fucking asked me that. #of...</td>\n",
       "      <td>anger</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>11215</td>\n",
       "      <td>13 hour @bus rides make me  #sorry</td>\n",
       "      <td>anger</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>11511</td>\n",
       "      <td>How on earth can the projection of all that is...</td>\n",
       "      <td>anger</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>11232</td>\n",
       "      <td>@Thatguy_dree @RecklessWonder_ neither one of ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>11647</td>\n",
       "      <td>You insult the late Sherri Martel</td>\n",
       "      <td>anger</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet  class  \\\n",
       "742  11683  Like he really just fucking asked me that. #of...  anger   \n",
       "274  11215                 13 hour @bus rides make me  #sorry  anger   \n",
       "570  11511  How on earth can the projection of all that is...  anger   \n",
       "291  11232  @Thatguy_dree @RecklessWonder_ neither one of ...  anger   \n",
       "706  11647                  You insult the late Sherri Martel  anger   \n",
       "\n",
       "     sentiment_intensity  \n",
       "742                  NaN  \n",
       "274                  NaN  \n",
       "570                  NaN  \n",
       "291                  NaN  \n",
       "706                  NaN  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de algunas filas aleatorias del dataset no etiquetado\n",
    "target['anger'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5aNqEfVzvIv"
   },
   "source": [
    "### Analizar los datos \n",
    "\n",
    "En esta secci칩n analizaremos el balance de los datos. Para ello se imprime la cantidad de tweets de cada dataset agrupados por la intensidad de sentimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.117633Z",
     "start_time": "2020-04-07T15:44:21.090703Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1647890204981,
     "user": {
      "displayName": "Gabriel Iturra",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghe-mqbWbrqQ1gVhaBhwEAK5Uu5cfHEnENzwLJUGA=s64",
      "userId": "02319919045117626989"
     },
     "user_tz": 180
    },
    "id": "u5007JRgzvIv",
    "outputId": "f8af2406-ea94-4716-9414-1b6d7e2450a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: anger \n",
      " sentiment_intensity\n",
      "high      163\n",
      "low       161\n",
      "medium    617\n",
      "dtype: int64\n",
      "----------------------------------------------------------\n",
      "\n",
      "Dataset: fear \n",
      " sentiment_intensity\n",
      "high      270\n",
      "low       288\n",
      "medium    699\n",
      "dtype: int64\n",
      "----------------------------------------------------------\n",
      "\n",
      "Dataset: joy \n",
      " sentiment_intensity\n",
      "high      195\n",
      "low       219\n",
      "medium    488\n",
      "dtype: int64\n",
      "----------------------------------------------------------\n",
      "\n",
      "Dataset: sadness \n",
      " sentiment_intensity\n",
      "high      197\n",
      "low       210\n",
      "medium    453\n",
      "dtype: int64\n",
      "----------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in train:\n",
    "    print(f'Dataset: {dataset_name} \\n', train[dataset_name].groupby(['sentiment_intensity']).size())\n",
    "    print('----------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stZ6ig5hzvIv"
   },
   "source": [
    "### Custom Features \n",
    "\n",
    "Para crear features personalizadas implementaremos nuestros propios Transformers (estandar de scikit para crear nuevas features entre otras cosas). Para esto:\n",
    "\n",
    "1. Creamos nuestra clase Transformer extendiendo BaseEstimator y TransformerMixin. En este ejemplo, definiremos `CharsCountTransformer` que cuenta caracteres relevantes ('!', '?', '#', '@') en los tweets.\n",
    "2. Definimos una funci칩n c칩mo `get_relevant_chars` que opera por cada tweet y retorna un arreglo.\n",
    "3. Hacemos un override de la funci칩n `transform` en donde iteramos por cada tweet, llamamos a la funci칩n que hicimos antes y agregamos sus resultados a un arreglo. Finalmente lo retornamos.\n",
    "\n",
    "Esto nos facilitar치 el trabajo mas adelante. Una Guia completa de las transformaciones predefinidas en scikit pueden encontrarla [aqu칤](https://scikit-learn.org/stable/data_transforms.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.128600Z",
     "start_time": "2020-04-07T15:44:21.119624Z"
    },
    "id": "tNPB8zc9zvIw"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "    def tokenizer_tweet(self, tweet):\n",
    "        return self.tweet_tokenizer.tokenize(tweet)\n",
    "    \n",
    "    def tokenizer_with_stemming_doc(self, doc):\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        doc_tokenize = list(map(lambda x: self.tweet_tokenizer.tokenize(x), doc))\n",
    "        doc_tokenize = reduce(lambda x,y: x + y, doc_tokenize, [])\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tweet = list(filter(lambda x: x not in stop_words, doc_tokenize))\n",
    "        return [stemmer.stem(word) for word in doc_tokenize]\n",
    "    \n",
    "    def tokenizer_with_lemmatization_tweet(self, tweet):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tweet = self.tweet_tokenizer.tokenize(tweet)\n",
    "        tweet = reduce(lambda l, el: l + re.split(r\"(\\w+)\", el), tweet, [])\n",
    "        tweet = list(filter(lambda x: x != '', tweet))\n",
    "        \n",
    "        return [lemmatizer.lemmatize(word) for word in tweet]\n",
    "\n",
    "\n",
    "class Feature(ABC):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = CustomTokenizer()\n",
    "        \n",
    "    @abstractmethod\n",
    "    def get_feature(self, tweet: list):\n",
    "        pass\n",
    "\n",
    "class Polarity(Feature):\n",
    "    # override\n",
    "    def get_feature(self, tweet: str):\n",
    "        values = list(SentimentIntensityAnalyzer().polarity_scores(tweet).values())\n",
    "        values = list(map(lambda x: x if x > 0 else 0, values))\n",
    "        return list(map(lambda x: int(x*10), values))\n",
    "    \n",
    "                    \n",
    "class Lexicon(Feature):\n",
    "    # override\n",
    "    def get_feature(self, tweet: str):\n",
    "        negative_words = set(opinion_lexicon.negative())\n",
    "        positive_words = set(opinion_lexicon.positive())\n",
    "        \n",
    "        negatives = 0\n",
    "        positives = 0\n",
    "        \n",
    "        tweet = self.tokenizer.tokenizer_with_lemmatization_tweet(tweet)\n",
    "        for word in tweet:\n",
    "            if word in negative_words:\n",
    "                negatives += 1\n",
    "            if word in positive_words:\n",
    "                positives += 1\n",
    "\n",
    "        return [positives, negatives]\n",
    "\n",
    "\n",
    "class ElongatedWords(Feature):\n",
    "    # override\n",
    "    def get_feature(self, tweet: str):\n",
    "\n",
    "        elongated_words = 0\n",
    "        tweet = self.tokenizer.tokenizer_tweet(tweet)\n",
    "        for word in tweet:\n",
    "            current_letter = word[0]\n",
    "            count = -1\n",
    "            for letter in word:\n",
    "                if letter == current_letter:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    if count >= 4:\n",
    "                        elongated_words += 1\n",
    "                        current_letter = letter\n",
    "                        count = 0\n",
    "\n",
    "        return [elongated_words]\n",
    "\n",
    "    \n",
    "class EmojiLexicon(Feature):\n",
    "    def __init__(self):\n",
    "        self.df = pd.read_csv(\n",
    "            \"emoji-sentiment-ranking/emoji-sentiment-data.csv\",\n",
    "            index_col='Emoji'\n",
    "        )\n",
    "        self.df['sentiment-score'] = (self.df.Positive - self.df.Negative) / self.df.Occurrences\n",
    "        self.df['neut-score'] = (self.df.Neutral) / self.df.Occurrences\n",
    "        self.df = self.df.loc[self.df['neut-score'] <= 0.3]\n",
    "        \n",
    "    def get_feature(self, tweet: str):\n",
    "        positives = 0\n",
    "        negatives = 0\n",
    "        positive_score = 0\n",
    "        negative_score = 0\n",
    "        for word in tweet:\n",
    "            if word in self.df.index:\n",
    "                score = self.df.loc[word, 'sentiment-score']\n",
    "                positives += 1 if score >= 0.01 else 0\n",
    "                negatives += 1 if score <= -0.01 else 0\n",
    "                positive_score += score if score >= 0.01 else 0\n",
    "                negative_score += abs(score) if score <= -0.01 else 0\n",
    "        return [positives, negatives, positive_score, negative_score]\n",
    "\n",
    "class CharsCount(Feature):\n",
    "    # override\n",
    "    def get_feature(self, tweet: str):\n",
    "        chars = ['#', '!', '춰', '?', '', '@', '.', '.']\n",
    "        return list(map(lambda x: tweet.count(x), chars))\n",
    "\n",
    "\n",
    "class UpperCount(Feature):\n",
    "    # override\n",
    "    def get_feature(self, tweet: str):\n",
    "        \n",
    "        tweet = self.tokenizer.tokenizer_tweet(tweet)\n",
    "        upper_count = 0\n",
    "        for word in tweet:\n",
    "            word_list = list(map(lambda x: x.isupper(), word))\n",
    "            word_list = list(map(lambda x: 1 if x else 0, word_list))\n",
    "            upper_count = reduce(lambda x,y: x+y, word_list, 0)\n",
    "        return [upper_count]\n",
    "\n",
    "\n",
    "class CustomTransformer(ABC, BaseEstimator, TransformerMixin):\n",
    "    def transform(self, X, y=None):\n",
    "        chars = []\n",
    "\n",
    "        for tweet in X:\n",
    "            features = []\n",
    "\n",
    "            for class_ in self.classes:\n",
    "                feature = class_().get_feature(tweet)\n",
    "                features += feature\n",
    "\n",
    "            chars.append(features)\n",
    "\n",
    "        return np.array(chars)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "class FirstTransformer(CustomTransformer):\n",
    "    \"\"\"\n",
    "    Custom non verbal features\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.classes = [\n",
    "            ElongatedWords,\n",
    "            CharsCount, \n",
    "            UpperCount,\n",
    "            EmojiLexicon\n",
    "        ]\n",
    "\n",
    "class SecondTransformer(CustomTransformer):\n",
    "    \"\"\"\n",
    "    Custom verbal features\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.classes = [\n",
    "            Lexicon,\n",
    "            Polarity\n",
    "        ]\n",
    "\n",
    "class ThirdTransformer(CustomTransformer):\n",
    "    \"\"\"\n",
    "    All custom features\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.classes = [\n",
    "            Lexicon,\n",
    "            ElongatedWords,\n",
    "            CharsCount, \n",
    "            UpperCount,\n",
    "            Sentiments,\n",
    "            EmojiLexicon\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.145564Z",
     "start_time": "2020-04-07T15:44:21.131593Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1647890207937,
     "user": {
      "displayName": "Gabriel Iturra",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghe-mqbWbrqQ1gVhaBhwEAK5Uu5cfHEnENzwLJUGA=s64",
      "userId": "02319919045117626989"
     },
     "user_tz": 180
    },
    "id": "lCQzsuAbzvIw",
    "outputId": "6c10a7d9-6289-445e-aa2d-d61a08d4f0ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet original: @cineworld 'Congratulations your Free 1 month has been activated' Then charges 춲34.80 the same month. Absolutely furious 游땨\n",
      "Features creados: [0.         0.         0.         0.         0.         0.\n",
      " 1.         2.         2.         0.         0.         1.\n",
      " 0.         0.17328042]\n"
     ]
    }
   ],
   "source": [
    "# Veamos que sucede si ejecutamos el transformer\n",
    "sample = train['anger'].sample(5, random_state = 22).tweet\n",
    "sample_features = FirstTransformer().transform(sample)\n",
    "\n",
    "# Se puede verificar que el conteo de s칤mbolos es consistente con el transformer creado.\n",
    "print(f'Tweet original: {sample.iloc[0]}')\n",
    "print(f'Features creados: {sample_features[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MO_yIepczvIx"
   },
   "source": [
    "### Definir la representaci칩n y el clasificador\n",
    "\n",
    "Para esto, definiremos Pipelines. Un `Pipeline` es una lista de transformaciones y un estimador(clasificador) ubicado al final el cual define el flujo que seguiran nuestros datos dentro del sistema que creemos. Nos permite ejecutar facilmente el mismo proceso sobre todos los datasets que usemos, simplificando as칤 nuestra programaci칩n.\n",
    "\n",
    "El pipeline m치s b치sico que podemos hacer es transformar el dataset a Bag of Words y despu칠s usar clasificar el BoW usando NaiveBayes:\n",
    "\n",
    "```python\n",
    "    Pipeline([('bow', CountVectorizer()), ('clf', MultinomialNB())])\n",
    "```\n",
    "\n",
    "\n",
    "Ahora, si queremos usar nuestra transformaci칩n para agregar las features que creamos, usaremos `FeatureUnion`. Esta simplemente concatenar치 los vectores resultantes de ejecutar BoW y los Transformer en un solo vector.\n",
    "\n",
    "```python\n",
    "    Pipeline([('features',FeatureUnion([('bow', CountVectorizer()),\n",
    "                                        ('chars_count',CharsCountTransformer())])),\n",
    "              ('clf', MultinomialNB())])\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDbHjXv-zvIx"
   },
   "source": [
    "Recuerden que cada pipeline representa un sistema de clasificaci칩n distinto. Por lo mismo, deben instanciar uno por cada problema que resuelvan. De lo contrario, podr칤an solapar resultados.  Para esto, les recomendamos crear los pipeline en distintas funciones, como la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.155528Z",
     "start_time": "2020-04-07T15:44:21.149545Z"
    },
    "id": "z_R6tyMCzvIy"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_experiment_1_pipeline():\n",
    "    \"\"\"\n",
    "    Non verbal features\n",
    "    \"\"\"\n",
    "    features = (\n",
    "        'features', \n",
    "        FeatureUnion([\n",
    "            ('tfidf', TfidfVectorizer(tokenizer=CustomTokenizer().tokenizer_with_stemming_doc)),\n",
    "            ('non_verbal', FirstTransformer())\n",
    "        ])\n",
    "    )\n",
    "    clf = ('clf', RandomForestClassifier())\n",
    "    \n",
    "    return Pipeline([features, clf])\n",
    "\n",
    "def get_experiment_2_pipeline():\n",
    "    \"\"\"\n",
    "    Verbal features\n",
    "    \"\"\"\n",
    "    features = (\n",
    "        'features', \n",
    "        FeatureUnion([\n",
    "            ('tfidf', TfidfVectorizer(tokenizer=CustomTokenizer().tokenizer_with_stemming_doc, ngram_range=(1,2))),\n",
    "            ('verbal_features', SecondTransformer())\n",
    "        ])\n",
    "    )\n",
    "    clf = ('clf', LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 10000))\n",
    "\n",
    "    return Pipeline([features, clf])\n",
    "\n",
    "def get_experiment_3_pipeline():\n",
    "    \"\"\"\n",
    "    All features\n",
    "    \"\"\"\n",
    "    features = (\n",
    "        'features', \n",
    "        FeatureUnion([\n",
    "            ('tfidf', TfidfVectorizer(tokenizer=CustomTokenizer().tokenizer_with_stemming_doc)),\n",
    "            ('all_features', ThirdTransformer())\n",
    "        ])\n",
    "    )\n",
    "    clf = ('clf', RandomForestClassifier())\n",
    "    return Pipeline([features, clf])\n",
    "            \n",
    "def get_experiment_4_pipeline():\n",
    "    \"\"\"\n",
    "    All features\n",
    "    \"\"\"\n",
    "    features = (\n",
    "        'features', \n",
    "        FeatureUnion([\n",
    "            ('tfidf', TfidfVectorizer(tokenizer=CustomTokenizer().tokenizer_with_stemming_doc, ngram_range=(1,2))),\n",
    "            ('all_features', ThirdTransformer())\n",
    "        ])\n",
    "    )\n",
    "    clf = ('clf', LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 10000))\n",
    "    return Pipeline([features, clf])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmMdm98vzvIy"
   },
   "source": [
    "### Ejecutar el pipeline para alg칰n dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.167498Z",
     "start_time": "2020-04-07T15:44:21.157540Z"
    },
    "id": "_eX0cEu-zvIz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run(dataset, dataset_name, pipeline):\n",
    "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
    "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n",
    "\n",
    "    # Dividimos el dataset en train y test, a칰n no se transforma de Strings a valores num칠ricos.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataset.tweet,\n",
    "        dataset.sentiment_intensity,\n",
    "        shuffle=True,\n",
    "        test_size=0.33)\n",
    "    \n",
    "    print(f'# Datos de entrenamiento en dataset {dataset_name}: {len(X_train)}')\n",
    "    print(f'# Datos de testing en dataset {dataset_name}: {len(X_test)}')\n",
    "\n",
    "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline). \n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
    "    predicted_probabilities = pipeline.predict_proba(X_test)\n",
    "\n",
    "    # Obtenemos el orden de las clases aprendidas.\n",
    "    learned_labels = pipeline.classes_\n",
    "    \n",
    "    # Evaluamos:\n",
    "    scores = evaluate(predicted_probabilities, y_test, learned_labels, dataset_name)\n",
    "    return pipeline, learned_labels, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z96C1ZOfzvIz"
   },
   "source": [
    "### Ejecutar el sistema creado por cada train set\n",
    "\n",
    "Este c칩digo crea y entrena los 4 sistemas de clasificaci칩n y luego los evalua. Para los experimentos, pueden copiar este c칩digo variando el pipeline cuantas veces estimen conveniente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.384119Z",
     "start_time": "2020-04-07T15:44:21.170488Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 629,
     "status": "ok",
     "timestamp": 1647890223742,
     "user": {
      "displayName": "Gabriel Iturra",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghe-mqbWbrqQ1gVhaBhwEAK5Uu5cfHEnENzwLJUGA=s64",
      "userId": "02319919045117626989"
     },
     "user_tz": 180
    },
    "id": "OXAxZBdVzvI0",
    "outputId": "dfc387b4-3121-43f4-d892-eca1ee100367",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados pipeline: 1\n",
      "\n",
      "# Datos de entrenamiento en dataset anger: 630\n",
      "# Datos de testing en dataset anger: 311\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  2  63   0]\n",
      " [  3 187   2]\n",
      " [  0  49   5]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.40      0.03      0.06        65\n",
      "      medium       0.63      0.97      0.76       192\n",
      "        high       0.71      0.09      0.16        54\n",
      "\n",
      "    accuracy                           0.62       311\n",
      "   macro avg       0.58      0.37      0.33       311\n",
      "weighted avg       0.59      0.62      0.51       311\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.683\tKappa: 0.058\tAccuracy: 0.624\n",
      "------------------------------------------------------\n",
      "\n",
      "# Datos de entrenamiento en dataset fear: 842\n",
      "# Datos de testing en dataset fear: 415\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 32  52   1]\n",
      " [ 19 215   5]\n",
      " [  2  80   9]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.60      0.38      0.46        85\n",
      "      medium       0.62      0.90      0.73       239\n",
      "        high       0.60      0.10      0.17        91\n",
      "\n",
      "    accuracy                           0.62       415\n",
      "   macro avg       0.61      0.46      0.46       415\n",
      "weighted avg       0.61      0.62      0.55       415\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.67\tKappa: 0.209\tAccuracy: 0.617\n",
      "------------------------------------------------------\n",
      "\n",
      "# Datos de entrenamiento en dataset joy: 604\n",
      "# Datos de testing en dataset joy: 298\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[ 37  32   1]\n",
      " [ 27 138   5]\n",
      " [  2  35  21]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.56      0.53      0.54        70\n",
      "      medium       0.67      0.81      0.74       170\n",
      "        high       0.78      0.36      0.49        58\n",
      "\n",
      "    accuracy                           0.66       298\n",
      "   macro avg       0.67      0.57      0.59       298\n",
      "weighted avg       0.67      0.66      0.64       298\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.746\tKappa: 0.364\tAccuracy: 0.658\n",
      "------------------------------------------------------\n",
      "\n",
      "# Datos de entrenamiento en dataset sadness: 576\n",
      "# Datos de testing en dataset sadness: 284\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[ 26  38   0]\n",
      " [ 17 127   5]\n",
      " [  6  48  17]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.53      0.41      0.46        64\n",
      "      medium       0.60      0.85      0.70       149\n",
      "        high       0.77      0.24      0.37        71\n",
      "\n",
      "    accuracy                           0.60       284\n",
      "   macro avg       0.63      0.50      0.51       284\n",
      "weighted avg       0.63      0.60      0.56       284\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.703\tKappa: 0.268\tAccuracy: 0.599\n",
      "------------------------------------------------------\n",
      "\n",
      "Average scores:\n",
      "\n",
      " Average AUC: 0.701\t Average Kappa: 0.225\t Average Accuracy: 0.625\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_pipelines = [\n",
    "    # get_experiment_1_pipeline,\n",
    "    # get_experiment_2_pipeline,\n",
    "    # get_experiment_3_pipeline,\n",
    "    get_experiment_4_pipeline\n",
    "]\n",
    "\n",
    "classifiers = []\n",
    "learned_labels_array = []\n",
    "for i, f_pipeline in enumerate(f_pipelines):\n",
    "    print(f\"Resultados pipeline: {i+1}\\n\")\n",
    "    scores_array = []\n",
    "    # Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
    "    for dataset_name, dataset in train.items():\n",
    "        # creamos el pipeline\n",
    "        pipeline = f_pipeline()\n",
    "        \n",
    "        # ejecutamos el pipeline sobre el dataset\n",
    "        classifier, learned_labels, scores = run(dataset, dataset_name, pipeline)\n",
    "        # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
    "        classifiers.append(classifier)\n",
    "        \n",
    "        # guardamos las labels aprendidas por el clasificador\n",
    "        learned_labels_array.append(learned_labels)\n",
    "        \n",
    "        # guardamos los scores obtenidos\n",
    "        scores_array.append(scores)\n",
    "    # print avg scores\n",
    "    print(\n",
    "        \"Average scores:\\n\\n\",\n",
    "        \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\\n\\n\"\n",
    "        .format(*np.array(scores_array).mean(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:37:43.169737Z",
     "start_time": "2019-08-21T19:37:43.166744Z"
    },
    "id": "IUKwcde_zvI0"
   },
   "source": [
    "### Predecir los target set y crear la submission\n",
    "\n",
    "Aqu칤 predecimos los target set usando los clasificadores creados y creamos los archivos de las submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.392097Z",
     "start_time": "2020-04-07T15:44:21.386114Z"
    },
    "id": "mWDUoSmbzvI1"
   },
   "outputs": [],
   "source": [
    "def predict_target(dataset, classifier, labels):\n",
    "    # Predecir las probabilidades de intensidad de cada elemento del target set.\n",
    "    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n",
    "    \n",
    "    # Agregar ids\n",
    "    predicted['id'] = dataset.id.values\n",
    "    # Reordenar las columnas\n",
    "    predicted = predicted[['id', 'low', 'medium', 'high']]\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.588573Z",
     "start_time": "2020-04-07T15:44:21.394094Z"
    },
    "id": "5CJ4PTwZzvI1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_target = {}\n",
    "\n",
    "# Crear carpeta ./predictions\n",
    "if (not os.path.exists('./predictions')):\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "else:\n",
    "    # Eliminar predicciones anteriores:\n",
    "    shutil.rmtree('./predictions')\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "# por cada target set:\n",
    "for idx, key in enumerate(target):\n",
    "    # Predecirlo\n",
    "    predicted_target[key] = predict_target(target[key], classifiers[idx],\n",
    "                                           learned_labels_array[idx])\n",
    "    # Guardar predicciones en archivos separados. \n",
    "    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n",
    "                                 sep='\\t',\n",
    "                                 header=False,\n",
    "                                 index=False)\n",
    "\n",
    "# Crear archivo zip\n",
    "a = shutil.make_archive('predictions', 'zip', './predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCAyJIj8nTlU"
   },
   "source": [
    "## **7. Resultados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAYwupS3naYX"
   },
   "source": [
    "    Muestre y analice sus resultados aqu칤\n",
    "\n",
    "A continuaci칩n una tabla de ejemplo para mostrar los resultados.  En este caso s칩lo est치 el experimento del baseline m치s otro clasificador, pero ustedes debiesen generar una mayor cantidad de experimentos. \n",
    "\n",
    "| No. | Approach                       || Dataset   | AUC   | Kappa | Accuracy |\n",
    "|-----|--------------------------------||-----------|-------|-------|----------|\n",
    "|     | Features        | Clasifier     |           |       |       |          |\n",
    "| 0   | bow+chars_count | MultinomialNB | anger     | 0.622 | 0.163 | 0.688    |\n",
    "|     |                 |               | fear      | 0.597 | 0.091 | 0.559    |\n",
    "|     |                 |               | joy       | 0.728 | 0.251 | 0.601    |\n",
    "|     |                 |               | sadness   | 0.645 | 0.166 | 0.581    |\n",
    "|     |                 |               |**average**| 0.648 | 0.168 | 0.607    |\n",
    "| 1   | tus features    | tu clasifier  | anger     |       |       |          |\n",
    "|     |                 |               | fear      |       |       |          |\n",
    "|     |                 |               | joy       |       |       |          |\n",
    "|     |                 |               | sadness   |       |       |          |\n",
    "|     |                 |               |**average**|       |       |          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sqlew0iizvI1"
   },
   "source": [
    "## **8. Conclusiones**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFTikGbszZuc"
   },
   "source": [
    "    Escriba aqu칤 sus conclusiones"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "mxlNrNf_p0ZY",
    "N6lhhfl2zvIk",
    "a6moqxkEwCe-",
    "LMSn_tDYwOb1",
    "E29LEMZ9zvIo",
    "OTAIEnSJzvIp",
    "kMOjYSQezvIq",
    "ECjkdgdwzvIq",
    "RkOP6ugwzvIt",
    "q5aNqEfVzvIv",
    "stZ6ig5hzvIv",
    "gmMdm98vzvIy"
   ],
   "name": "Assignment_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
