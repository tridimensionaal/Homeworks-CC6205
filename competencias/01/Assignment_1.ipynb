{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:49:08.174519Z",
     "start_time": "2020-03-31T13:49:08.165989Z"
    },
    "id": "gpbvNOH0zvIi"
   },
   "source": [
    "# **Competencia 1 - CC6205 Natural Language Processing **\n",
    "\n",
    "**Integrantes:** Nicolas Lemu帽ir y Mat铆as Seda\n",
    "\n",
    "**Usuario del equipo en CodaLab:**\n",
    "\n",
    "**Fecha l铆mite de entrega :** Jueves 14 de Abril.\n",
    "\n",
    "**Tiempo estimado de dedicaci贸n:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxlNrNf_p0ZY"
   },
   "source": [
    "## **Objetivo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrtvsKf2p3A4"
   },
   "source": [
    "En esta tarea grupal participar谩n de una competencia estilo [Kaggle](https://www.kaggle.com/) pero utilizando la p谩gina [CodaLab](https://codalab.org/). El objetivo es trabajar en la clasificaci贸n de tweets  seg煤n su intensidad de emoci贸n, esto corresponde a la task de clasificaci贸n de texto. \n",
    "\n",
    "Tendr谩n a su disposici贸n 4 datasets de tweets con distintas emociones: `anger`, `fear`, `sadness` y `joy`. Deber谩n crear un clasificador para cada uno de estos datasets que indique la intensidad de dicha emoci贸n en sus tweets (`low`, `medium`, `high`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6lhhfl2zvIk"
   },
   "source": [
    "## **Instrucciones**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T14:34:38.796217Z",
     "start_time": "2020-04-07T14:34:38.782255Z"
    },
    "id": "7wTyult1zvIl"
   },
   "source": [
    "- La competencia consiste en resolver 4 problemas de clasificaci贸n **distintos**, cada uno con tres clases posibles. Por cada problema deber谩n crear un clasificador distinto. La evaluaci贸n de la competencia se realiza en base a 3 m茅tricas: `AUC`, `Kappa` y `Accuracy`. Los mejores puntajes en cada una de estas m茅tricas ser谩n quienes ganen la competencia.\n",
    "\n",
    "- Para comenzar se les entregar谩 en este notebook la estructura del informe a entregar y el c贸digo de un baseline con el cu谩l pueden comparar los resultados de sus experimentos. Este baseline consiste en la creaci贸n de features y una clasificaci贸n simple siguiendo el paradigma de Machine Learning emp铆rico. Los puntajes obtenidos por el baseline los pueden visualizar en el link de CodaLab buscando al usuario **cc6205**. Esperamos que los superen f谩cilmente \n",
    "\n",
    "- Para participar, deben registrarse en CodaLab y luego ingresar a la competencia usando el siguiente [link](https://competitions.codalab.org/competitions/30330?secret_key=b2ed0be7-bf23-42a1-9400-f85fa1b7bae7). \n",
    "\n",
    "- Est谩 permitido hacer grupos de m谩ximo 4 alumnos. Cada grupo debe tener un nombre de equipo, para ello en CodaLab pueden dirigirse a settings y luego cambiar el Team Name. S贸lo una persona debe administrar la cuenta del grupo y se verificar谩 que no se hayan creado m煤ltiples cuentas por grupo.\n",
    "\n",
    "- En total pueden hacer un **m谩ximo de 4 submissions**, hagan muchos experimentos probando en el conjunto de test antes de realizar el env铆o.\n",
    "\n",
    "- Es importante que hagan varios experimentos incorporando t茅cnicas como [cross-validation](https://es.wikipedia.org/wiki/Validaci%C3%B3n_cruzada#:~:text=La%20validaci%C3%B3n%20cruzada%20o%20cross,datos%20de%20entrenamiento%20y%20prueba.) o [random sampling](https://towardsdatascience.com/the-5-sampling-algorithms-every-data-scientist-need-to-know-43c7bc11d17c) antes de enviar sus predicciones a CodaLab, ya que les puede dar un mejor indicio del nivel de generalizaci贸n de sus modelos. \n",
    "\n",
    "- Aseg煤rense que la distribuci贸n de las clases sea balanceada en las particiones de training y testing debido a que existe un desbalanceo. \n",
    "\n",
    "- Verificar que el formato de la submission coincida con el de la competencia. De lo contrario, se les ser谩 evaluado incorrectamente ya que el Script de evaluaci贸n espera como input dicho formato. En el c贸digo de las m茅tricas pueden verificar c贸mo son los inputs.\n",
    "\n",
    "- No se limiten a los contenidos vistos ni a scikit ni a este baseline. No tienen restricciones entre utilizar Deep Learning o Machine Learning emp铆rico. Si reutilizan gran cantidad de c贸digo de alguna p谩gina por favor mostrar la referencia en su c贸digo. 隆Usen todo su conocimiento e ingenio en mejorar sus sistemas para poder ganar la competencia! \n",
    "\n",
    "- **Es requisito entregar el reporte con el c贸digo y haber participado en la competencia para ser evaluado. Un c贸digo sin reporte o un reporte sin c贸digo ser谩n evaluados con la nota m铆nima.**\n",
    "\n",
    "- Todas las dudas escr铆banlas en el canal de Discord de competencias. Los emails que lleguen al equipo docente ser谩n remitidos a ese medio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:25:19.677190Z",
     "start_time": "2020-04-07T15:25:19.671206Z"
    },
    "id": "jiDISxa-zvIn"
   },
   "source": [
    "**Importante**: Recuerden poner su nombre y el de su usuario o de equipo (en caso de que aplique) en el reporte. NO ser谩n evaluados Notebooks sin nombre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igf7TBfSzvIo"
   },
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6moqxkEwCe-"
   },
   "source": [
    "## **Reporte a entregar**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo7vfXV4wD8s"
   },
   "source": [
    "Uno de los puntos claves en la evaluaci贸n de esta competencia es elaborar un informe claro y preciso, argumentando las decisiones tomadas al momento de crear sus modelos para que el cuerpo docente pueda entenderlos. La estructura que debe contener es la siguiente:\n",
    "\n",
    "1.\t**Introducci贸n**: Presentar brevemente el problema a resolver, incluyendo la formalizaci贸n de la task (c贸mo son los inputs y outputs del problema) y los desaf铆os que ven al analizar el corpus entregado. (**0.5 puntos**)\n",
    "2.\t**Representaciones**: Describir los atributos y representaciones usadas como entrada de los clasificadores, **recordar** que para entrenar modelos el input debe tener su representaci贸n num茅rica. Si bien, con Bag of Words (**baseline**) ya se comienzan a percibir buenos resultados, pueden mejorar su evaluaci贸n agregando m谩s atributos y representaciones dise帽adas a mano, sean lo m谩s creativos posible. M谩s abajo encontrar谩n una lista de estos posibles atributos que les podr谩 ser de utilidad. (**1 punto**)\n",
    "3.\t**Algoritmos**: Describir **brevemente** los algoritmos de clasificaci贸n usados, tanto si fueron algoritmos ya vistos en clases o bien arquitecturas de Deep Learning. (**0.5 puntos**)\n",
    "4.\t**M茅tricas de evaluaci贸n**: Describir brevemente las m茅tricas utilizadas en la evaluaci贸n, indicando qu茅 miden y su interpretaci贸n. (**0.5 puntos**)\n",
    "\n",
    "5.  **Dise帽o experimental**: Esta es una de las secciones m谩s importantes del reporte. Deben describir minuciosamente los experimentos que realizar谩n en la siguiente secci贸n. Describir las variables de control que manejar谩n, algunos ejemplos pueden ser: Los par谩metros de los clasificadores, par谩metros en las funciones con que procesan los textos y los transforman, par谩metros para el cross-validation, particiones de datos utilizadas, etc. En caso que utilicen redes neuronales, ser claros con el conjunto de hiperpar谩metros que probar谩n, la decisi贸n en las funciones de optimizaci贸n, funci贸n de p茅rdida,  regulaci贸n, etc. B谩sicamente explicar qu茅 es lo que veremos en la siguiente secci贸n.\n",
    "(**1 punto**)\n",
    "\n",
    "6.\t**Experimentos**: Incluyan todo el c贸digo de sus experimentos aqu铆. 隆Es vital haber realizado varios experimentos para sacar una buena nota! (**1.5 puntos**)\n",
    "\n",
    "7. **Resultados**: Comparar los resultados obtenidos utilizando diferentes algoritmos y representaciones.  Pueden mostrar los resultados sobre la partici贸n de validaci贸n en caso que la generen o sobre los resultados del conjunto de testing. Mostrar los resultados en alguna tabla, pueden poner aqu铆 tambi茅n los resultados obtenidos al realizar la submission. (**0.5 puntos**)\n",
    "\n",
    "8. **Conclusiones**:  Discutir resultados, proponer trabajo futuro. (**0.5 puntos**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMSn_tDYwOb1"
   },
   "source": [
    "## **Descripci贸n Baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:18:43.301002Z",
     "start_time": "2019-08-21T19:18:43.298037Z"
    },
    "id": "30fPWG5pzvIm"
   },
   "source": [
    "` `\n",
    "El baseline de la secci贸n **Experimentos** contiene un c贸digo b谩sico que resuelve la task y es el modelo subido por **cc6205-baseline** a la competencia. Pueden modificar el c贸digo como quieran siempre y cuando no cambien las funciones de las m茅tricas y el formato en que se suben los archivos a la competencia, ya que ese es el formato en que realizamos la evaluaci贸n. En concretro, el baseline hace lo siguiente:\n",
    "` `\n",
    "\n",
    "- Obtiene los datasets desde el repositorio del curso.\n",
    "- Divide los datasets en train (datos que est谩n etiquetados) y target set (datos no etiquetados para la competencia). Adem谩s, por cada dataset en train, se divide en un conjunto de entrenamiento y uno de prueba. Aqu铆 la mejor pr谩ctica ser铆a cambiar el c贸digo para obtener un conjunto de entrenamiento, validaci贸n y prueba.\n",
    "\n",
    "- Crea un Pipeline que: \n",
    "    - Crea features personalizados para la representaci贸n num茅rica.\n",
    "    - Transforma los dataset a bag of words (BoW).  \n",
    "    - Entrena un clasificador usando cada train set.\n",
    "- Clasifica y evalua el sistema creado usando el test set.\n",
    "- Clasifica el target set.\n",
    "- Genera una submission con el target en formato zip en el directorio en donde se est谩 ejecutando el notebook. \n",
    "\n",
    "` `\n",
    "\n",
    "Algunas pistas sobre como mejorar el rendimiento de los sistemas que creen. (Esto tendr谩 mas sentido cuando vean el c贸digo)\n",
    "\n",
    "- **Vectorizador**: Investigar los modulos de `nltk`, en particular, `TweetTokenizer`, `mark_negation` para reemplazar los tokenizadores. Tambi茅n, el par谩metro `ngram_range` (Ojo que el clf naive bayes no deber铆a usarse con n-gramas, ya que rompe el supuesto de independencia). Adem谩s, implementar los atributos que crean 煤tiles desde el listado del enunciado. Investigar tambi茅n el vectorizador tf-idf.\n",
    "\n",
    "- **Clasificador**: Investigar otros clasificadores m谩s efectivos que naive bayes. Estos deben poder retornar la probabilidad de pertenecia de las clases (ie: implementar la funci贸n `predict_proba`).\n",
    "\n",
    "- **Features**: Recuerden que pueden implementar todas las features que se les ocurra! Aqu铆 les adjuntamos algunos ejemplos:\n",
    "    -\tWord n-grams.\n",
    "    -\tCharacter n-grams. \n",
    "    -\tPart-of-speech tags.\n",
    "    -\tSentiment Lexicons (Lexicon = A set of words with a label or associated value.).\n",
    "        - Count the number of positive and negative words within a sentence.\n",
    "        - If the lexicon has associated intensity of feeling (for example in a decimal), then take the average of the intensity of the sentence according to the feeling, the sum, etc.\n",
    "        -\tA good lexicon of sentiment: [Bing Liu](http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar) \n",
    "        - A reference with a lot of [sentiment lexicons](https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c). \n",
    "    -\tThe number of elongated words (words with one character repeated more than two times).\n",
    "    -\tThe number of words with all characters in uppercase.\n",
    "    -\tThe presence and the number of positive or negative emoticons.\n",
    "    -\tThe number of individual negations.\n",
    "    -\tThe number of contiguous sequences of dots, question marks and exclamation marks.\n",
    "    -\tWord Embeddings: Here are some good ideas on how to use them.\n",
    "    https://stats.stackexchange.com/questions/221715/apply-word-embeddings-to-entire-document-to-get-a-feature-vector\n",
    "\n",
    "- **Reducci贸n de dimensionalidad**: Tambi茅n puede serles de ayuda. Referencias [aqu铆](https://scikit-learn.org/stable/modules/unsupervised_reduction.html).\n",
    "\n",
    "- Por 煤ltimo, pueden encontrar mas referencias de c贸mo mejorar sus features, el vectorizador y el clasificador [aqu铆](https://affectivetweets.cms.waikato.ac.nz/benchmark/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IT7ZpVRuzGAF"
   },
   "source": [
    "# **Entregable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:34:25.683540Z",
     "start_time": "2020-03-31T13:34:25.673430Z"
    },
    "id": "E29LEMZ9zvIo"
   },
   "source": [
    "## **1. Introducci贸n**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W20NnoduzvIo"
   },
   "source": [
    "    Escriba su introducci贸n aqu铆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:13.474238Z",
     "start_time": "2020-03-31T13:47:13.454068Z"
    },
    "id": "OTAIEnSJzvIp"
   },
   "source": [
    "## **2. Representaciones**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:17.719268Z",
     "start_time": "2020-03-31T13:47:17.709207Z"
    },
    "id": "EV1qBv-MzvIp"
   },
   "source": [
    "    Escriba sus decisiones para la representaci贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMOjYSQezvIq"
   },
   "source": [
    "## **3. Algoritmos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bPiFs33zilS"
   },
   "source": [
    "    Explique los algoritmos utilizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:52.064631Z",
     "start_time": "2020-03-31T13:47:52.044451Z"
    },
    "id": "ECjkdgdwzvIq"
   },
   "source": [
    "## **4. M茅tricas de Evaluaci贸n**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6eHJdHBzvIr"
   },
   "source": [
    "- AUC: ...\n",
    "- Kappa: ...\n",
    "- Accuracy: ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJyTrr2onLOo"
   },
   "source": [
    "## **5. Dise帽o experimental**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnhEwjh_nP9M"
   },
   "source": [
    "    Descripci贸n de la metodolog铆a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OX5Ib_pCzvIr"
   },
   "source": [
    "## **6. Experimentos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:31:40.023344Z",
     "start_time": "2020-03-31T13:31:40.003541Z"
    },
    "id": "aK24MJ8jzvIr"
   },
   "source": [
    "### Importar librer铆as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:20.587160Z",
     "start_time": "2020-04-07T15:44:19.319386Z"
    },
    "id": "FukgFUTUzvIs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import codecs\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FevBPus0zvIs"
   },
   "source": [
    "### Definir m茅todos de evaluaci贸n (**NO tocar este c贸digo**)\n",
    "\n",
    "Estas funciones est谩n a cargo de evaluar los resultados de la tarea. No deber铆an cambiarlas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:20.604066Z",
     "start_time": "2020-04-07T15:44:20.589106Z"
    },
    "id": "9wlllV7PzvIs"
   },
   "outputs": [],
   "source": [
    "def auc_score(test_set, predicted_set):\n",
    "    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
    "    medium_predicted = np.array(\n",
    "        [prediction[1] for prediction in predicted_set])\n",
    "    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
    "    high_test = np.where(test_set == 'high', 1.0, 0.0)\n",
    "    medium_test = np.where(test_set == 'medium', 1.0, 0.0)\n",
    "    low_test = np.where(test_set == 'low', 1.0, 0.0)\n",
    "    auc_high = roc_auc_score(high_test, high_predicted)\n",
    "    auc_med = roc_auc_score(medium_test, medium_predicted)\n",
    "    auc_low = roc_auc_score(low_test, low_predicted)\n",
    "    auc_w = (low_test.sum() * auc_low + medium_test.sum() * auc_med +\n",
    "             high_test.sum() * auc_high) / (\n",
    "                 low_test.sum() + medium_test.sum() + high_test.sum())\n",
    "    return auc_w\n",
    "\n",
    "\n",
    "def evaluate(predicted_probabilities, y_test, labels, dataset_name):\n",
    "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
    "    # entregar el arreglo de clases aprendido por el clasificador.\n",
    "    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n",
    "    predicted_labels = [\n",
    "        labels[np.argmax(item)] for item in predicted_probabilities\n",
    "    ]\n",
    "\n",
    "    print('Confusion Matrix for {}:\\n'.format(dataset_name))\n",
    "    print(\n",
    "        confusion_matrix(y_test,\n",
    "                         predicted_labels,\n",
    "                         labels=['low', 'medium', 'high']))\n",
    "\n",
    "    print('\\nClassification Report:\\n')\n",
    "    print(\n",
    "        classification_report(y_test,\n",
    "                              predicted_labels,\n",
    "                              labels=['low', 'medium', 'high']))\n",
    "    # Reorder predicted probabilities array.\n",
    "    labels = labels.tolist()\n",
    "    \n",
    "    predicted_probabilities = predicted_probabilities[:, [\n",
    "        labels.index('low'),\n",
    "        labels.index('medium'),\n",
    "        labels.index('high')\n",
    "    ]]\n",
    "    \n",
    "    \n",
    "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
    "    print(\"Scores:\\n\\nAUC: \", auc, end='\\t')\n",
    "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
    "    print(\"Kappa:\", kappa, end='\\t')\n",
    "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print('------------------------------------------------------\\n')\n",
    "    return np.array([auc, kappa, accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkOP6ugwzvIt"
   },
   "source": [
    "### Datos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.068137Z",
     "start_time": "2020-04-07T15:44:20.606061Z"
    },
    "id": "D1XhFPhrzvIt"
   },
   "outputs": [],
   "source": [
    "# Datasets de entrenamiento.\n",
    "train = {\n",
    "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/anger-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/fear-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/joy-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/sadness-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'])\n",
    "}\n",
    "# Datasets que deber谩n predecir para la competencia.\n",
    "target = {\n",
    "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/anger-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/fear-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/joy-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/sadness-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.088707Z",
     "start_time": "2020-04-07T15:44:21.069757Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1647890199751,
     "user": {
      "displayName": "Gabriel Iturra",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghe-mqbWbrqQ1gVhaBhwEAK5Uu5cfHEnENzwLJUGA=s64",
      "userId": "02319919045117626989"
     },
     "user_tz": 180
    },
    "id": "flg2Zw2mzvIt",
    "outputId": "1444867d-5d1f-4ba8-de05-6e13a5c1e113"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>sentiment_intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>10726</td>\n",
       "      <td>So Rangers v Celtic ll #revenge</td>\n",
       "      <td>anger</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>10700</td>\n",
       "      <td>@The_Nasty_P now you gotta do that with fast n...</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>10615</td>\n",
       "      <td>@StarFlowerz17 @DreamerAbe #bitter. Nicole and...</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>10568</td>\n",
       "      <td>@SageHillfarms @HintonAlisa @hb_heather Hey, J...</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>10288</td>\n",
       "      <td>luv seeing a man with a scowl on his face walk...</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet  class  \\\n",
       "726  10726                    So Rangers v Celtic ll #revenge  anger   \n",
       "700  10700  @The_Nasty_P now you gotta do that with fast n...  anger   \n",
       "615  10615  @StarFlowerz17 @DreamerAbe #bitter. Nicole and...  anger   \n",
       "568  10568  @SageHillfarms @HintonAlisa @hb_heather Hey, J...  anger   \n",
       "288  10288  luv seeing a man with a scowl on his face walk...  anger   \n",
       "\n",
       "    sentiment_intensity  \n",
       "726                 low  \n",
       "700              medium  \n",
       "615              medium  \n",
       "568              medium  \n",
       "288              medium  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de algunas filas aleatorias del dataset etiquetado:\n",
    "train['anger'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1647890201421,
     "user": {
      "displayName": "Gabriel Iturra",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghe-mqbWbrqQ1gVhaBhwEAK5Uu5cfHEnENzwLJUGA=s64",
      "userId": "02319919045117626989"
     },
     "user_tz": 180
    },
    "id": "XB7hb7KH2DFK",
    "outputId": "4c3397ce-471d-45d8-a944-fcde1abdf2cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>sentiment_intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>11602</td>\n",
       "      <td>@simondawkins23 @Titchlar81 clicked the wrong ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>11284</td>\n",
       "      <td>Marcus Rojo is the worst player i have ever se...</td>\n",
       "      <td>anger</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>11066</td>\n",
       "      <td>I don't like pineapple I only eat them on pizz...</td>\n",
       "      <td>anger</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>11100</td>\n",
       "      <td>@NJDDanin123 I personally liked #relentless d...</td>\n",
       "      <td>anger</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>11550</td>\n",
       "      <td>@TalkTalkCare  how on earth can I send an emai...</td>\n",
       "      <td>anger</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet  class  \\\n",
       "661  11602  @simondawkins23 @Titchlar81 clicked the wrong ...  anger   \n",
       "343  11284  Marcus Rojo is the worst player i have ever se...  anger   \n",
       "125  11066  I don't like pineapple I only eat them on pizz...  anger   \n",
       "159  11100  @NJDDanin123 I personally liked #relentless d...  anger   \n",
       "609  11550  @TalkTalkCare  how on earth can I send an emai...  anger   \n",
       "\n",
       "     sentiment_intensity  \n",
       "661                  NaN  \n",
       "343                  NaN  \n",
       "125                  NaN  \n",
       "159                  NaN  \n",
       "609                  NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de algunas filas aleatorias del dataset no etiquetado\n",
    "target['anger'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5aNqEfVzvIv"
   },
   "source": [
    "### Analizar los datos \n",
    "\n",
    "En esta secci贸n analizaremos el balance de los datos. Para ello se imprime la cantidad de tweets de cada dataset agrupados por la intensidad de sentimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.117633Z",
     "start_time": "2020-04-07T15:44:21.090703Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1647890204981,
     "user": {
      "displayName": "Gabriel Iturra",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghe-mqbWbrqQ1gVhaBhwEAK5Uu5cfHEnENzwLJUGA=s64",
      "userId": "02319919045117626989"
     },
     "user_tz": 180
    },
    "id": "u5007JRgzvIv",
    "outputId": "f8af2406-ea94-4716-9414-1b6d7e2450a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: anger \n",
      " sentiment_intensity\n",
      "high      163\n",
      "low       161\n",
      "medium    617\n",
      "dtype: int64\n",
      "----------------------------------------------------------\n",
      "\n",
      "Dataset: fear \n",
      " sentiment_intensity\n",
      "high      270\n",
      "low       288\n",
      "medium    699\n",
      "dtype: int64\n",
      "----------------------------------------------------------\n",
      "\n",
      "Dataset: joy \n",
      " sentiment_intensity\n",
      "high      195\n",
      "low       219\n",
      "medium    488\n",
      "dtype: int64\n",
      "----------------------------------------------------------\n",
      "\n",
      "Dataset: sadness \n",
      " sentiment_intensity\n",
      "high      197\n",
      "low       210\n",
      "medium    453\n",
      "dtype: int64\n",
      "----------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in train:\n",
    "    print(f'Dataset: {dataset_name} \\n', train[dataset_name].groupby(['sentiment_intensity']).size())\n",
    "    print('----------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stZ6ig5hzvIv"
   },
   "source": [
    "### Custom Features \n",
    "\n",
    "Para crear features personalizadas implementaremos nuestros propios Transformers (estandar de scikit para crear nuevas features entre otras cosas). Para esto:\n",
    "\n",
    "1. Creamos nuestra clase Transformer extendiendo BaseEstimator y TransformerMixin. En este ejemplo, definiremos `CharsCountTransformer` que cuenta caracteres relevantes ('!', '?', '#', '@') en los tweets.\n",
    "2. Definimos una funci贸n c贸mo `get_relevant_chars` que opera por cada tweet y retorna un arreglo.\n",
    "3. Hacemos un override de la funci贸n `transform` en donde iteramos por cada tweet, llamamos a la funci贸n que hicimos antes y agregamos sus resultados a un arreglo. Finalmente lo retornamos.\n",
    "\n",
    "Esto nos facilitar谩 el trabajo mas adelante. Una Guia completa de las transformaciones predefinidas en scikit pueden encontrarla [aqu铆](https://scikit-learn.org/stable/data_transforms.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.128600Z",
     "start_time": "2020-04-07T15:44:21.119624Z"
    },
    "id": "tNPB8zc9zvIw"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import nltk\n",
    "\n",
    "\n",
    "class Feature(ABC):\n",
    "    @abstractmethod\n",
    "    def get_feature(self, tweet: list):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Lexicon(Feature):\n",
    "    def __init__(self):\n",
    "        self.lists_words = {}\n",
    "        self.create_list_words(\"positive\")\n",
    "        self.create_list_words(\"negative\")\n",
    "\n",
    "    def create_list_words(self, word_type: str):\n",
    "        with codecs.open(\n",
    "                f\"./opinion-lexicon-English/{word_type}-words.txt\",\n",
    "                \"r\",\n",
    "                \"iso-8859-1\"\n",
    "                ) as f:\n",
    "            list_words = map(lambda x: x.replace(\"\\n\", \"\"), f.readlines())\n",
    "        self.lists_words[word_type] = list(list_words)\n",
    "\n",
    "    # override\n",
    "    def get_feature(self, tweet: list):\n",
    "        negative_words = self.lists_words[\"negative\"]\n",
    "        positive_words = self.lists_words[\"positive\"]\n",
    "\n",
    "        negatives = 0\n",
    "        positives = 0\n",
    "        for word in tweet:\n",
    "            if word in negative_words:\n",
    "                negatives += 1\n",
    "            if word in positive_words:\n",
    "                positives += 1\n",
    "\n",
    "        return [positives, negatives]\n",
    "\n",
    "\n",
    "class ElongatedWords(Feature):\n",
    "    # override\n",
    "    def get_feature(self, tweet: list):\n",
    "\n",
    "        elongated_words = 0\n",
    "\n",
    "        for word in tweet:\n",
    "            current_letter = word[0]\n",
    "            count = -1\n",
    "            for letter in word:\n",
    "                if letter == current_letter:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    if count >= 4:\n",
    "                        elongated_words += 1\n",
    "                        current_letter = letter\n",
    "                        count = 0\n",
    "\n",
    "        return [elongated_words]\n",
    "\n",
    "\n",
    "class CharsCount(Feature):\n",
    "    # override\n",
    "    def get_feature(self, tweet: list):\n",
    "        num_hashtags = 0\n",
    "        num_exclamations = 0\n",
    "        num_interrogations = 0\n",
    "        num_at = 0\n",
    "\n",
    "        for word in tweet:\n",
    "            num_hashtags += word.count('#')\n",
    "            num_exclamations += word.count('!')\n",
    "            num_interrogations += word.count('?')\n",
    "            num_at += word.count('@')\n",
    "\n",
    "        return [num_hashtags, num_exclamations, num_interrogations, num_at]\n",
    "\n",
    "\n",
    "class UpperCount(Feature):\n",
    "    # override\n",
    "    def get_feature(self, tweet: list):\n",
    "        upper_count = 0\n",
    "        for word in tweet:\n",
    "            if word.isupper():\n",
    "                upper_count += 1\n",
    "        return [upper_count]\n",
    "\n",
    "\n",
    "class CustomTransformer(ABC, BaseEstimator, TransformerMixin):\n",
    "    def transform(self, X, y=None):\n",
    "        chars = []\n",
    "\n",
    "        tokenizer = nltk.TweetTokenizer()\n",
    "\n",
    "        for tweet in X:\n",
    "            features = []\n",
    "            split_tweet = tokenizer.tokenize(tweet)\n",
    "\n",
    "            for class_ in self.classes:\n",
    "                feature = class_().get_feature(split_tweet)\n",
    "                features += feature\n",
    "\n",
    "            chars.append(features)\n",
    "\n",
    "        return np.array(chars)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "\n",
    "class AngryTransformer(CustomTransformer):\n",
    "    def __init__(self):\n",
    "        self.classes = [\n",
    "                Lexicon,\n",
    "                ElongatedWords,\n",
    "                CharsCount,\n",
    "                UpperCount\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.145564Z",
     "start_time": "2020-04-07T15:44:21.131593Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1647890207937,
     "user": {
      "displayName": "Gabriel Iturra",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghe-mqbWbrqQ1gVhaBhwEAK5Uu5cfHEnENzwLJUGA=s64",
      "userId": "02319919045117626989"
     },
     "user_tz": 180
    },
    "id": "lCQzsuAbzvIw",
    "outputId": "6c10a7d9-6289-445e-aa2d-d61a08d4f0ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet original: @everycolorbot more like every color looks the same #triggered #colorblind \n",
      "Features creados: [0 0 0 2 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Veamos que sucede si ejecutamos el transformer\n",
    "sample = train['anger'].sample(5, random_state = 1).tweet\n",
    "sample_features = AngryTransformer().transform(sample)\n",
    "\n",
    "# Se puede verificar que el conteo de s铆mbolos es consistente con el transformer creado.\n",
    "print(f'Tweet original: {sample.iloc[0]}')\n",
    "print(f'Features creados: {sample_features[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MO_yIepczvIx"
   },
   "source": [
    "### Definir la representaci贸n y el clasificador\n",
    "\n",
    "Para esto, definiremos Pipelines. Un `Pipeline` es una lista de transformaciones y un estimador(clasificador) ubicado al final el cual define el flujo que seguiran nuestros datos dentro del sistema que creemos. Nos permite ejecutar facilmente el mismo proceso sobre todos los datasets que usemos, simplificando as铆 nuestra programaci贸n.\n",
    "\n",
    "El pipeline m谩s b谩sico que podemos hacer es transformar el dataset a Bag of Words y despu茅s usar clasificar el BoW usando NaiveBayes:\n",
    "\n",
    "```python\n",
    "    Pipeline([('bow', CountVectorizer()), ('clf', MultinomialNB())])\n",
    "```\n",
    "\n",
    "\n",
    "Ahora, si queremos usar nuestra transformaci贸n para agregar las features que creamos, usaremos `FeatureUnion`. Esta simplemente concatenar谩 los vectores resultantes de ejecutar BoW y los Transformer en un solo vector.\n",
    "\n",
    "```python\n",
    "    Pipeline([('features',FeatureUnion([('bow', CountVectorizer()),\n",
    "                                        ('chars_count',CharsCountTransformer())])),\n",
    "              ('clf', MultinomialNB())])\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDbHjXv-zvIx"
   },
   "source": [
    "Recuerden que cada pipeline representa un sistema de clasificaci贸n distinto. Por lo mismo, deben instanciar uno por cada problema que resuelvan. De lo contrario, podr铆an solapar resultados.  Para esto, les recomendamos crear los pipeline en distintas funciones, como la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.155528Z",
     "start_time": "2020-04-07T15:44:21.149545Z"
    },
    "id": "z_R6tyMCzvIy"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "def get_experiment_0_pipeline():\n",
    "\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', CountVectorizer()),\n",
    "                                    ('chars_count', CharsCountTransformer()),\n",
    "                                    ])), ('clf', MultinomialNB())])\n",
    "\n",
    "def get_experiment_1_pipeline():\n",
    "        return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', CountVectorizer()),\n",
    "                                    ('chars_count', CharsCountTransformer()),\n",
    "                                    ])), ('clf', MultinomialNB())])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmMdm98vzvIy"
   },
   "source": [
    "### Ejecutar el pipeline para alg煤n dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.167498Z",
     "start_time": "2020-04-07T15:44:21.157540Z"
    },
    "id": "_eX0cEu-zvIz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run(dataset, dataset_name, pipeline):\n",
    "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
    "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n",
    "\n",
    "    # Dividimos el dataset en train y test, a煤n no se transforma de Strings a valores num茅ricos.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataset.tweet,\n",
    "        dataset.sentiment_intensity,\n",
    "        shuffle=True,\n",
    "        test_size=0.33)\n",
    "    \n",
    "    print(f'# Datos de entrenamiento en dataset {dataset_name}: {len(X_train)}')\n",
    "    print(f'# Datos de testing en dataset {dataset_name}: {len(X_test)}')\n",
    "\n",
    "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline). \n",
    "    # En este caso el Bag of Words es el encargado de transformar de Strings a vectores num茅ricos.\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
    "    predicted_probabilities = pipeline.predict_proba(X_test)\n",
    "\n",
    "    # Obtenemos el orden de las clases aprendidas.\n",
    "    learned_labels = pipeline.classes_\n",
    "    \n",
    "    # Evaluamos:\n",
    "    scores = evaluate(predicted_probabilities, y_test, learned_labels, dataset_name)\n",
    "    return pipeline, learned_labels, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z96C1ZOfzvIz"
   },
   "source": [
    "### Ejecutar el sistema creado por cada train set\n",
    "\n",
    "Este c贸digo crea y entrena los 4 sistemas de clasificaci贸n y luego los evalua. Para los experimentos, pueden copiar este c贸digo variando el pipeline cuantas veces estimen conveniente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.384119Z",
     "start_time": "2020-04-07T15:44:21.170488Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 629,
     "status": "ok",
     "timestamp": 1647890223742,
     "user": {
      "displayName": "Gabriel Iturra",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghe-mqbWbrqQ1gVhaBhwEAK5Uu5cfHEnENzwLJUGA=s64",
      "userId": "02319919045117626989"
     },
     "user_tz": 180
    },
    "id": "OXAxZBdVzvI0",
    "outputId": "dfc387b4-3121-43f4-d892-eca1ee100367",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Datos de entrenamiento en dataset anger: 630\n",
      "# Datos de testing en dataset anger: 311\n",
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  8  42   0]\n",
      " [  3 190   5]\n",
      " [  1  53   9]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.67      0.16      0.26        50\n",
      "      medium       0.67      0.96      0.79       198\n",
      "        high       0.64      0.14      0.23        63\n",
      "\n",
      "    accuracy                           0.67       311\n",
      "   macro avg       0.66      0.42      0.43       311\n",
      "weighted avg       0.66      0.67      0.59       311\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.604\tKappa: 0.167\tAccuracy: 0.666\n",
      "------------------------------------------------------\n",
      "\n",
      "# Datos de entrenamiento en dataset fear: 842\n",
      "# Datos de testing en dataset fear: 415\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 20  65   2]\n",
      " [ 13 217  10]\n",
      " [  4  62  22]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.54      0.23      0.32        87\n",
      "      medium       0.63      0.90      0.74       240\n",
      "        high       0.65      0.25      0.36        88\n",
      "\n",
      "    accuracy                           0.62       415\n",
      "   macro avg       0.61      0.46      0.48       415\n",
      "weighted avg       0.62      0.62      0.57       415\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.7\tKappa: 0.224\tAccuracy: 0.624\n",
      "------------------------------------------------------\n",
      "\n",
      "# Datos de entrenamiento en dataset joy: 604\n",
      "# Datos de testing en dataset joy: 298\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[ 18  50   1]\n",
      " [ 16 137   8]\n",
      " [  0  36  32]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.53      0.26      0.35        69\n",
      "      medium       0.61      0.85      0.71       161\n",
      "        high       0.78      0.47      0.59        68\n",
      "\n",
      "    accuracy                           0.63       298\n",
      "   macro avg       0.64      0.53      0.55       298\n",
      "weighted avg       0.63      0.63      0.60       298\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.725\tKappa: 0.308\tAccuracy: 0.628\n",
      "------------------------------------------------------\n",
      "\n",
      "# Datos de entrenamiento en dataset sadness: 576\n",
      "# Datos de testing en dataset sadness: 284\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[  8  64   0]\n",
      " [  4 136   5]\n",
      " [  1  52  14]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.62      0.11      0.19        72\n",
      "      medium       0.54      0.94      0.69       145\n",
      "        high       0.74      0.21      0.33        67\n",
      "\n",
      "    accuracy                           0.56       284\n",
      "   macro avg       0.63      0.42      0.40       284\n",
      "weighted avg       0.61      0.56      0.47       284\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.619\tKappa: 0.146\tAccuracy: 0.556\n",
      "------------------------------------------------------\n",
      "\n",
      "Average scores:\n",
      "\n",
      " Average AUC: 0.662\t Average Kappa: 0.211\t Average Accuracy: 0.619\n"
     ]
    }
   ],
   "source": [
    "classifiers = []\n",
    "learned_labels_array = []\n",
    "scores_array = []\n",
    "\n",
    "# Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
    "for dataset_name, dataset in train.items():\n",
    "    \n",
    "    # creamos el pipeline\n",
    "    pipeline = get_experiment_0_pipeline()\n",
    "    \n",
    "    # ejecutamos el pipeline sobre el dataset\n",
    "    classifier, learned_labels, scores = run(dataset, dataset_name, pipeline)\n",
    "\n",
    "    # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
    "    classifiers.append(classifier)\n",
    "\n",
    "    # guardamos las labels aprendidas por el clasificador\n",
    "    learned_labels_array.append(learned_labels)\n",
    "\n",
    "    # guardamos los scores obtenidos\n",
    "    scores_array.append(scores)\n",
    "\n",
    "# print avg scores\n",
    "print(\n",
    "    \"Average scores:\\n\\n\",\n",
    "    \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
    "    .format(*np.array(scores_array).mean(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:37:43.169737Z",
     "start_time": "2019-08-21T19:37:43.166744Z"
    },
    "id": "IUKwcde_zvI0"
   },
   "source": [
    "### Predecir los target set y crear la submission\n",
    "\n",
    "Aqu铆 predecimos los target set usando los clasificadores creados y creamos los archivos de las submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.392097Z",
     "start_time": "2020-04-07T15:44:21.386114Z"
    },
    "id": "mWDUoSmbzvI1"
   },
   "outputs": [],
   "source": [
    "def predict_target(dataset, classifier, labels):\n",
    "    # Predecir las probabilidades de intensidad de cada elemento del target set.\n",
    "    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n",
    "    \n",
    "    # Agregar ids\n",
    "    predicted['id'] = dataset.id.values\n",
    "    # Reordenar las columnas\n",
    "    predicted = predicted[['id', 'low', 'medium', 'high']]\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.588573Z",
     "start_time": "2020-04-07T15:44:21.394094Z"
    },
    "id": "5CJ4PTwZzvI1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_target = {}\n",
    "\n",
    "# Crear carpeta ./predictions\n",
    "if (not os.path.exists('./predictions')):\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "else:\n",
    "    # Eliminar predicciones anteriores:\n",
    "    shutil.rmtree('./predictions')\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "# por cada target set:\n",
    "for idx, key in enumerate(target):\n",
    "    # Predecirlo\n",
    "    predicted_target[key] = predict_target(target[key], classifiers[idx],\n",
    "                                           learned_labels_array[idx])\n",
    "    # Guardar predicciones en archivos separados. \n",
    "    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n",
    "                                 sep='\\t',\n",
    "                                 header=False,\n",
    "                                 index=False)\n",
    "\n",
    "# Crear archivo zip\n",
    "a = shutil.make_archive('predictions', 'zip', './predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCAyJIj8nTlU"
   },
   "source": [
    "## **7. Resultados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAYwupS3naYX"
   },
   "source": [
    "    Muestre y analice sus resultados aqu铆\n",
    "\n",
    "A continuaci贸n una tabla de ejemplo para mostrar los resultados.  En este caso s贸lo est谩 el experimento del baseline m谩s otro clasificador, pero ustedes debiesen generar una mayor cantidad de experimentos. \n",
    "\n",
    "| No. | Approach                       || Dataset   | AUC   | Kappa | Accuracy |\n",
    "|-----|--------------------------------||-----------|-------|-------|----------|\n",
    "|     | Features        | Clasifier     |           |       |       |          |\n",
    "| 0   | bow+chars_count | MultinomialNB | anger     | 0.622 | 0.163 | 0.688    |\n",
    "|     |                 |               | fear      | 0.597 | 0.091 | 0.559    |\n",
    "|     |                 |               | joy       | 0.728 | 0.251 | 0.601    |\n",
    "|     |                 |               | sadness   | 0.645 | 0.166 | 0.581    |\n",
    "|     |                 |               |**average**| 0.648 | 0.168 | 0.607    |\n",
    "| 1   | tus features    | tu clasifier  | anger     |       |       |          |\n",
    "|     |                 |               | fear      |       |       |          |\n",
    "|     |                 |               | joy       |       |       |          |\n",
    "|     |                 |               | sadness   |       |       |          |\n",
    "|     |                 |               |**average**|       |       |          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sqlew0iizvI1"
   },
   "source": [
    "## **8. Conclusiones**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFTikGbszZuc"
   },
   "source": [
    "    Escriba aqu铆 sus conclusiones"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "mxlNrNf_p0ZY",
    "N6lhhfl2zvIk",
    "a6moqxkEwCe-",
    "LMSn_tDYwOb1",
    "E29LEMZ9zvIo",
    "OTAIEnSJzvIp",
    "kMOjYSQezvIq",
    "ECjkdgdwzvIq",
    "RkOP6ugwzvIt",
    "q5aNqEfVzvIv",
    "stZ6ig5hzvIv",
    "gmMdm98vzvIy"
   ],
   "name": "Assignment_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
