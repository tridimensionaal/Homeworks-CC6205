{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2obO44rRIDIm"
   },
   "source": [
    "# **Tarea 2 - CC6205 Natural Language Processing üìö**\n",
    "\n",
    "**Integrantes:**\n",
    "\n",
    "**Fecha l√≠mite de entrega üìÜ:** Martes 12 de abril.\n",
    "\n",
    "**Tiempo estimado de dedicaci√≥n:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zpupcv6QW2fd"
   },
   "source": [
    "Bienvenid@s a la segunda tarea del curso de Natural Language Processing (NLP). En esta tarea estaremos modelando probabil√≠sticamente el lenguaje mediante **Languaje Modeling** y clasificando textos mediante el m√©todo **Na√Øve Bayes**. Espec√≠ficamente, la tarea consta de una parte te√≥rica que busca evaluar conceptos vistos en clases sobre los **Language Models** y una parte pr√°ctica donde **programar√°n a mano** el m√©todo **Na√Øve Bayes**. \n",
    "\n",
    "**Instrucciones:**\n",
    "- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
    "- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n",
    "- El formato de entrega es este mismo Jupyter Notebook o el archivo .ipynb si lo ejecuto de forma local.\n",
    "- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n. \n",
    "- Est√° **PROHIBIDO** usar cualquier librer√≠a que implemente los algoritmos pedidos (Spacy, scikit, etc). S√≥lo se podr√°n utilizar las librer√≠as importadas al inicio de la secci√≥n de pr√°ctica.\n",
    "- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav√©s del canal de Discord del curso.\n",
    "\n",
    "Si a√∫n no han visto las clases, se recomienda visitar los links de las referencias.\n",
    "\n",
    "**Referencias:**\n",
    "\n",
    "Slides:\n",
    "    \n",
    "- [Language Models](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/lmslides.pdf) (slides by Michael Collins)\n",
    "- [Text Classification and Naive Bayes](https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf) (slides by Dan Jurafsky)\n",
    "\n",
    "Videos: \n",
    "\n",
    "- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 1](https://www.youtube.com/watch?v=9E2jJ6kcb4Y&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=4)\n",
    "- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 2](https://www.youtube.com/watch?v=ZWqbEQXLra0&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=5)\n",
    "- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 3](https://www.youtube.com/watch?v=tsumFqwFlaA&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=6)\n",
    "- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 4](https://www.youtube.com/watch?v=s3TWdv4sqkg&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=7)\n",
    "- [CC6205 - Procesamiento de Lenguaje Natural: Naive Bayes Parte 1](https://www.youtube.com/watch?v=kG9BK9Oy1hU)\n",
    "- [CC6205 - Procesamiento de Lenguaje Natural: Naive Bayes Parte 2](https://www.youtube.com/watch?v=Iqte5kKHvzE)\n",
    "- [CC6205 - Procesamiento de Lenguaje Natural: Naive Bayes Parte 3](https://www.youtube.com/watch?v=TSJg0_X3Abk)\n",
    "\n",
    "` ` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JstKx3TiKcIp"
   },
   "source": [
    "---------------------------\n",
    "## Parte 1. Language Modeling (3 pts)\n",
    "\n",
    "En esta parte responder√°n preguntas **te√≥ricas** sobre los Lenguage Models. Para comprender como funcionan muchas de las t√©cnicas que veremos posteriormente durante el curso ser√° muy √∫til que dominen estos modelos y sus fundamentos.\n",
    "\n",
    "Recuerden que los **Language Models** b√°sicamente nos permiten, dado un corpus, estimar un modelo probabilista al que le podemos pasar una oraci√≥n y determinar que tan probable es que esa oraci√≥n haya sido generada. Para esto, tenemos que un modelo de $n$-gramas puede ser definido por una *cadena de M√°rkov* de orden $n-1$.\n",
    "\n",
    "En clases vimos los modelos basados en unigramas, bigramas y trigramas. En esta pregunta trabajaremos con modelos de lenguaje basados en 4-gramas (*cadena de M√°rkov* de tercer orden).\n",
    "\n",
    "**Nota:** Las preguntas deben ser resueltas con desarrollo, pero sin c√≥digo. Por favor usen $\\LaTeX$ para las f√≥rmulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hwW-07MrRpt"
   },
   "source": [
    "\n",
    "### 1.1. (1 pt)\n",
    "\n",
    "Asuma que tenemos calculados los par√°metros $q(w_i | w_{i-3}, w_{i-2}, w_{i-1})$ para todas las posibles secuencias de tama√±o 4 que aparecen en un corpus $\\mathcal{C}$. Dado esto, muestre c√≥mo el modelo le asignar√≠a una probabilidad a la frase `una persona corriendo r√°pido`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzlQDAVqWNdX"
   },
   "source": [
    "` `\n",
    "\n",
    "**Respuesta:** \n",
    "\n",
    "\\begin{equation}\n",
    "P(\\text{una persona corriendo r√°pido STOP}) = q(\\text{una} | \\text{*,*,*}) \\cdot q(\\text{persona} | \\text{*,*, una}) \\cdot q(\\text{corriendo} | \\text{*, una, persona}) \\cdot q(\\text{r√°pido} | \\text{una, persona, corriendo}) \\cdot q(\\text{STOP} | \\text{persona, corriendo, r√°pido})\n",
    "\\end{equation}\n",
    "\n",
    "` `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAej_jqtVwm1"
   },
   "source": [
    "### 1.2 Estimando las probabilidades (1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXSFlCIex8kq"
   },
   "source": [
    "#### 1.2.1. Modelo simple (0.5 puntos)\n",
    "\n",
    "Si hubieses tenido que estimar las probabilidades condicionales (par√°metros del modelo) $q(w_i | w_{i-3}, w_{i-2}, w_{i-1})$ a partir de $\\mathcal{C}$, ¬øc√≥mo la definir√≠as siguiendo el principio de m√°xima verosimilitud?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjNisxPzWsMG"
   },
   "source": [
    "` ` \n",
    "\n",
    "**Respuesta:** \n",
    "\n",
    "\\begin{equation}\n",
    "q_{ML} (w_{i} | w_{i-3}, w_{i-2}, w_{i-1}) = \\frac{\\text{Count}(w_{i-3}, w_{i-2}, w_{i-1}, w_{i})}{\\text{Count}( w_{i-2}, w_{i-1}, w_{i})}\n",
    "\\end{equation}\n",
    "\n",
    "donde $\\text{Count}(w_{i-3}, w_{i-2}, w_{i-1}, w_{i})$ es la cantidad de veces que la frase \"$w_{i}$ $w_{i-1}$ $w_{i-2}$ $w_{i-3}$\" aparece en el corpus $\\mathcal{C}$ y $\\text{Count}(w_{i-2}, w_{i-1}, w_{i})$  es la cantidad de veces que la frase \"$w_{i}$ $w_{i-1}$ $w_{i-2}$\" aparece en el corpus $\\mathcal{C}$\n",
    "` `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwNkPIXure0C"
   },
   "source": [
    "#### 1.2.2. Modelo interpolado (0.5 puntos)\n",
    "Muestre c√≥mo se calcular√≠a $q(w_{i} | w_{i-3}, w_{i-2}, w_{i-1})$ usando un modelo que interpola modelos de lenguajes basados en 4-grams, trigramas, bigramas y unigramas. ¬øTe fue necesario definir nuevos par√°metros? En caso afirmativo, ¬øqu√© los diferencia de los par√°metros del modelo simple y qu√© propiedades deben cumplir?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeLZAd0Tr9ne"
   },
   "source": [
    "` ` \n",
    "\n",
    "**Respuesta:** \n",
    "\n",
    "\\begin{equation}\n",
    "    q(w_{i} | w_{i-3}, w_{i-2}, w_{i-1}) = \\lambda_{1} \\cdot q_{ML}(w_{i} | w_{i-3}, w_{i-2}, w{i-1}) + \\lambda_{2} \\cdot q_{ML}(w_{i} | w_{i-2}, w_{i-1}) + \\lambda_{3} \\cdot q_{ML}(w_{i} | w_{i-1}) + \\lambda_{4} \\cdot q_{ML}(w_{i})\n",
    "\\end{equation}\n",
    "\n",
    "donde $ \\lambda_{4} + \\lambda_{3} + \\lambda_{2} + \\lambda_{1} = 1$ y $\\forall i, \\lambda_{i} \\geq 0$\n",
    "\n",
    "` `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHqcRJ7Vr_8x"
   },
   "source": [
    "### 1.3. Ventajas modelo interpolado (0.5 puntos)\n",
    "¬øQu√© ventajas tiene el modelo interpolado sobre el modelo de 4-gramas simple?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6F5R3Ji7sHjt"
   },
   "source": [
    "` ` \n",
    "\n",
    "**Respuesta:** \n",
    "\n",
    "El modelo de 4-gramas simple tiene el problema de *sparse*, es decir, en un modelo 4-gramas simple ocurre que mucho par√°metros del modelo $q(w_{i} | w_{i-3}, w_{i-2}, w_{i-1})$ son nulos (porque muchos *Count* pueden ser nulos). En cambio, un modelo interpolado considera  4-gramas, trigramas, bigramas y unigramas para calcular los par√°metros $q$ del modelo y, por dicha raz√≥n, reduce la cantidad de valores nulos (dado que al utilizar 4-gramas, trigramas, bigramas y unigramas, la posibilidad de tener nulos en todos los n-gramas mencionados es mucho m√°s baja que la probabilidad de obtener nulo solo en un 4-grama).\n",
    "\n",
    "` ` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vC2BGjlXxULB"
   },
   "source": [
    "### 1.4 Evaluaci√≥n de Language Model (0.5 puntos)\n",
    "Suponga que usted est√° modelando un language model sobre un corpus con un vocabulario $\\nu$, en donde N es igual a $\\nu$ + 1. Seg√∫n lo visto en clases, ¬øC√≥mo evaluar√≠a su modelo? Y ¬øC√≥mo podr√≠a determinar que su modelo tiene un mal rendimiento?, Justifique su respuesta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnBqiOGhxYfG"
   },
   "source": [
    "` ` \n",
    "\n",
    "**Respuesta:** \n",
    "\n",
    "escriba su respuesta aqu√≠\n",
    "\n",
    "` ` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdmB-07ZKfaa"
   },
   "source": [
    "-----------------------\n",
    "## Parte 2. Na√Øve Bayes (3 pts)\n",
    "En esta parte programaremos nuestro primer clasificador de documentos. Para esto implementaremos el m√©todo **Na√Øve Bayes Multinomial** usando **Laplace Smothing**.\n",
    "\n",
    "Por favor, documenten su c√≥digo. Escriban que hacen las funciones, que reciben, que entregan, etc. Si en alg√∫n lugar de su c√≥digo hacen algo que creen que debe ser explicado, lox comentarios son bienvenidos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpjjKnJUvRiA"
   },
   "source": [
    "### 2.1. Clase para clasificador (0.5 pt)\n",
    "\n",
    "Programa una clase `MyMultinomialNB` que en su inicializador reciba el parametro de generalizaci√≥n `alpha`.\n",
    "\n",
    "```python\n",
    "class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n",
    "  def __init__(self, ...):\n",
    "    ...\n",
    "```\n",
    "\n",
    "Para m√°s informacion sobre la construcci√≥n de esta clase puedes revisar [aqu√≠](https://sklearn-template.readthedocs.io/en/latest/user_guide.html#classifier) \n",
    "\n",
    "Una llamada de ejemplo para crear un objeto de tu clase ser√≠a:\n",
    "```python\n",
    "my_clf = MyMultinomialNB(alpha=1)\n",
    "``` \n",
    "lo que debiera crear un clasificador con par√°metro `alpha` igual a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROG50eH0xfxO"
   },
   "source": [
    "### 2.2. Entrenamiento del clasificador (1 pt)\n",
    "\n",
    "Programa el entrenamiento de tu clasificador en el m√©todo `fit` de la clase `MyMultinomialNB`. La funci√≥n debiera recibir el par√°metro X que es un `DataFrame` de `pandas` con las columnas `words` y `class_`, donde `words` es una tupla con las palabras asociadas al cada documento y `class_` es el identificador de la clase a la que pertenece cada documento.\n",
    "\n",
    "Para computar el entrenamiento de nuestro clasificador debemos: \n",
    "- extraer el vocabulario,\n",
    "- determinar las probabilidades $p(c_j)$ para cada una de las clases posibles, \n",
    "- determinar las probabilidades $p(w_i|c_j)$ para cada una de las palabras y cada una de las clases usando **Laplace Smothing**.\n",
    "\n",
    "El resultado del metodo `fit` ser√° la misma instancia de nuestro clasificador `self`.\n",
    "\n",
    "```python\n",
    "class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n",
    "  def __init__(self, ...):\n",
    "    ...\n",
    "\n",
    "  def fit(self, X):\n",
    "    ...\n",
    "    return self\n",
    "```\n",
    "\n",
    "**Underflow prevention:** En vez de hacer muchas multiplicacions de `float`s, reempl√°cenlas por sumas de logaritmos para prevenir errores de precisi√≥n. Revisen la diapo 69 de las slides. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNouTCmR2FgY"
   },
   "source": [
    "### 2.3. Predicci√≥n (1 pt)\n",
    "\n",
    "Programa la predicci√≥n de tu clasificador en el m√©todo `predict` de la clase `MyMultinomialNB`. Al igual que la funci√≥n `fit`, `predict` debe recibir un `DataFrame` X con valores `None` en la columna `class_` y devolver una lista con las clases que maximizan la probabilidad de Bayes para cada uno de los elmentos de X (filas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wyhFWeLgYDI"
   },
   "source": [
    "### Implementaci√≥n 2.1, 2.2 y 2.3 (2.5 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DYFEgTyw2ELL"
   },
   "outputs": [],
   "source": [
    "# Ac√° implementar√°n las preguntas 2.1, 2.2 y 2.3,\n",
    "# tu c√≥digo debiera comenzar as√≠\n",
    "\n",
    "# importamos algunos paquetes necesarios, puede que necesites otros\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "\n",
    "class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n",
    "  def __init__(self, alpha=1.0):\n",
    "    # ac√° tu c√≥digo para inicializar el clasificador\n",
    "    self.alpha = alpha\n",
    "\n",
    "  def fit(self, X):\n",
    "    # ac√° tu c√≥digo para el entrenamiento del modelo\n",
    "    \n",
    "    # extrar vocabulario\n",
    "    words = reduce(lambda x, y: x + y, X.words)\n",
    "    vocab = set(words)\n",
    "    self.vocab = vocab\n",
    "    \n",
    "    # calcular probabilidad para cada clase\n",
    "    total_classes = len(X.class_)\n",
    "    count = Counter(list(X.class_))\n",
    "    count_list = list(count.items())\n",
    "    prob_classes = list(map(lambda x: (x[0], x[1]/total_classes), count_list))\n",
    "    self.prob_classes = prob_classes\n",
    "    \n",
    "    # calcular p(w | c) para cada palabra y  cada clase\n",
    "    prob_word_classes = {}\n",
    "    \n",
    "    for class_ in X.class_:\n",
    "        prob_word = {}\n",
    "        \n",
    "        for word in vocab:\n",
    "            # para cada palabra del vocabulario\n",
    "            # calcular p(w | c) donde es c es la actual class_\n",
    "            # p(w | c ) = count(w, c) + 1 / count(c) + |v| \n",
    "            # donde count(w, c) es la cantidad de veces que la palabra w\n",
    "            # aparece en los documentos de la clase c \n",
    "            # y |v| es la cantidad de palabras en el vocabulario (len(self.vocab))\n",
    "            \n",
    "            count_w = 1\n",
    "            count_c = len(vocab)\n",
    "            \n",
    "            for document in X.iterrows():\n",
    "                if document[1].class_ == class_:\n",
    "                    count_c += len(document[1].words)\n",
    "                    for w_d in document[1].words:\n",
    "                        if w_d == word:\n",
    "                            count_w += 1\n",
    "            prob_word[word] = count_w/count_c\n",
    "                    \n",
    "        prob_word_classes[class_] = prob_word\n",
    "    \n",
    "    self.prob_word_classes = prob_word_classes\n",
    "            \n",
    "    \n",
    "    return self\n",
    "\n",
    "  def predict(self, X):\n",
    "    # Chequea que fit ha sido ejecutado anteriormente\n",
    "    check_is_fitted(self)\n",
    "\n",
    "    # ac√° tu c√≥digo para computar la predicci√≥n\n",
    "    ...\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KOMJ-CS8PRP"
   },
   "source": [
    "### 2.4. Probando el clasificador (0.5 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hucdz-R7xerG"
   },
   "source": [
    "A continuaci√≥n probar√°n el funcionamiento de su clasificador. Para esto, les presentamos un conjunto de documentos de entrenamiento `train_set` divididos en 2 categorias distintas. Ustedes deber√°n primero entrenar su clasificador usando el m√©todo `fit` de su clase y luego, clasificar los documentos del conjunto de prueva `test_set` usando el m√©todo `predict`.\n",
    "\n",
    "**NOTA:** Como pueden ver, los objetos `namedtuple`s tienen dos atributos: `words` donde est√°n las palabras del documento y `class_` donde se guarda la clase de ese documento. Estos objetos son inmutables, lo que quiere decir que si quieren modificar un documento y cambiarle la clase, tienen que crear otro documento. Otra cosa es que son tuplas como cualquier otra, es decir se pueden acceder usando indices como `doc[0]` o `doc[1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "JjTnFLDGyCEL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple\n",
    "from nltk.tokenize import word_tokenize\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfS5wXfwxx6t"
   },
   "source": [
    "#### 2.4.1 Primer Caso: Clasificaci√≥n de ejemplo visto en clases (0.20 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 230,
     "status": "ok",
     "timestamp": 1648470177360,
     "user": {
      "displayName": "Ignacio Meza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhI9R2Sl9kpnSGYDYcmCbQ2_IwnY3_WeFFdC_YSXg=s64",
      "userId": "07738957670140287594"
     },
     "user_tz": 180
    },
    "id": "5yXBv2Kqxyyf",
    "outputId": "ac27f7bf-e30a-41cc-e79c-02710804431a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos de entrenamiento\n",
      "                          words class_\n",
      "0   (Chinese, Beijing, Chinese)      c\n",
      "1  (Chinese, Chinese, Shanghai)      c\n",
      "2              (Chinese, Macao)      c\n",
      "3       (Tokyo, Japan, Chinese)      j\n",
      "\n",
      "Documentos de prueba:\n",
      "                                       words class_\n",
      "0  (Chinese, Chinese, Chinese, Tokyo, Japan)   None\n",
      "('Chinese', 'Beijing', 'Chinese', 'Chinese', 'Chinese', 'Shanghai', 'Chinese', 'Macao', 'Tokyo', 'Japan', 'Chinese')\n"
     ]
    }
   ],
   "source": [
    "document = namedtuple(\n",
    "    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",
    ")\n",
    "\n",
    "train_set = [['Chinese Beijing\tChinese', 'c'],\n",
    "             ['Chinese\tChinese\tShanghai','c'],\n",
    "             ['Chinese\tMacao','c'],\n",
    "             ['Tokyo\tJapan\tChinese','j']]\n",
    "\n",
    "train_set = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in train_set]\n",
    "X_train = pd.DataFrame(data=train_set)\n",
    "\n",
    "words = reduce(lambda x,y: x + y, X_train.words)\n",
    "\n",
    "\n",
    "\n",
    "test_set = [['Chinese\tChinese\tChinese\tTokyo Japan', None]]\n",
    "test_set = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in test_set]\n",
    "X_test = pd.DataFrame(data=test_set)\n",
    "\n",
    "X_train = pd.DataFrame(data=train_set)\n",
    "print(\"Documentos de entrenamiento\")\n",
    "print(X_train)\n",
    "\n",
    "X_test = pd.DataFrame(data=test_set)\n",
    "print(\"\\nDocumentos de prueba:\")\n",
    "print(X_test)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "UAx0t3zQx2PJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab:  {'Shanghai', 'Chinese', 'Japan', 'Beijing', 'Tokyo', 'Macao'}\n",
      "\n",
      "Test probs:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MyMultinomialNB' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# si implementaron el m√©todo predict_proba en el clasificador (no era obligatorio), \u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ac√° lo pueden probar\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest probs:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmy_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m(X_test)]))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# obtengamos las predicciones \u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest predictions:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MyMultinomialNB' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "# Ac√° probar√°n su clasificador\n",
    "\n",
    "# inicializamos el clasificador\n",
    "my_clf = MyMultinomialNB(alpha=1)\n",
    "\n",
    "# entrenamos el clasificador para los datos de entrenamiento X_train\n",
    "my_clf.fit(X_train)\n",
    "\n",
    "# ac√° puedes ver el vocabulario extra√≠do por tu clasificador, \n",
    "# intenta tenerlo guardado en my_clf.vocab\n",
    "print('vocab: ', my_clf.vocab)\n",
    "\n",
    "# si implementaron el m√©todo predict_proba en el clasificador (no era obligatorio), \n",
    "# ac√° lo pueden probar\n",
    "print('\\nTest probs:')\n",
    "print('\\n'.join([str(l) for l in my_clf.predict_proba(X_test)]))\n",
    "\n",
    "# obtengamos las predicciones \n",
    "print('\\nTest predictions:')\n",
    "print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_clf.predict(X_test), X_test['words'])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMyCgXxvx29L"
   },
   "source": [
    "**Respuesta esperada:**\n",
    "\n",
    "**Nota:** No es necesario que obtenga exactamente la misma probabilidad, lo importante es que su clasificador genere la predicci√≥n expuesta.\n",
    "\n",
    "```python\n",
    "vocab:  ['Beijing', 'Chinese', 'Macao', 'Tokyo', 'Japan', 'Shanghai']\n",
    "\n",
    "Test probs:\n",
    "[0.68975861 0.31024139]\n",
    "\n",
    "Test predictions:\n",
    "c <- Chinese Chinese Chinese Tokyo Japan\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpG_t1wTx99m"
   },
   "source": [
    "#### 2.4.2 Segundo Caso (0.30 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1648470184312,
     "user": {
      "displayName": "Ignacio Meza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhI9R2Sl9kpnSGYDYcmCbQ2_IwnY3_WeFFdC_YSXg=s64",
      "userId": "07738957670140287594"
     },
     "user_tz": 180
    },
    "id": "HLi8PxdV2VQX",
    "outputId": "9e8d2fbd-399d-4aab-f045-a965220360f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos de entrenamiento\n",
      "                                           words  class_\n",
      "0            (w03, w01, w02, w06, w02, w08, w07)       0\n",
      "1  (w05, w04, w00, w06, w09, w07, w06, w09, w05)       1\n",
      "2  (w07, w06, w00, w08, w01, w08, w08, w09, w02)       0\n",
      "3            (w08, w09, w02, w06, w05, w08, w07)       1\n",
      "4            (w09, w08, w05, w08, w05, w00, w08)       1\n",
      "5            (w05, w05, w06, w01, w06, w08, w02)       1\n",
      "6            (w04, w03, w07, w05, w04, w00, w02)       0\n",
      "7       (w01, w00, w01, w04, w09, w02, w04, w07)       1\n",
      "\n",
      "Documentos de prueba:\n",
      "                                      words class_\n",
      "0  (w02, w09, w06, w01, w05, w04, w03, w03)   None\n",
      "1  (w03, w03, w04, w05, w01, w06, w09, w02)   None\n"
     ]
    }
   ],
   "source": [
    "document = namedtuple(\n",
    "    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",
    ")\n",
    "\n",
    "train_set = (\n",
    "    document(words=('w03', 'w01', 'w02', 'w06', 'w02', 'w08', 'w07'), class_=0),\n",
    "    document(words=('w05', 'w04', 'w00', 'w06', 'w09', 'w07', 'w06', 'w09', 'w05'), class_=1),\n",
    "    document(words=('w07', 'w06', 'w00', 'w08', 'w01', 'w08', 'w08', 'w09', 'w02'), class_=0),\n",
    "    document(words=('w08', 'w09', 'w02', 'w06', 'w05', 'w08', 'w07'), class_=1),\n",
    "    document(words=('w09', 'w08', 'w05', 'w08', 'w05', 'w00', 'w08'), class_=1),\n",
    "    document(words=('w05', 'w05', 'w06', 'w01', 'w06', 'w08', 'w02'), class_=1),\n",
    "    document(words=('w04', 'w03', 'w07', 'w05', 'w04', 'w00', 'w02'), class_=0),\n",
    "    document(words=('w01', 'w00', 'w01', 'w04', 'w09', 'w02', 'w04', 'w07'), class_=1)\n",
    ")\n",
    "X_train = pd.DataFrame(data=train_set)\n",
    "print(\"Documentos de entrenamiento\")\n",
    "print(X_train)\n",
    "\n",
    "test_set = (document(words=('w02', 'w09', 'w06', 'w01', 'w05', 'w04', 'w03', 'w03'), class_=None),\n",
    "            document(words=('w03', 'w03', 'w04', 'w05', 'w01', 'w06', 'w09', 'w02'), class_=None))\n",
    "X_test = pd.DataFrame(data=test_set)\n",
    "print(\"\\nDocumentos de prueba:\")\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXHwmOWB-4Aa"
   },
   "outputs": [],
   "source": [
    "# Ac√° probar√°n su clasificador\n",
    "\n",
    "# inicializamos el clasificador\n",
    "my_clf = ...\n",
    "\n",
    "# entrenamos el clasificador para los datos de entrenamiento X_train\n",
    "my_clf.fit(...)\n",
    "\n",
    "# ac√° puedes ver el vocabulario extra√≠do por tu clasificador, \n",
    "# intenta tenerlo guardado en my_clf.vocab\n",
    "print('vocab: ', my_clf.vocab)\n",
    "\n",
    "# si implementaron el m√©todo predict_proba en el clasificador (no era obligatorio), \n",
    "# ac√° lo pueden probar\n",
    "# print('\\nTest probs:')\n",
    "# print('\\n'.join([str(l) for l in my_clf.predict_proba(X_test)]))\n",
    "\n",
    "# obtengamos las predicciones \n",
    "print('\\nTest predictions:')\n",
    "print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_clf.predict(X_test), X_test['words'])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tDZnmns_1dW"
   },
   "source": [
    "#### 2.4.3 (OPCIONAL) Oraciones reales\n",
    "\n",
    "Aqu√≠ intentaremos entrenar un clasificador para determinar cuando una oracion en ingl√©s es interrogativa, afirmativa o negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1648470188138,
     "user": {
      "displayName": "Ignacio Meza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhI9R2Sl9kpnSGYDYcmCbQ2_IwnY3_WeFFdC_YSXg=s64",
      "userId": "07738957670140287594"
     },
     "user_tz": 180
    },
    "id": "YCWi3oytd2nf",
    "outputId": "ff582f23-6756-4d7d-fe64-f1add154b4c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Documentos de entrenamiento:\n",
      "                                                words class_\n",
      "0                (Do, you, have, plenty, of, time, ?)      ?\n",
      "1                 (Does, she, have, enough, money, ?)      ?\n",
      "2           (Did, they, have, any, useful, advice, ?)      ?\n",
      "3                           (What, day, is, today, ?)      ?\n",
      "4                      (I, do, n't, have, much, time)      -\n",
      "5                  (She, does, n't, have, any, money)      -\n",
      "6      (They, did, n't, have, any, advice, to, offer)      -\n",
      "7                    (Have, you, plenty, of, time, ?)      ?\n",
      "8                        (Has, she, enough, money, ?)      ?\n",
      "9                 (Had, they, any, useful, advice, ?)      ?\n",
      "10                         (I, have, n't, much, time)      -\n",
      "11                        (She, has, n't, any, money)      -\n",
      "12             (He, had, n't, any, advice, to, offer)      -\n",
      "13                                 (How, are, you, ?)      ?\n",
      "14    (How, do, you, make, questions, in, English, ?)      ?\n",
      "15             (How, long, have, you, lived, here, ?)      ?\n",
      "16      (How, often, do, you, go, to, the, cinema, ?)      ?\n",
      "17                    (How, much, is, this, dress, ?)      ?\n",
      "18                            (How, old, are, you, ?)      ?\n",
      "19     (How, many, people, came, to, the, meeting, ?)      ?\n",
      "20                            (I, ‚Äô, m, from, France)      +\n",
      "21                           (I, come, from, the, UK)      +\n",
      "22               (My, phone, number, is, 61709832145)      +\n",
      "23  (I, work, as, a, tour, guide, for, a, local, t...      +\n",
      "24                     (I, ‚Äô, m, not, dating, anyone)      -\n",
      "25           (I, live, with, my, wife, and, children)      +\n",
      "26        (I, often, do, morning, exercises, at, 6am)      +\n",
      "27                                 (I, run, everyday)      +\n",
      "28                         (She, walks, very, slowly)      +\n",
      "29               (They, eat, a, lot, of, meat, daily)      +\n",
      "30                  (We, were, in, France, that, day)      +\n",
      "31                           (He, speaks, very, fast)      +\n",
      "32          (They, told, us, they, came, back, early)      +\n",
      "33                  (I, told, her, I, 'll, be, there)      +\n",
      "\n",
      "Documentos de prueba:\n",
      "                                                words class_\n",
      "0                (Do, you, know, who, lives, here, ?)      ?\n",
      "1                             (What, time, is, it, ?)      ?\n",
      "2    (Can, you, tell, me, where, she, comes, from, ?)      ?\n",
      "3                                  (How, are, you, ?)      ?\n",
      "4                              (I, fill, good, today)      +\n",
      "5              (There, is, a, lot, of, history, here)      +\n",
      "6                              (I, love, programming)      +\n",
      "7      (He, told, us, not, to, make, so, much, noise)      +\n",
      "8   (We, were, asked, not, to, park, in, front, of...      +\n",
      "9                      (I, do, n't, have, much, time)      -\n",
      "10                 (She, does, n't, have, any, money)      -\n",
      "11     (They, did, n't, have, any, advice, to, offer)      -\n",
      "12                         (I, am, not, really, sure)      +\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "document = namedtuple(\n",
    "    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",
    ")\n",
    "\n",
    "train_set2 = [\n",
    "              ['Do you have plenty of time?', '?'],\n",
    "              ['Does she have enough money?','?'],\n",
    "              ['Did they have any useful advice?','?'],\n",
    "              ['What day is today?','?'],\n",
    "              [\"I don't have much time\",'-'],\n",
    "              [\"She doesn't have any money\",'-'],\n",
    "              [\"They didn't have any advice to offer\",'-'],\n",
    "              ['Have you plenty of time?','?'],\n",
    "              ['Has she enough money?','?'],\n",
    "              ['Had they any useful advice?','?'],\n",
    "              [\"I haven't much time\",'-'],\n",
    "              [\"She hasn't any money\",'-'],\n",
    "              [\"He hadn't any advice to offer\",'-'],\n",
    "              ['How are you?','?'],\n",
    "              ['How do you make questions in English?','?'],\n",
    "              ['How long have you lived here?','?'],\n",
    "              ['How often do you go to the cinema?','?'],\n",
    "              ['How much is this dress?','?'],\n",
    "              ['How old are you?','?'],\n",
    "              ['How many people came to the meeting?','?'],\n",
    "              ['I‚Äôm from France','+'],\n",
    "              ['I come from the UK','+'],\n",
    "              ['My phone number is 61709832145','+'],\n",
    "              ['I work as a tour guide for a local tour company','+'],\n",
    "              ['I‚Äôm not dating anyone','-'],\n",
    "              ['I live with my wife and children','+'],\n",
    "              ['I often do morning exercises at 6am','+'],\n",
    "              ['I run everyday','+'],\n",
    "              ['She walks very slowly','+'],\n",
    "              ['They eat a lot of meat daily','+'],\n",
    "              ['We were in France that day', '+'],\n",
    "              ['He speaks very fast', '+'],\n",
    "              ['They told us they came back early', '+'],\n",
    "              [\"I told her I'll be there\", '+']\n",
    "]\n",
    "train_set2 = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in train_set2]\n",
    "X_train2 = pd.DataFrame(data=train_set2)\n",
    "print(\"Documentos de entrenamiento:\")\n",
    "print(X_train2)\n",
    "\n",
    "test_set2 = [\n",
    "             ['Do you know who lives here?','?'],\n",
    "             ['What time is it?','?'],\n",
    "             ['Can you tell me where she comes from?','?'],\n",
    "             ['How are you?','?'],\n",
    "             ['I fill good today', '+'],\n",
    "             ['There is a lot of history here','+'],\n",
    "             ['I love programming','+'],\n",
    "             ['He told us not to make so much noise','+'],  # interesing case\n",
    "             ['We were asked not to park in front of the house','+'],  # interesing case\n",
    "             [\"I don't have much time\",'-'],\n",
    "             [\"She doesn't have any money\",'-'],\n",
    "             [\"They didn't have any advice to offer\",'-'],\n",
    "             ['I am not really sure','+']\n",
    "]\n",
    "test_set2 = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in test_set2]\n",
    "X_test2 = pd.DataFrame(data=test_set2)\n",
    "print(\"\\nDocumentos de prueba:\")\n",
    "print(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Wdp22w2ArUl"
   },
   "outputs": [],
   "source": [
    "# Ac√° probar√°n su clasificador y computaremos algunas m√©tricas de evaluacion\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# inicializamos el clasificador\n",
    "my_clf2 = ...\n",
    "\n",
    "# entrenamos el clasificador para los datos de entrenamiento X_train2\n",
    "my_clf2.fit(...)\n",
    "\n",
    "# ac√° puedes ver el vocabulario extra√≠do por tu clasificador, \n",
    "# intenta tenerlo guardado en my_clf.vocab\n",
    "print('vocab: ', len(my_clf2.vocab), my_clf2.vocab)\n",
    "\n",
    "# si implementaron el m√©todo predict_proba en el clasificador (no era obligatorio), \n",
    "# ac√° lo pueden probar\n",
    "# print('\\nTest probs:')\n",
    "# print('\\n'.join([str(l) for l in my_clf.predict_proba(X_test2)]))\n",
    "\n",
    "# obtengamos las predicciones para X_test2\n",
    "print('\\nTest predictions:')\n",
    "my_y_preds = my_clf2.predict(X_test2)\n",
    "print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_y_preds, X_test2['words'])]))\n",
    "print(classification_report(y_true=X_test2['class_'], y_pred=my_y_preds, target_names=['?', '+', '-']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXbg6sNTdAlO"
   },
   "source": [
    "**Respuesta aproximada:**\n",
    "\n",
    "**Nota:** No es necesario que obtenga exactamente los mismos resultados.\n",
    "\n",
    "```python\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           ?       1.00      0.67      0.80         6\n",
    "           +       0.75      1.00      0.86         3\n",
    "           -       0.80      1.00      0.89         4\n",
    "\n",
    "    accuracy                           0.85        13\n",
    "   macro avg       0.85      0.89      0.85        13\n",
    "weighted avg       0.88      0.85      0.84        13\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "JstKx3TiKcIp",
    "2hwW-07MrRpt",
    "gXSFlCIex8kq",
    "bwNkPIXure0C",
    "sHqcRJ7Vr_8x",
    "vC2BGjlXxULB",
    "rdmB-07ZKfaa",
    "8wyhFWeLgYDI",
    "JfS5wXfwxx6t",
    "mpG_t1wTx99m",
    "5tDZnmns_1dW"
   ],
   "name": "Tarea 2 - Language Modeling y Na√Øve Bayes.ipynb",
   "provenance": [
    {
     "file_id": "1kQCSkkokIVSZkHeo8sGEKwn6ROSItQme",
     "timestamp": 1618336151140
    },
    {
     "file_id": "1jZG4e_poENFYDeljHjx9goskyQ3Jy3GU",
     "timestamp": 1618284831252
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
