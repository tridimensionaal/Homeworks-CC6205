{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ckbt7VPDhBwb"
   },
   "source": [
    "# **Tarea 3 - Word Embeddings üìö**\n",
    "\n",
    "**Integrantes:**\n",
    "\n",
    "**Fecha l√≠mite de entrega üìÜ:** 3 de mayo.\n",
    "\n",
    "**Tiempo estimado de dedicaci√≥n:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T18:30:18.109327Z",
     "start_time": "2020-03-19T18:30:18.103344Z"
    },
    "id": "q5CSRY4oNCHK"
   },
   "source": [
    "\n",
    "**Instrucciones:**\n",
    "- El ejercicio consiste en:\n",
    "    - Responder preguntas relativas a los contenidos vistos en los v√≠deos y slides de las clases. \n",
    "    - Entrenar Word2Vec y Word Context Matrix sobre un peque√±o corpus.\n",
    "    - Evaluar los embeddings obtenidos en una tarea de clasificaci√≥n.\n",
    "- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
    "- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n",
    "- El formato de entrega es este mismo Jupyter Notebook.\n",
    "- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n. \n",
    "- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav√©s del canal de Discord del curso. \n",
    "\n",
    "\n",
    "**Referencias**\n",
    "\n",
    "V√≠deos: \n",
    "\n",
    "- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n",
    "- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n",
    "- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4wYf0vgnbTv"
   },
   "source": [
    "## **Preguntas te√≥ricas üìï (2 puntos).** ##\n",
    "Para estas preguntas no es necesario implementar c√≥digo, pero pueden utilizar pseudo c√≥digo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5hUG6-8ngoK"
   },
   "source": [
    "### **Parte 1: Modelos Lineales (1 ptos)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yRvZbhsoi8f"
   },
   "source": [
    "Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categor√≠as: pol√≠tica, deporte, negocios y otros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irsqBVmCnx3M"
   },
   "source": [
    "**Pregunta 1**: Dise√±e un modelo lineal capaz de clasificar un documento seg√∫n estas categor√≠as donde el output sea un vector con una distribuci√≥n de probabilidad con la pertenencia a cada clase. \n",
    "\n",
    "Especifique: representaci√≥n de los documentos de entrada, par√°metros del modelo, transformaciones necesarias para obtener la probabilidad de cada etiqueta y funci√≥n de p√©rdida escogida. **(0.5 puntos)**\n",
    "\n",
    "**Respuesta**:\n",
    "\n",
    "En primer lugar, la representaci√≥n de los documentos de entrada ser√° mediante vectores *word count*, es decir, dado el vocabulario del modelo, la representaci√≥n de un documento ser√° un vector del largo del vocabulario donde cada columna representar√° una palabra del vocabulario y el valor en dicha columna ser√° la cantidad de veces que aparece la palabra asociada a la columna en el documento del input.\n",
    "\n",
    "Los par√°metros del modelo ser√°n la matriz $W$ y el vector $\\vec{b}$. La matriz $W$ ser√° una matriz $W \\in \\mathcal{R}^{N\\times4}$ donde $N$ es el tama√±o del vocabulario y donde cada vector de una fila representar√° la *importancia* o *peso* de la palabra asociada a dicha fila para las clases asociadas a cada columna. El vector $\\vec{b}$ ser√° una vector $\\vec{b} \\in \\mathbb{R}^{4}$. De esa forma, se podr√° modelar el problema mediante una funci√≥n linear de la siguiente forma.\n",
    "\\begin{equation}\n",
    "f(\\vec{x}) = \\vec{x} \\cdot W + \\vec{b}\n",
    "\\end{equation}\n",
    "\n",
    "Ahora, para que la funci√≥n $f(x) = \\vec{\\hat{y}}$ represente una distribuci√≥n de probabilidad que indique la probabilidad de pertenencia del documento asociado al vector $x$ a cada una de las 4 clases, se necesita aplicar *softmax* tal que:\n",
    "\\begin{equation}\n",
    "softmax(\\vec{x})_{[i]} = \\frac{e^{\\vec{x}_{[i]}}}{\\sum_{j}^{} e^{\\vec{x}_{[j]}}}\n",
    "\\end{equation}\n",
    "\n",
    "As√≠, vector $\\vec{\\hat{y}}$ queda definido de la siguiente forma:\n",
    "\\begin{equation}\n",
    "\\vec{\\hat{y}} = softmax(\\vec{x} \\cdot W + \\vec{b})\n",
    "\\end{equation}\n",
    "\n",
    "Con *softmax*, el vector $\\vec{\\hat{y}}$ representa una distribuci√≥n de probabilidad que indica la probabilidad de pertenencia del documento asociado al vector $x$ a cada una de las 4 clases, es decir, cada columna del vector $\\vec{\\hat{y}}$ representa la probabilidad de que el documento pertenezca a la clase asociada a dicha columna y, adem√°s, al ser un vector que representa una distribuci√≥n de probabilidad, la suma de todas las columnas del vector deben dar 1.\n",
    "\n",
    "Finalmente, la funci√≥n de p√©rdida a utilizar ser√° la funci√≥n *cross-entropy loss function*:\n",
    "\\begin{equation}\n",
    "L_{cross-entropy}(\\vec{\\hat{y}}, \\vec{y}) = - \\sum_{i}^{}\\vec{y}_{[i]}\\cdot \\log(\\vec{\\hat{y}}_{[i]})\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5FaWqBVvL90"
   },
   "source": [
    "**Pregunta 2**: Explique c√≥mo funciona el proceso de entrenamiento en este tipo de modelos y su evaluaci√≥n. **(0.5 puntos)**\n",
    "\n",
    "**Respuesta**: \n",
    "\n",
    "Siguiendo con el ejemplo de la pregunta anterior, el proceso de entrenamiento consiste en encontrar los par√°metros $\\hat{W}$ y $\\hat{B}$ tales que la suma de las funciones de p√©rdida para los documentos del dataset y sus respectivas predicciones sea m√≠nima.\n",
    "\n",
    "Es decir, definiendo el conjunto de par√°metros como $\\Theta$ y definiendo la funci√≥n $\\mathcal{L}(\\Theta)$ de la forma:\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\Theta) = \\frac{1}{n} \\sum_{i=1}^{n} L(f(\\vec{x};\\Theta), y_{i})\n",
    "\\end{equation}\n",
    "donde $L$ es la funci√≥n de p√©rdida escogida y $n$ es la cantidad de inputs en el dataset de entrenamiento, el proceso de entrenamiento del modelo consiste en encontrar $\\hat{\\Theta}$ tal que la funci√≥n\n",
    "\\begin{equation}\n",
    "\\hat{\\Theta} = argmin_{\\Theta} \\mathcal{L}(\\Theta) =  argmin_{\\Theta} \\frac{1}{n} \\sum_{i=1}^{n} L(f(\\vec{x};\\Theta), y_{i})\n",
    "\\end{equation}\n",
    "\n",
    "Luego, para evaluar el modelo, se puede particionar el dataset original en tres dataset distintos: un dataset de entrenamiento, un dataset de validaci√≥n y un dataset de testing. \n",
    "- El dataset de entrenamiento se utiliza para entrenar el modelo de la forma explicada anteriormente.\n",
    "- El dataset de validaci√≥n se utilza para *tunear* y *evaluar* el modelo generado comparando las predicciones realizadas por el modelo entrenado y las clases originales de los datos, es decir, dado el modelo *creado*, se *pasan* los datos por el modelo, se obtienen las predicciones y se comparan las predicciones con las *clases* originales de los datos.\n",
    "- El dataset de testeo se utiliza para evaluar la performance del modelo *final* (tambi√©n comparando las predicciones realizadas por el modelo entrenado y las clases originales de los datos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkK7pc54njZq"
   },
   "source": [
    "### **Parte 2: Redes Neuronales (1 ptos)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUbJjlj_9AFC"
   },
   "source": [
    "Supongamos que tenemos la siguiente red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obUfuOYB_TOC"
   },
   "source": [
    "![image.png](https://drive.google.com/uc?export=view&id=1nV1G0dOeVGPn40qGcGF9l_pVEFNtLU-w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2z-8zKW0_6q"
   },
   "source": [
    "**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matem√°tica. Dada la red neuronal anterior, defina la salida $\\vec{\\hat{y}}$ en funci√≥n del vector $\\vec{x}$, pesos $W^i$, bias $b^i$ y funciones $g,f,h$. \n",
    "\n",
    "Adicionalmente liste y explicite las dimensiones de cada matriz y vector involucrado en la red neuronal. **(0.5 Puntos)**\n",
    "\n",
    "**Respuesta**: \n",
    "\n",
    "Formula:\n",
    "$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) = h(g(f(\\vec{x}W^{1} + \\vec{b^1})W^2 + \\vec{b^2})W^3 + \\vec{b^3})W^4 + \\vec{b^4}$\n",
    "\n",
    "Dimensiones:\n",
    "- $\\vec{x} \\in \\mathcal{R}^{3}$\n",
    "- $f: \\mathcal{R}^{2} \\rightarrow \\mathcal{R}^{2}$\n",
    "- $g: \\mathcal{R}^{3} \\rightarrow \\mathcal{R}^{3}$\n",
    "- $h: \\mathcal{R}^{1} \\rightarrow \\mathcal{R}^{1}$\n",
    "- $W^1 \\in \\mathcal{R}^{3\\times2} \\text{, } b^1 \\in \\mathcal{R}^{2} $\n",
    "- $W^2 \\in \\mathcal{R}^{2\\times3} \\text{, } b^2 \\in \\mathcal{R}^{3} $\n",
    "- $W^3 \\in \\mathcal{R}^{3\\times1} \\text{, } b^3 \\in \\mathcal{R}^{1} $\n",
    "- $W^4 \\in \\mathcal{R}^{1\\times4} \\text{, } b^4 \\in \\mathcal{R}^{4} $\n",
    "\n",
    "**Pregunta 2**: Explique qu√© es backpropagation. ¬øCuales ser√≠an los par√°metros a evaluar en la red neuronal anterior durante backpropagation? **(0.25 puntos)**\n",
    "\n",
    "**Respuesta**:\n",
    "\n",
    "Backpropagation es un m√©todo para entrenar redes neuronales que permite calcular de manera eficiente el gradiente de una funci√≥n de p√©rdida respecto a todos sus par√°metros. Backpropagation es eficaz ya que calcula las derivadas parciales respecto a cada *peso* mediante la regla de la cadena, calculando los gradientes de manera secuencial para cada capa, partiendo desde las capas exteriores hacia las capas inferiores para evitar c√°lculos redundantes de t√©rminos intermedios en la regla de la cadena.\n",
    "\n",
    "En la red neuronal anterior los par√°metros a evaluar durante backpropagation son $W^1$, $\\vec{b^1}$, $W^2$, $\\vec{b^2}$, $W^3$, $\\vec{b^3}$, $W^4$ y $\\vec{b^4}$ (se pueden *agregar* los \n",
    "\n",
    "**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? **(0.25 puntos)**\n",
    "\n",
    "**Respuesta**:\n",
    "\n",
    "- Se calcularon los outputs de la red, es decir, desde la *input layer* se pasan los inputs a trav√©s de las *hidden layer* hasta la *ouput layer*.\n",
    "- Luego, se quiere calcular las siguientes derivadas parciales:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{l}_{[i,j]}} = \\vec{\\delta^{l}_{[j]}}\\times\\vec{z^{(l-1)}_{[i]}}\n",
    "\\end{equation}\n",
    "\n",
    "- N√≥tese que los valores $\\vec{z^{(l-1)}_{[i]}}$ fueron calculados inicialmente, por lo que es necesario calcular $\\vec{\\delta^{l}_{[j]}}$\n",
    "- $\\vec{\\delta^{l}_{[j]}}$ viene dado por:\n",
    "\\begin{equation}\n",
    "\\vec{\\delta^{l}_{[j]}} = g^{l}(h^{l}_{[j]}) \\times \\sum_{k}{}\\left(\\vec{\\delta^{l}_{[k]}} \\times W^{l+1}_{[j,k]} \\right)\n",
    "\\end{equation}\n",
    "- Como $\\delta^{l}_{[j]}$ depende los $\\delta^{l+1}_{[k]}$ se aplica la l√≥gica de backpropagation: las derivadas parciales de la capa m√°s superior son directas de calcular. Luego, con dichas derivadas se calculan las derivadas de las capas inferiores (que depende de una derivada de una capa superior que ya est√° calculada). As√≠, se calculan las derivadas de manera eficiente.\n",
    "\n",
    "En el ejemplo anterior, se tienen que calcular las derivadas parciales de la funci√≥n de p√©rdida con respecto a los √∫ltimos nodos de la red y, con dichas derivadas, calcular las derivadas de los nodos internos. As√≠, solo se realizaron c√°lculos directos de cuatro derivadas (asociadas a los nodos de la capa superior) y luego, las derivadas de las capas inferiores son calculadas de forma *indirecta* utilizando las derivadas ya calculadas.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocS_vQhR1gcU"
   },
   "source": [
    "## **Preguntas pr√°cticas üíª (4 puntos).** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ol82nJ0FnmcP"
   },
   "source": [
    "### **Parte 3: Word Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Daw7Ee5cdQTb"
   },
   "source": [
    "En la auxiliar 2 se nombraron dos formas de crear word vectors:\n",
    "\n",
    "-  Distributional Vectors.\n",
    "-  Distributed Vectors.\n",
    "\n",
    "El objetivo de esta parte es comparar las dos embeddings obtenidos de estas dos estrategias en una tarea de clasificaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "E2G1qcb7AJqW"
   },
   "outputs": [],
   "source": [
    "import re  \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict \n",
    "import string \n",
    "import multiprocessing\n",
    "import os\n",
    "import gensim\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuEAv-whdMCG"
   },
   "source": [
    "#### **Parte A (1 punto)** \n",
    "\n",
    "En esta parte debe crear una matriz palabra contexto, para esto, complete el siguiente template (para esta parte puede utilizar las librer√≠as ```numpy``` y/o ```scipy```). Hint: revise como utilizar matrices sparse de ```scipy```\n",
    "\n",
    "```python\n",
    "class WordContextMatrix:\n",
    "\n",
    "  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Utilice el constructor para definir los parametros.\n",
    "    \"\"\"\n",
    "\n",
    "    # se sugiere agregar un una estructura de datos para guardar las\n",
    "    # palabras del vocab y para guardar el conteo de coocurrencia\n",
    "    ...\n",
    "    \n",
    "  def add_word_to_vocab(self, word):\n",
    "    \"\"\"\n",
    "    Utilice este m√©todo para agregar token\n",
    "    a sus vocabulario\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Le puede ser √∫til considerar un token unk al vocab\n",
    "    # para palabras fuera del vocab\n",
    "    ...\n",
    "  \n",
    "  def build_matrix(self):\n",
    "    \"\"\"\n",
    "    Utilice este m√©todo para crear la palabra contexto\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "  def matrix2dict(self):\n",
    "    \"\"\"\n",
    "    Utilice este m√©todo para convertir la matriz a un diccionario de embeddings, donde las llaves deben ser los token del vocabulario y los embeddings los valores obtenidos de la matriz. \n",
    "    \"\"\"\n",
    "\n",
    "    # se recomienda transformar la matrix a un diccionario de embedding.\n",
    "    # por ejemplo {palabra1:vec1, palabra2:vec2, ...}\n",
    "    ...\n",
    "\n",
    "```\n",
    "\n",
    "puede modificar los par√°metros o m√©todos si lo considera necesario. Para probar la matrix puede utilizar el siguiente corpus.\n",
    "\n",
    "```python\n",
    "corpus = [\n",
    "  \"I like deep learning.\",\n",
    "  \"I like NLP.\",\n",
    "  \"I enjoy flying.\"\n",
    "]\n",
    "```\n",
    "\n",
    "Obteniendo una matriz parecia a esta:\n",
    "\n",
    "***Resultado esperado***: \n",
    "\n",
    "| counts   | I  | like | enjoy | deep | learning | NLP | flying | . |   \n",
    "|----------|---:|-----:|------:|-----:|---------:|----:|-------:|--:|\n",
    "| I        | 0  |  2   |  1    |    0 |  0       |   0 | 0      | 0|            \n",
    "| like     |  2 |    0 |  0    |    1 |  0       |   1 | 0      | 0 | \n",
    "| enjoy    |  1 |    0 |  0    |    0 |  0       |   0 | 1      | 0 |\n",
    "| deep     |  0 |    1 |  0    |    0 |  1       |   0 | 0      | 0 |  \n",
    "| learning |  0 |    0 |  0    |    1 |  0       |   0 | 0      | 1 |          \n",
    "| NLP      |  0 |    1 |  0    |    0 |  0       |   0 | 0      | 1 |\n",
    "| flying   |  0 |    0 |  1    |    0 |  0       |   0 | 0      | 1 | \n",
    "| .        |  0 |    0 |  0    |    0 |  1       |   1 | 1      | 0 | \n",
    "\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ur16vkyO37B5"
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOI1FL8MlGZB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgmeSFqKLpFL"
   },
   "source": [
    "#### **Parte B (1.5 puntos)**\n",
    "\n",
    "En esta parte es debe entrenar Word2Vec de gensim y construir la matriz palabra contexto utilizando el dataset de di√°logos de los Simpson. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZgN06q4QPi3"
   },
   "source": [
    "Utilizando el dataset adjunto con la tarea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eY3kmg4onnsu",
    "outputId": "d3525a54-0c10-401e-b3e2-9c6e9e714a2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-05 17:58:59,568 : INFO : NumExpr defaulting to 2 threads.\n"
     ]
    }
   ],
   "source": [
    "data_file = \"dialogue-lines-of-the-simpsons.zip\"\n",
    "df = pd.read_csv(data_file)\n",
    "stopwords = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
    ").values\n",
    "stopwords = Counter(stopwords.flatten().tolist())\n",
    "df = df.dropna().reset_index(drop=True) # Quitar filas vacias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAg5a5bmWk3T"
   },
   "source": [
    "**Pregunta 1**: Ayud√°ndose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec. **(0.75 punto)** (Hint, le puede servir explorar un poco los datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWw2fXFRXe5Y"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bvwplz7yTNcr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vBkF3hreGjg"
   },
   "source": [
    "**Pregunta 2**: Cree una matriz palabra contexto usando el mismo dataset. Configure el largo del vocabulario a 1000 o 2000 tokens, puede agregar valores mayores pero tenga en cuenta que la construcci√≥n de la matriz puede tomar varios minutos. Puede que esto tarde un poco. **(0.75 punto)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzLuH6MneWIY"
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gPyW8fMeXNX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRCB-jqgTNcs"
   },
   "source": [
    "#### **Parte C (1.5 puntos): Aplicar embeddings para clasificar**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlqzlJRSTNcs"
   },
   "source": [
    "Ahora utilizaremos los embeddings que acabamos de calcular para clasificar palabras basadas en su polaridad (positivas o negativas). \n",
    "\n",
    "Para esto ocuparemos el lexic√≥n AFINN incluido en la tarea, que incluye una lista de palabras y un 1 si su connotaci√≥n es positiva y un -1 si es negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMskFDmHTNcs"
   },
   "outputs": [],
   "source": [
    "AFINN = 'AFINN_full.csv'\n",
    "df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaKl8hsCTNcs"
   },
   "source": [
    "Hint: Para w2v y la wcm son esperables KeyErrors debido a que no todas las palabras del corpus de los simpsons tendr√°n una representaci√≥n en AFINN. Para el caso de la matriz palabra contexto se recomienda convertir su matrix a un diccionario. Pueden utilizar esta funci√≥n auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWSSuctiTNcs"
   },
   "outputs": [],
   "source": [
    "def try_apply(model,word):\n",
    "    try:\n",
    "        aux = model[word]\n",
    "        return True\n",
    "    except KeyError:\n",
    "        #logger.error('Word {} not in dictionary'.format(word))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrVPeEzgTNcs"
   },
   "source": [
    "**Pregunta 1**: Transforme las palabras del corpus de AFINN a la representaci√≥n en embedding que acabamos de calcular (con ambos modelos). \n",
    "\n",
    "Su dataframe final debe ser del estilo [embedding, sentimiento], donde los embeddings corresponden a $X$ y el sentimiento asociado con el embedding a $y$ (positivo/negativo, 1/-1). \n",
    "\n",
    "Para ambos modelos, separar train y test de acuerdo a la siguiente funci√≥n. **(0.5 puntos)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Bkt26BwTNcs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmFoKWKO2EKA"
   },
   "source": [
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDcq5czXTNct"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upAn_eT4TNct"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDKe4gA3TNct"
   },
   "source": [
    "**Pregunta 2**: Entrenar una regresi√≥n log√≠stica (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qu√© se obtienen estos resultados? C√≥mo los mejorar√≠as? Como podr√≠as mejorar los resultados de la matriz palabra contexto? es equivalente al modelo word2vec? **(1 punto)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJMzq_dETNct"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bj1r_BnKn_7L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izppruGQTNct"
   },
   "source": [
    "# Bonus: +0.25 puntos en cualquier pregunta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YW0aeK2KTNct"
   },
   "source": [
    "**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset m√°s grande y obtener mejores resultados. Les puede servir [√©sta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim **(0.25 puntos)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvHcVS3sTNct"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSc8p-T8TNcu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tarea_3_Word_Embeddings.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
