{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ckbt7VPDhBwb"
   },
   "source": [
    "# **Tarea 3 - Word Embeddings 📚**\n",
    "\n",
    "**Integrantes:**\n",
    "\n",
    "**Fecha límite de entrega 📆:** 3 de mayo.\n",
    "\n",
    "**Tiempo estimado de dedicación:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T18:30:18.109327Z",
     "start_time": "2020-03-19T18:30:18.103344Z"
    },
    "id": "q5CSRY4oNCHK"
   },
   "source": [
    "\n",
    "**Instrucciones:**\n",
    "- El ejercicio consiste en:\n",
    "    - Responder preguntas relativas a los contenidos vistos en los vídeos y slides de las clases. \n",
    "    - Entrenar Word2Vec y Word Context Matrix sobre un pequeño corpus.\n",
    "    - Evaluar los embeddings obtenidos en una tarea de clasificación.\n",
    "- La tarea se realiza en grupos de **máximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
    "- La entrega es a través de u-cursos a más tardar el día estipulado arriba. No se aceptan atrasos.\n",
    "- El formato de entrega es este mismo Jupyter Notebook.\n",
    "- Al momento de la revisión tu código será ejecutado. Por favor verifica que tu entrega no tenga errores de compilación. \n",
    "- En el horario de auxiliar pueden realizar consultas acerca de la tarea a través del canal de Discord del curso. \n",
    "\n",
    "\n",
    "**Referencias**\n",
    "\n",
    "Vídeos: \n",
    "\n",
    "- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n",
    "- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n",
    "- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4wYf0vgnbTv"
   },
   "source": [
    "## **Preguntas teóricas 📕 (2 puntos).** ##\n",
    "Para estas preguntas no es necesario implementar código, pero pueden utilizar pseudo código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5hUG6-8ngoK"
   },
   "source": [
    "### **Parte 1: Modelos Lineales (1 ptos)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yRvZbhsoi8f"
   },
   "source": [
    "Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categorías: política, deporte, negocios y otros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irsqBVmCnx3M"
   },
   "source": [
    "**Pregunta 1**: Diseñe un modelo lineal capaz de clasificar un documento según estas categorías donde el output sea un vector con una distribución de probabilidad con la pertenencia a cada clase. \n",
    "\n",
    "Especifique: representación de los documentos de entrada, parámetros del modelo, transformaciones necesarias para obtener la probabilidad de cada etiqueta y función de pérdida escogida. **(0.5 puntos)**\n",
    "\n",
    "**Respuesta**:\n",
    "\n",
    "En primer lugar, la representación de los documentos de entrada será mediante vectores *word count*, es decir, dado el vocabulario del modelo, la representación de un documento será un vector del largo del vocabulario donde cada columna representará una palabra del vocabulario y el valor en dicha columna será la cantidad de veces que aparece la palabra asociada a la columna en el documento del input.\n",
    "\n",
    "Los parámetros del modelo serán la matriz $W$ y el vector $\\vec{b}$. La matriz $W$ será una matriz $W \\in \\mathcal{R}^{N\\times4}$ donde $N$ es el tamaño del vocabulario y donde cada vector de una fila representará la *importancia* o *peso* de la palabra asociada a dicha fila para las clases asociadas a cada columna. El vector $\\vec{b}$ será una vector $\\vec{b} \\in \\mathbb{R}^{4}$. De esa forma, se podrá modelar el problema mediante una función linear de la siguiente forma.\n",
    "\\begin{equation}\n",
    "f(\\vec{x}) = \\vec{x} \\cdot W + \\vec{b}\n",
    "\\end{equation}\n",
    "\n",
    "Ahora, para que la función $f(x) = \\vec{\\hat{y}}$ represente una distribución de probabilidad que indique la probabilidad de pertenencia del documento asociado al vector $x$ a cada una de las 4 clases, se necesita aplicar *softmax* tal que:\n",
    "\\begin{equation}\n",
    "softmax(\\vec{x})_{[i]} = \\frac{e^{\\vec{x}_{[i]}}}{\\sum_{j}^{} e^{\\vec{x}_{[j]}}}\n",
    "\\end{equation}\n",
    "\n",
    "Así, vector $\\vec{\\hat{y}}$ queda definido de la siguiente forma:\n",
    "\\begin{equation}\n",
    "\\vec{\\hat{y}} = softmax(\\vec{x} \\cdot W + \\vec{b})\n",
    "\\end{equation}\n",
    "\n",
    "Con *softmax*, el vector $\\vec{\\hat{y}}$ representa una distribución de probabilidad que indica la probabilidad de pertenencia del documento asociado al vector $x$ a cada una de las 4 clases, es decir, cada columna del vector $\\vec{\\hat{y}}$ representa la probabilidad de que el documento pertenezca a la clase asociada a dicha columna y, además, al ser un vector que representa una distribución de probabilidad, la suma de todas las columnas del vector deben dar 1.\n",
    "\n",
    "Finalmente, la función de pérdida a utilizar será la función *cross-entropy loss function*:\n",
    "\\begin{equation}\n",
    "L_{cross-entropy}(\\vec{\\hat{y}}, \\vec{y}) = - \\sum_{i}^{}\\vec{y}_{[i]}\\cdot \\log(\\vec{\\hat{y}}_{[i]})\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5FaWqBVvL90"
   },
   "source": [
    "**Pregunta 2**: Explique cómo funciona el proceso de entrenamiento en este tipo de modelos y su evaluación. **(0.5 puntos)**\n",
    "\n",
    "**Respuesta**: \n",
    "\n",
    "Siguiendo con el ejemplo de la pregunta anterior, el proceso de entrenamiento consiste en encontrar los parámetros $\\hat{W}$ y $\\hat{B}$ tales que la suma de las funciones de pérdida para los documentos del dataset y sus respectivas predicciones sea mínima.\n",
    "\n",
    "Es decir, definiendo el conjunto de parámetros como $\\Theta$ y definiendo la función $\\mathcal{L}(\\Theta)$ de la forma:\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\Theta) = \\frac{1}{n} \\sum_{i=1}^{n} L(f(\\vec{x};\\Theta), y_{i})\n",
    "\\end{equation}\n",
    "donde $L$ es la función de pérdida escogida y $n$ es la cantidad de inputs en el dataset de entrenamiento, el proceso de entrenamiento del modelo consiste en encontrar $\\hat{\\Theta}$ tal que la función\n",
    "\\begin{equation}\n",
    "\\hat{\\Theta} = argmin_{\\Theta} \\mathcal{L}(\\Theta) =  argmin_{\\Theta} \\frac{1}{n} \\sum_{i=1}^{n} L(f(\\vec{x};\\Theta), y_{i})\n",
    "\\end{equation}\n",
    "\n",
    "Luego, para evaluar el modelo, se puede particionar el dataset original en tres dataset distintos: un dataset de entrenamiento, un dataset de validación y un dataset de testing. \n",
    "- El dataset de entrenamiento se utiliza para entrenar el modelo de la forma explicada anteriormente.\n",
    "- El dataset de validación se utilza para *tunear* y *evaluar* el modelo generado comparando las predicciones realizadas por el modelo entrenado y las clases originales de los datos, es decir, dado el modelo *creado*, se *pasan* los datos por el modelo, se obtienen las predicciones y se comparan las predicciones con las *clases* originales de los datos.\n",
    "- El dataset de testeo se utiliza para evaluar la performance del modelo *final* (también comparando las predicciones realizadas por el modelo entrenado y las clases originales de los datos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkK7pc54njZq"
   },
   "source": [
    "### **Parte 2: Redes Neuronales (1 ptos)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUbJjlj_9AFC"
   },
   "source": [
    "Supongamos que tenemos la siguiente red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obUfuOYB_TOC"
   },
   "source": [
    "![image.png](https://drive.google.com/uc?export=view&id=1nV1G0dOeVGPn40qGcGF9l_pVEFNtLU-w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2z-8zKW0_6q"
   },
   "source": [
    "**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matemática. Dada la red neuronal anterior, defina la salida $\\vec{\\hat{y}}$ en función del vector $\\vec{x}$, pesos $W^i$, bias $b^i$ y funciones $g,f,h$. \n",
    "\n",
    "Adicionalmente liste y explicite las dimensiones de cada matriz y vector involucrado en la red neuronal. **(0.5 Puntos)**\n",
    "\n",
    "**Respuesta**: \n",
    "\n",
    "Formula:\n",
    "$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) = h(g(f(\\vec{x}W^{1} + \\vec{b^1})W^2 + \\vec{b^2})W^3 + \\vec{b^3})W^4 + \\vec{b^4}$\n",
    "\n",
    "Dimensiones:\n",
    "- $\\vec{x} \\in \\mathcal{R}^{3}$\n",
    "- $f: \\mathcal{R}^{2} \\rightarrow \\mathcal{R}^{2}$\n",
    "- $g: \\mathcal{R}^{3} \\rightarrow \\mathcal{R}^{3}$\n",
    "- $h: \\mathcal{R}^{1} \\rightarrow \\mathcal{R}^{1}$\n",
    "- $W^1 \\in \\mathcal{R}^{3\\times2} \\text{, } b^1 \\in \\mathcal{R}^{2} $\n",
    "- $W^2 \\in \\mathcal{R}^{2\\times3} \\text{, } b^2 \\in \\mathcal{R}^{3} $\n",
    "- $W^3 \\in \\mathcal{R}^{3\\times1} \\text{, } b^3 \\in \\mathcal{R}^{1} $\n",
    "- $W^4 \\in \\mathcal{R}^{1\\times4} \\text{, } b^4 \\in \\mathcal{R}^{4} $\n",
    "\n",
    "**Pregunta 2**: Explique qué es backpropagation. ¿Cuales serían los parámetros a evaluar en la red neuronal anterior durante backpropagation? **(0.25 puntos)**\n",
    "\n",
    "**Respuesta**:\n",
    "\n",
    "Backpropagation es un método para entrenar redes neuronales que permite calcular de manera eficiente el gradiente de una función de pérdida respecto a todos sus parámetros. Backpropagation es eficaz ya que calcula las derivadas parciales respecto a cada *peso* mediante la regla de la cadena, calculando los gradientes de manera secuencial para cada capa, partiendo desde las capas exteriores hacia las capas inferiores para evitar cálculos redundantes de términos intermedios en la regla de la cadena.\n",
    "\n",
    "En la red neuronal anterior los parámetros a evaluar durante backpropagation son $W^1$, $\\vec{b^1}$, $W^2$, $\\vec{b^2}$, $W^3$, $\\vec{b^3}$, $W^4$ y $\\vec{b^4}$ (se pueden *agregar* los \n",
    "\n",
    "**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? **(0.25 puntos)**\n",
    "\n",
    "**Respuesta**:\n",
    "\n",
    "- Se calcularon los outputs de la red, es decir, desde la *input layer* se pasan los inputs a través de las *hidden layer* hasta la *ouput layer*.\n",
    "- Luego, se quiere calcular las siguientes derivadas parciales:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{l}_{[i,j]}} = \\vec{\\delta^{l}_{[j]}}\\times\\vec{z^{(l-1)}_{[i]}}\n",
    "\\end{equation}\n",
    "\n",
    "- Nótese que los valores $\\vec{z^{(l-1)}_{[i]}}$ fueron calculados inicialmente, por lo que es necesario calcular $\\vec{\\delta^{l}_{[j]}}$\n",
    "- $\\vec{\\delta^{l}_{[j]}}$ viene dado por:\n",
    "\\begin{equation}\n",
    "\\vec{\\delta^{l}_{[j]}} = g^{l}(h^{l}_{[j]}) \\times \\sum_{k}{}\\left(\\vec{\\delta^{l}_{[k]}} \\times W^{l+1}_{[j,k]} \\right)\n",
    "\\end{equation}\n",
    "- Como $\\delta^{l}_{[j]}$ depende los $\\delta^{l+1}_{[k]}$ se aplica la lógica de backpropagation: las derivadas parciales de la capa más superior son directas de calcular. Luego, con dichas derivadas se calculan las derivadas de las capas inferiores (que depende de una derivada de una capa superior que ya está calculada). Así, se calculan las derivadas de manera eficiente.\n",
    "\n",
    "En el ejemplo anterior, se tienen que calcular las derivadas parciales de la función de pérdida con respecto a los últimos nodos de la red y, con dichas derivadas, calcular las derivadas de los nodos internos. Así, solo se realizaron cálculos directos de cuatro derivadas (asociadas a los nodos de la capa superior) y luego, las derivadas de las capas inferiores son calculadas de forma *indirecta* utilizando las derivadas ya calculadas.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocS_vQhR1gcU"
   },
   "source": [
    "## **Preguntas prácticas 💻 (4 puntos).** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ol82nJ0FnmcP"
   },
   "source": [
    "### **Parte 3: Word Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Daw7Ee5cdQTb"
   },
   "source": [
    "En la auxiliar 2 se nombraron dos formas de crear word vectors:\n",
    "\n",
    "-  Distributional Vectors.\n",
    "-  Distributed Vectors.\n",
    "\n",
    "El objetivo de esta parte es comparar las dos embeddings obtenidos de estas dos estrategias en una tarea de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "E2G1qcb7AJqW"
   },
   "outputs": [],
   "source": [
    "import re  \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict \n",
    "import string \n",
    "import multiprocessing\n",
    "import os\n",
    "import gensim\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuEAv-whdMCG"
   },
   "source": [
    "#### **Parte A (1 punto)** \n",
    "\n",
    "En esta parte debe crear una matriz palabra contexto, para esto, complete el siguiente template (para esta parte puede utilizar las librerías ```numpy``` y/o ```scipy```). Hint: revise como utilizar matrices sparse de ```scipy```\n",
    "\n",
    "```python\n",
    "class WordContextMatrix:\n",
    "\n",
    "  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Utilice el constructor para definir los parametros.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # se sugiere agregar un una estructura de datos para guardar las\n",
    "    # palabras del vocab y para guardar el conteo de coocurrencia\n",
    "    ...\n",
    "    \n",
    "  def add_word_to_vocab(self, word):\n",
    "    \"\"\"\n",
    "    Utilice este método para agregar token\n",
    "    a sus vocabulario\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Le puede ser útil considerar un token unk al vocab\n",
    "    # para palabras fuera del vocab\n",
    "    ...\n",
    "  \n",
    "  def build_matrix(self):\n",
    "    \"\"\"\n",
    "    Utilice este método para crear la palabra contexto\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "  def matrix2dict(self):\n",
    "    \"\"\"\n",
    "    Utilice este método para convertir la matriz a un diccionario de embeddings, donde las llaves deben ser los token del vocabulario y los embeddings los valores obtenidos de la matriz. \n",
    "    \"\"\"\n",
    "\n",
    "    # se recomienda transformar la matrix a un diccionario de embedding.\n",
    "    # por ejemplo {palabra1:vec1, palabra2:vec2, ...}\n",
    "    ...\n",
    "\n",
    "```\n",
    "\n",
    "puede modificar los parámetros o métodos si lo considera necesario. Para probar la matrix puede utilizar el siguiente corpus.\n",
    "\n",
    "```python\n",
    "corpus = [\n",
    "  \"I like deep learning.\",\n",
    "  \"I like NLP.\",\n",
    "  \"I enjoy flying.\"\n",
    "]\n",
    "```\n",
    "\n",
    "Obteniendo una matriz parecia a esta:\n",
    "\n",
    "***Resultado esperado***: \n",
    "\n",
    "| counts   | I  | like | enjoy | deep | learning | NLP | flying | . |   \n",
    "|----------|---:|-----:|------:|-----:|---------:|----:|-------:|--:|\n",
    "| I        | 0  |  2   |  1    |    0 |  0       |   0 | 0      | 0|            \n",
    "| like     |  2 |    0 |  0    |    1 |  0       |   1 | 0      | 0 | \n",
    "| enjoy    |  1 |    0 |  0    |    0 |  0       |   0 | 1      | 0 |\n",
    "| deep     |  0 |    1 |  0    |    0 |  1       |   0 | 0      | 0 |  \n",
    "| learning |  0 |    0 |  0    |    1 |  0       |   0 | 0      | 1 |          \n",
    "| NLP      |  0 |    1 |  0    |    0 |  0       |   0 | 0      | 1 |\n",
    "| flying   |  0 |    0 |  1    |    0 |  0       |   0 | 0      | 1 | \n",
    "| .        |  0 |    0 |  0    |    0 |  1       |   1 | 1      | 0 | \n",
    "\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ur16vkyO37B5"
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "gOI1FL8MlGZB"
   },
   "outputs": [],
   "source": [
    "class WordContextMatrix:\n",
    "    def __init__(self, vocab_size, window_size, dataset, tokenizer):\n",
    "        \"\"\"\n",
    "        Utilice el constructor para definir los parametros.\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n = 0\n",
    "        self.window_size = window_size\n",
    "        self.vocab = {}\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mat = np.zeros((self.vocab_size, self.vocab_size))\n",
    "\n",
    "        self.create_vocab()\n",
    "        self.build_matrix()\n",
    "\n",
    "    def create_vocab(self):\n",
    "        cleaned_content = list(map(self.tokenizer, self.dataset))\n",
    "        phrases = Phrases(cleaned_content, min_count=100, progress_per=5000) \n",
    "        bigram = Phraser(phrases)\n",
    "        self.dataset = bigram[cleaned_content]\n",
    "\n",
    "        all_words = []\n",
    "        for words in self.dataset:\n",
    "            for word in words:\n",
    "                all_words.append(word)\n",
    "\n",
    "        count = Counter(all_words)\n",
    "        count = count.most_common(self.vocab_size)\n",
    "\n",
    "        for (word, _) in count:\n",
    "            self.add_word_to_vocab(word)\n",
    "\n",
    "    def add_word_to_vocab(self, word):\n",
    "        \"\"\"\n",
    "        Utilice este método para agregar token\n",
    "        a sus vocabulario\n",
    "        \"\"\"\n",
    "        # Le puede ser útil considerar un token unk al vocab\n",
    "        # para palabras fuera del vocab\n",
    "\n",
    "        if self.n >= self.vocab_size:\n",
    "            return\n",
    "        else:\n",
    "            self.vocab[word] = self.n\n",
    "            self.n += 1\n",
    "\n",
    "    def build_matrix(self):\n",
    "        \"\"\"\n",
    "        Utilice este método para crear la palabra contexto\n",
    "        \"\"\"\n",
    "        for words in self.dataset:\n",
    "            for (i, word) in enumerate(words):\n",
    "                if word in self.vocab:\n",
    "                    before = i - self.window_size\n",
    "                    if before >= 0:\n",
    "                        for j in range(before, i):\n",
    "                            if words[j] in self.vocab:\n",
    "                                word_1_indx = self.vocab[word]\n",
    "                                word_2_indx = self.vocab[words[j]]\n",
    "                                self.mat[word_1_indx][word_2_indx] += 1\n",
    "                                self.mat[word_2_indx][word_1_indx] += 1\n",
    "\n",
    "    def matrix2dict(self):\n",
    "        \"\"\"\n",
    "        Utilice este método para convertir la matriz a un diccionario de\n",
    "        embeddings, donde las llaves deben ser los token del vocabulario y los\n",
    "        embeddings los valores obtenidos de la matriz.\n",
    "        \"\"\"\n",
    "        dic_matrix = {}\n",
    "        for key in self.vocab.keys():\n",
    "            dic_matrix[key] = self.mat[self.vocab[key]]\n",
    "        return dic_matrix\n",
    "\n",
    "        # se recomienda transformar la matrix a un diccionario de embedding.\n",
    "        # por ejemplo {palabra1:vec1, palabra2:vec2, ...}\n",
    "\n",
    "    def print_matrix(self):\n",
    "        hola = \"        \"\n",
    "        print(hola, end=\" \")\n",
    "        for key in self.vocab.keys():\n",
    "            print(key, end=\" \")\n",
    "        print(\"\")\n",
    "\n",
    "        for key in self.vocab.keys():\n",
    "            print(key + \" \"*(len(hola)-len(key)), end=\" \")\n",
    "            i = self.vocab[key]\n",
    "\n",
    "            for key in self.vocab.keys():\n",
    "                j = self.vocab[key]\n",
    "                print(self.mat[i][j], end=\" \")\n",
    "            print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgmeSFqKLpFL"
   },
   "source": [
    "#### **Parte B (1.5 puntos)**\n",
    "\n",
    "En esta parte es debe entrenar Word2Vec de gensim y construir la matriz palabra contexto utilizando el dataset de diálogos de los Simpson. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZgN06q4QPi3"
   },
   "source": [
    "Utilizando el dataset adjunto con la tarea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eY3kmg4onnsu",
    "outputId": "d3525a54-0c10-401e-b3e2-9c6e9e714a2c"
   },
   "outputs": [],
   "source": [
    "data_file = \"dialogue-lines-of-the-simpsons.zip\"\n",
    "df = pd.read_csv(data_file)\n",
    "df = df.dropna().reset_index(drop=True) # Quitar filas vacias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAg5a5bmWk3T"
   },
   "source": [
    "**Pregunta 1**: Ayudándose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec. **(0.75 punto)** (Hint, le puede servir explorar un poco los datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWw2fXFRXe5Y"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "Bvwplz7yTNcr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 02:51:32,579 : INFO : collecting all words and their counts\n",
      "2022-05-30 02:51:32,580 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2022-05-30 02:51:32,600 : INFO : PROGRESS: at sentence #5000, processed 16897 words and 17696 word types\n",
      "2022-05-30 02:51:32,626 : INFO : PROGRESS: at sentence #10000, processed 33320 words and 32774 word types\n",
      "2022-05-30 02:51:32,655 : INFO : PROGRESS: at sentence #15000, processed 49225 words and 46074 word types\n",
      "2022-05-30 02:51:32,684 : INFO : PROGRESS: at sentence #20000, processed 67371 words and 61074 word types\n",
      "2022-05-30 02:51:32,711 : INFO : PROGRESS: at sentence #25000, processed 85568 words and 75938 word types\n",
      "2022-05-30 02:51:32,746 : INFO : PROGRESS: at sentence #30000, processed 104569 words and 91367 word types\n",
      "2022-05-30 02:51:32,776 : INFO : PROGRESS: at sentence #35000, processed 122041 words and 104739 word types\n",
      "2022-05-30 02:51:32,798 : INFO : PROGRESS: at sentence #40000, processed 138266 words and 116598 word types\n",
      "2022-05-30 02:51:32,819 : INFO : PROGRESS: at sentence #45000, processed 154513 words and 128541 word types\n",
      "2022-05-30 02:51:32,845 : INFO : PROGRESS: at sentence #50000, processed 170239 words and 140055 word types\n",
      "2022-05-30 02:51:32,874 : INFO : PROGRESS: at sentence #55000, processed 185556 words and 150956 word types\n",
      "2022-05-30 02:51:32,896 : INFO : PROGRESS: at sentence #60000, processed 200105 words and 160956 word types\n",
      "2022-05-30 02:51:32,927 : INFO : PROGRESS: at sentence #65000, processed 215762 words and 171680 word types\n",
      "2022-05-30 02:51:32,954 : INFO : PROGRESS: at sentence #70000, processed 233512 words and 184773 word types\n",
      "2022-05-30 02:51:32,980 : INFO : PROGRESS: at sentence #75000, processed 251146 words and 197413 word types\n",
      "2022-05-30 02:51:33,012 : INFO : PROGRESS: at sentence #80000, processed 268907 words and 209846 word types\n",
      "2022-05-30 02:51:33,033 : INFO : PROGRESS: at sentence #85000, processed 286451 words and 222075 word types\n",
      "2022-05-30 02:51:33,058 : INFO : PROGRESS: at sentence #90000, processed 303348 words and 233815 word types\n",
      "2022-05-30 02:51:33,078 : INFO : PROGRESS: at sentence #95000, processed 320181 words and 245355 word types\n",
      "2022-05-30 02:51:33,108 : INFO : PROGRESS: at sentence #100000, processed 337260 words and 257141 word types\n",
      "2022-05-30 02:51:33,136 : INFO : PROGRESS: at sentence #105000, processed 354342 words and 268926 word types\n",
      "2022-05-30 02:51:33,164 : INFO : PROGRESS: at sentence #110000, processed 371777 words and 281027 word types\n",
      "2022-05-30 02:51:33,189 : INFO : PROGRESS: at sentence #115000, processed 388303 words and 292112 word types\n",
      "2022-05-30 02:51:33,219 : INFO : PROGRESS: at sentence #120000, processed 405287 words and 303526 word types\n",
      "2022-05-30 02:51:33,244 : INFO : PROGRESS: at sentence #125000, processed 421730 words and 314026 word types\n",
      "2022-05-30 02:51:33,268 : INFO : PROGRESS: at sentence #130000, processed 438262 words and 323837 word types\n",
      "2022-05-30 02:51:33,278 : INFO : collected 327493 token types (unigram + bigrams) from a corpus of 444264 words and 131853 sentences\n",
      "2022-05-30 02:51:33,279 : INFO : merged Phrases<327493 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000>\n",
      "2022-05-30 02:51:33,279 : INFO : Phrases lifecycle event {'msg': 'built Phrases<327493 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000> in 0.70s', 'datetime': '2022-05-30T02:51:33.279898', 'gensim': '4.1.2', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-113-generic-x86_64-with-glibc2.29', 'event': 'created'}\n",
      "2022-05-30 02:51:33,288 : INFO : exporting phrases from Phrases<327493 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000>\n",
      "2022-05-30 02:51:33,892 : INFO : FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<11 phrases, min_count=100, threshold=10.0> from Phrases<327493 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000> in 0.60s', 'datetime': '2022-05-30T02:51:33.892692', 'gensim': '4.1.2', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-113-generic-x86_64-with-glibc2.29', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "content = df[\"spoken_words\"]\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# limpiar puntuaciones y separar por tokens.\n",
    "punctuation = string.punctuation + \"«»“”‘’…—\"\n",
    "stopwords = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
    ").values\n",
    "stopwords = Counter(stopwords.flatten().tolist())\n",
    "def simple_tokenizer(doc, lower=False):\n",
    "    if lower:\n",
    "        tokenized_doc = doc.translate(str.maketrans(\n",
    "            '', '', punctuation)).lower().split()\n",
    "\n",
    "    tokenized_doc = doc.translate(str.maketrans('', '', punctuation)).split()\n",
    "\n",
    "    tokenized_doc = [\n",
    "        token for token in tokenized_doc if token.lower() not in stopwords\n",
    "    ]\n",
    "    return tokenized_doc\n",
    "\n",
    "\n",
    "cleaned_content = [simple_tokenizer(doc) for doc in content.values]\n",
    "phrases = Phrases(cleaned_content, min_count=100, progress_per=5000) \n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[cleaned_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 02:51:37,105 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=0, vector_size=300, alpha=0.03)', 'datetime': '2022-05-30T02:51:37.105836', 'gensim': '4.1.2', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-113-generic-x86_64-with-glibc2.29', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "simpsons_w2v = Word2Vec(min_count=10,\n",
    "                        window=2,\n",
    "                        vector_size=300,\n",
    "                        sample=6e-5,\n",
    "                        alpha=0.03,\n",
    "                        min_alpha=0.0007,\n",
    "                        negative=20,\n",
    "                        workers=multiprocessing.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 02:51:37,650 : INFO : collecting all words and their counts\n",
      "2022-05-30 02:51:37,652 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-05-30 02:51:37,709 : INFO : PROGRESS: at sentence #10000, processed 33138 words, keeping 10665 word types\n",
      "2022-05-30 02:51:37,755 : INFO : PROGRESS: at sentence #20000, processed 66935 words, keeping 17139 word types\n",
      "2022-05-30 02:51:37,819 : INFO : PROGRESS: at sentence #30000, processed 103943 words, keeping 23072 word types\n",
      "2022-05-30 02:51:37,870 : INFO : PROGRESS: at sentence #40000, processed 137471 words, keeping 27257 word types\n",
      "2022-05-30 02:51:37,922 : INFO : PROGRESS: at sentence #50000, processed 169305 words, keeping 31180 word types\n",
      "2022-05-30 02:51:37,974 : INFO : PROGRESS: at sentence #60000, processed 199034 words, keeping 34584 word types\n",
      "2022-05-30 02:51:38,029 : INFO : PROGRESS: at sentence #70000, processed 232290 words, keeping 38239 word types\n",
      "2022-05-30 02:51:38,087 : INFO : PROGRESS: at sentence #80000, processed 267551 words, keeping 41818 word types\n",
      "2022-05-30 02:51:38,138 : INFO : PROGRESS: at sentence #90000, processed 301884 words, keeping 45072 word types\n",
      "2022-05-30 02:51:38,197 : INFO : PROGRESS: at sentence #100000, processed 335694 words, keeping 48066 word types\n",
      "2022-05-30 02:51:38,248 : INFO : PROGRESS: at sentence #110000, processed 370099 words, keeping 51338 word types\n",
      "2022-05-30 02:51:38,304 : INFO : PROGRESS: at sentence #120000, processed 403504 words, keeping 54166 word types\n",
      "2022-05-30 02:51:38,361 : INFO : PROGRESS: at sentence #130000, processed 436305 words, keeping 56287 word types\n",
      "2022-05-30 02:51:38,373 : INFO : collected 56639 word types from a corpus of 442276 raw words and 131853 sentences\n",
      "2022-05-30 02:51:38,374 : INFO : Creating a fresh vocabulary\n",
      "2022-05-30 02:51:38,424 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 6503 unique words (11.481488020621834%% of original 56639, drops 50136)', 'datetime': '2022-05-30T02:51:38.424119', 'gensim': '4.1.2', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-113-generic-x86_64-with-glibc2.29', 'event': 'prepare_vocab'}\n",
      "2022-05-30 02:51:38,424 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 337000 word corpus (76.19676401161266%% of original 442276, drops 105276)', 'datetime': '2022-05-30T02:51:38.424897', 'gensim': '4.1.2', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-113-generic-x86_64-with-glibc2.29', 'event': 'prepare_vocab'}\n",
      "2022-05-30 02:51:38,468 : INFO : deleting the raw counts dictionary of 56639 items\n",
      "2022-05-30 02:51:38,470 : INFO : sample=6e-05 downsamples 1282 most-common words\n",
      "2022-05-30 02:51:38,470 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 203191.70004639882 word corpus (60.3%% of prior 337000)', 'datetime': '2022-05-30T02:51:38.470824', 'gensim': '4.1.2', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-113-generic-x86_64-with-glibc2.29', 'event': 'prepare_vocab'}\n",
      "2022-05-30 02:51:38,558 : INFO : estimated required memory for 6503 words and 300 dimensions: 18858700 bytes\n",
      "2022-05-30 02:51:38,559 : INFO : resetting layer weights\n",
      "2022-05-30 02:51:38,572 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-05-30T02:51:38.572868', 'gensim': '4.1.2', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-113-generic-x86_64-with-glibc2.29', 'event': 'build_vocab'}\n"
     ]
    }
   ],
   "source": [
    "simpsons_w2v.build_vocab(sentences, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 02:51:46,111 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 6503 vocabulary and 300 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2 shrink_windows=True', 'datetime': '2022-05-30T02:51:46.111757', 'gensim': '4.1.2', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-113-generic-x86_64-with-glibc2.29', 'event': 'train'}\n",
      "2022-05-30 02:51:47,139 : INFO : EPOCH 1 - PROGRESS: at 59.03% examples, 117390 words/s, in_qsize 0, out_qsize 0\n",
      "2022-05-30 02:51:47,613 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-05-30 02:51:47,614 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-30 02:51:47,626 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-30 02:51:47,660 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-30 02:51:47,660 : INFO : EPOCH - 1 : training on 442276 raw words (203250 effective words) took 1.5s, 132170 effective words/s\n",
      "2022-05-30 02:51:48,708 : INFO : EPOCH 2 - PROGRESS: at 56.90% examples, 110518 words/s, in_qsize 3, out_qsize 1\n",
      "2022-05-30 02:51:49,109 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-05-30 02:51:49,122 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-30 02:51:49,123 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-30 02:51:49,123 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-30 02:51:49,125 : INFO : EPOCH - 2 : training on 442276 raw words (203172 effective words) took 1.5s, 139601 effective words/s\n",
      "2022-05-30 02:51:50,182 : INFO : EPOCH 3 - PROGRESS: at 54.84% examples, 105638 words/s, in_qsize 7, out_qsize 0\n",
      "2022-05-30 02:51:50,589 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-05-30 02:51:50,599 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-30 02:51:50,619 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-30 02:51:50,642 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-30 02:51:50,642 : INFO : EPOCH - 3 : training on 442276 raw words (203228 effective words) took 1.5s, 135336 effective words/s\n",
      "2022-05-30 02:51:51,851 : INFO : EPOCH 4 - PROGRESS: at 72.33% examples, 122621 words/s, in_qsize 0, out_qsize 0\n",
      "2022-05-30 02:51:52,150 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-05-30 02:51:52,158 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-30 02:51:52,173 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-30 02:51:52,192 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-30 02:51:52,192 : INFO : EPOCH - 4 : training on 442276 raw words (203180 effective words) took 1.5s, 131907 effective words/s\n",
      "2022-05-30 02:51:53,421 : INFO : EPOCH 5 - PROGRESS: at 70.06% examples, 116659 words/s, in_qsize 5, out_qsize 0\n",
      "2022-05-30 02:51:53,655 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-05-30 02:51:53,669 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-30 02:51:53,671 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-30 02:51:53,702 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-30 02:51:53,703 : INFO : EPOCH - 5 : training on 442276 raw words (202932 effective words) took 1.5s, 135199 effective words/s\n",
      "2022-05-30 02:51:54,755 : INFO : EPOCH 6 - PROGRESS: at 74.56% examples, 145259 words/s, in_qsize 3, out_qsize 1\n",
      "2022-05-30 02:51:54,939 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-05-30 02:51:54,951 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-30 02:51:54,953 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-30 02:51:54,954 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-30 02:51:54,954 : INFO : EPOCH - 6 : training on 442276 raw words (203174 effective words) took 1.2s, 163566 effective words/s\n",
      "2022-05-30 02:51:55,996 : INFO : EPOCH 7 - PROGRESS: at 63.32% examples, 124809 words/s, in_qsize 0, out_qsize 1\n",
      "2022-05-30 02:51:56,390 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-05-30 02:51:56,407 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-30 02:51:56,431 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-30 02:51:56,440 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-30 02:51:56,441 : INFO : EPOCH - 7 : training on 442276 raw words (203236 effective words) took 1.5s, 137902 effective words/s\n",
      "2022-05-30 02:51:57,493 : INFO : EPOCH 8 - PROGRESS: at 50.44% examples, 96941 words/s, in_qsize 7, out_qsize 0\n",
      "2022-05-30 02:51:58,031 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-05-30 02:51:58,034 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-30 02:51:58,042 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-30 02:51:58,079 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-30 02:51:58,079 : INFO : EPOCH - 8 : training on 442276 raw words (203389 effective words) took 1.6s, 124843 effective words/s\n",
      "2022-05-30 02:51:59,090 : INFO : EPOCH 9 - PROGRESS: at 56.90% examples, 114203 words/s, in_qsize 1, out_qsize 0\n",
      "2022-05-30 02:51:59,548 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-05-30 02:51:59,551 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-30 02:51:59,561 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-30 02:51:59,595 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-30 02:51:59,595 : INFO : EPOCH - 9 : training on 442276 raw words (203125 effective words) took 1.5s, 134626 effective words/s\n",
      "2022-05-30 02:52:00,650 : INFO : EPOCH 10 - PROGRESS: at 59.06% examples, 114206 words/s, in_qsize 0, out_qsize 1\n",
      "2022-05-30 02:52:01,123 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-05-30 02:52:01,126 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-30 02:52:01,135 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-30 02:52:01,154 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-30 02:52:01,155 : INFO : EPOCH - 10 : training on 442276 raw words (203670 effective words) took 1.6s, 131355 effective words/s\n",
      "2022-05-30 02:52:02,331 : INFO : EPOCH 11 - PROGRESS: at 72.33% examples, 126270 words/s, in_qsize 1, out_qsize 0\n",
      "2022-05-30 02:52:02,688 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-05-30 02:52:02,699 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-30 02:52:02,702 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-30 02:52:02,731 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-30 02:52:02,732 : INFO : EPOCH - 11 : training on 442276 raw words (203007 effective words) took 1.6s, 129766 effective words/s\n",
      "2022-05-30 02:52:03,912 : INFO : EPOCH 12 - PROGRESS: at 63.32% examples, 109853 words/s, in_qsize 7, out_qsize 1\n",
      "2022-05-30 02:52:04,191 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-05-30 02:52:04,197 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-30 02:52:04,205 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-30 02:52:04,206 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-30 02:52:04,206 : INFO : EPOCH - 12 : training on 442276 raw words (203494 effective words) took 1.5s, 138808 effective words/s\n",
      "2022-05-30 02:52:05,229 : INFO : EPOCH 13 - PROGRESS: at 72.33% examples, 145131 words/s, in_qsize 3, out_qsize 1\n",
      "2022-05-30 02:52:05,438 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-05-30 02:52:05,445 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-30 02:52:05,446 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-30 02:52:05,450 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 02:52:05,450 : INFO : EPOCH - 13 : training on 442276 raw words (203068 effective words) took 1.2s, 164445 effective words/s\n",
      "2022-05-30 02:52:06,493 : INFO : EPOCH 14 - PROGRESS: at 61.19% examples, 120666 words/s, in_qsize 0, out_qsize 0\n",
      "2022-05-30 02:52:06,900 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-05-30 02:52:06,906 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-30 02:52:06,931 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-30 02:52:06,941 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-30 02:52:06,941 : INFO : EPOCH - 14 : training on 442276 raw words (202848 effective words) took 1.5s, 137705 effective words/s\n",
      "2022-05-30 02:52:07,959 : INFO : EPOCH 15 - PROGRESS: at 54.75% examples, 109123 words/s, in_qsize 3, out_qsize 0\n",
      "2022-05-30 02:52:08,379 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-05-30 02:52:08,392 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-30 02:52:08,424 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-30 02:52:08,434 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-30 02:52:08,434 : INFO : EPOCH - 15 : training on 442276 raw words (203075 effective words) took 1.5s, 136927 effective words/s\n",
      "2022-05-30 02:52:08,435 : INFO : Word2Vec lifecycle event {'msg': 'training on 6634140 raw words (3047848 effective words) took 22.3s, 136538 effective words/s', 'datetime': '2022-05-30T02:52:08.435388', 'gensim': '4.1.2', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-113-generic-x86_64-with-glibc2.29', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.37 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "simpsons_w2v.train(sentences, total_examples=simpsons_w2v.corpus_count, epochs=15, report_delay=10)\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4669/358243844.py:1: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  simpsons_w2v.init_sims(replace=True)\n",
      "2022-05-30 02:52:20,660 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    }
   ],
   "source": [
    "simpsons_w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vBkF3hreGjg"
   },
   "source": [
    "**Pregunta 2**: Cree una matriz palabra contexto usando el mismo dataset. Configure el largo del vocabulario a 1000 o 2000 tokens, puede agregar valores mayores pero tenga en cuenta que la construcción de la matriz puede tomar varios minutos. Puede que esto tarde un poco. **(0.75 punto)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzLuH6MneWIY"
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "id": "9gPyW8fMeXNX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 03:35:17,440 : INFO : collecting all words and their counts\n",
      "2022-05-30 03:35:17,441 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2022-05-30 03:35:17,468 : INFO : PROGRESS: at sentence #5000, processed 16897 words and 17696 word types\n",
      "2022-05-30 03:35:17,491 : INFO : PROGRESS: at sentence #10000, processed 33320 words and 32774 word types\n",
      "2022-05-30 03:35:17,518 : INFO : PROGRESS: at sentence #15000, processed 49225 words and 46074 word types\n",
      "2022-05-30 03:35:17,539 : INFO : PROGRESS: at sentence #20000, processed 67371 words and 61074 word types\n",
      "2022-05-30 03:35:17,568 : INFO : PROGRESS: at sentence #25000, processed 85568 words and 75938 word types\n",
      "2022-05-30 03:35:17,596 : INFO : PROGRESS: at sentence #30000, processed 104569 words and 91367 word types\n",
      "2022-05-30 03:35:17,620 : INFO : PROGRESS: at sentence #35000, processed 122041 words and 104739 word types\n",
      "2022-05-30 03:35:17,643 : INFO : PROGRESS: at sentence #40000, processed 138266 words and 116598 word types\n",
      "2022-05-30 03:35:17,668 : INFO : PROGRESS: at sentence #45000, processed 154513 words and 128541 word types\n",
      "2022-05-30 03:35:17,688 : INFO : PROGRESS: at sentence #50000, processed 170239 words and 140055 word types\n",
      "2022-05-30 03:35:17,721 : INFO : PROGRESS: at sentence #55000, processed 185556 words and 150956 word types\n",
      "2022-05-30 03:35:17,743 : INFO : PROGRESS: at sentence #60000, processed 200105 words and 160956 word types\n",
      "2022-05-30 03:35:17,768 : INFO : PROGRESS: at sentence #65000, processed 215762 words and 171680 word types\n",
      "2022-05-30 03:35:17,800 : INFO : PROGRESS: at sentence #70000, processed 233512 words and 184773 word types\n",
      "2022-05-30 03:35:17,824 : INFO : PROGRESS: at sentence #75000, processed 251146 words and 197413 word types\n",
      "2022-05-30 03:35:17,854 : INFO : PROGRESS: at sentence #80000, processed 268907 words and 209846 word types\n",
      "2022-05-30 03:35:17,883 : INFO : PROGRESS: at sentence #85000, processed 286451 words and 222075 word types\n",
      "2022-05-30 03:35:17,910 : INFO : PROGRESS: at sentence #90000, processed 303348 words and 233815 word types\n",
      "2022-05-30 03:35:17,934 : INFO : PROGRESS: at sentence #95000, processed 320181 words and 245355 word types\n",
      "2022-05-30 03:35:17,960 : INFO : PROGRESS: at sentence #100000, processed 337260 words and 257141 word types\n",
      "2022-05-30 03:35:17,985 : INFO : PROGRESS: at sentence #105000, processed 354342 words and 268926 word types\n",
      "2022-05-30 03:35:18,010 : INFO : PROGRESS: at sentence #110000, processed 371777 words and 281027 word types\n",
      "2022-05-30 03:35:18,033 : INFO : PROGRESS: at sentence #115000, processed 388303 words and 292112 word types\n",
      "2022-05-30 03:35:18,065 : INFO : PROGRESS: at sentence #120000, processed 405287 words and 303526 word types\n",
      "2022-05-30 03:35:18,089 : INFO : PROGRESS: at sentence #125000, processed 421730 words and 314026 word types\n",
      "2022-05-30 03:35:18,115 : INFO : PROGRESS: at sentence #130000, processed 438262 words and 323837 word types\n",
      "2022-05-30 03:35:18,124 : INFO : collected 327493 token types (unigram + bigrams) from a corpus of 444264 words and 131853 sentences\n",
      "2022-05-30 03:35:18,125 : INFO : merged Phrases<327493 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000>\n",
      "2022-05-30 03:35:18,126 : INFO : Phrases lifecycle event {'msg': 'built Phrases<327493 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000> in 0.69s', 'datetime': '2022-05-30T03:35:18.126446', 'gensim': '4.1.2', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-113-generic-x86_64-with-glibc2.29', 'event': 'created'}\n",
      "2022-05-30 03:35:18,127 : INFO : exporting phrases from Phrases<327493 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000>\n",
      "2022-05-30 03:35:18,740 : INFO : FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<11 phrases, min_count=100, threshold=10.0> from Phrases<327493 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000> in 0.61s', 'datetime': '2022-05-30T03:35:18.740430', 'gensim': '4.1.2', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-113-generic-x86_64-with-glibc2.29', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "simpsons_wcm = WordContextMatrix(6000, 1, content, simple_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRCB-jqgTNcs"
   },
   "source": [
    "#### **Parte C (1.5 puntos): Aplicar embeddings para clasificar**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlqzlJRSTNcs"
   },
   "source": [
    "Ahora utilizaremos los embeddings que acabamos de calcular para clasificar palabras basadas en su polaridad (positivas o negativas). \n",
    "\n",
    "Para esto ocuparemos el lexicón AFINN incluido en la tarea, que incluye una lista de palabras y un 1 si su connotación es positiva y un -1 si es negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "CMskFDmHTNcs"
   },
   "outputs": [],
   "source": [
    "AFINN = 'AFINN_full.csv'\n",
    "df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaKl8hsCTNcs"
   },
   "source": [
    "Hint: Para w2v y la wcm son esperables KeyErrors debido a que no todas las palabras del corpus de los simpsons tendrán una representación en AFINN. Para el caso de la matriz palabra contexto se recomienda convertir su matrix a un diccionario. Pueden utilizar esta función auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "tWSSuctiTNcs"
   },
   "outputs": [],
   "source": [
    "def try_apply(model,word):\n",
    "    try:\n",
    "        aux = model[word]\n",
    "        return True\n",
    "    except KeyError:\n",
    "        #logger.error('Word {} not in dictionary'.format(word))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrVPeEzgTNcs"
   },
   "source": [
    "**Pregunta 1**: Transforme las palabras del corpus de AFINN a la representación en embedding que acabamos de calcular (con ambos modelos). \n",
    "\n",
    "Su dataframe final debe ser del estilo [embedding, sentimiento], donde los embeddings corresponden a $X$ y el sentimiento asociado con el embedding a $y$ (positivo/negativo, 1/-1). \n",
    "\n",
    "Para ambos modelos, separar train y test de acuerdo a la siguiente función. **(0.5 puntos)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "0Bkt26BwTNcs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807\n"
     ]
    }
   ],
   "source": [
    "wcm_dic = simpsons_wcm.matrix2dict()\n",
    "wmc_df = []\n",
    "\n",
    "for _, row in df_afinn.iterrows():\n",
    "    if try_apply(wcm_dic, row[0]):\n",
    "        wmc_df.append([wcm_dic[row[0]], row[1]])\n",
    "\n",
    "wmc_df = pd.DataFrame (wmc_df, columns = ['embedding', 'sentimiento'])\n",
    "print(len(wmc_df.sentimiento))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(wmc_df.embedding, wmc_df.sentimiento, random_state=0, test_size=0.1, stratify=wmc_df.sentimiento)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDKe4gA3TNct"
   },
   "source": [
    "**Pregunta 2**: Entrenar una regresión logística (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qué se obtienen estos resultados? Cómo los mejorarías? Como podrías mejorar los resultados de la matriz palabra contexto? es equivalente al modelo word2vec? **(1 punto)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJMzq_dETNct"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseFeature(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WCMTransformer(BaseFeature):\n",
    "    def transform(self, X, y=None):\n",
    "        lst = []\n",
    "        for x in X:\n",
    "            lst.append(x)\n",
    "        return np.array(lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "bj1r_BnKn_7L"
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=1000000)\n",
    "wmc_transformer = WCMTransformer()\n",
    "pipeline = Pipeline([('doc2vec', wmc_transformer), ('clf', clf)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('doc2vec', WCMTransformer()),\n",
       "                ('clf', LogisticRegression(max_iter=1000000))])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41  6]\n",
      " [17 17]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.71      0.87      0.78        47\n",
      "           1       0.74      0.50      0.60        34\n",
      "\n",
      "    accuracy                           0.72        81\n",
      "   macro avg       0.72      0.69      0.69        81\n",
      "weighted avg       0.72      0.72      0.70        81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izppruGQTNct"
   },
   "source": [
    "# Bonus: +0.25 puntos en cualquier pregunta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YW0aeK2KTNct"
   },
   "source": [
    "**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset más grande y obtener mejores resultados. Les puede servir [ésta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim **(0.25 puntos)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvHcVS3sTNct"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSc8p-T8TNcu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tarea_3_Word_Embeddings.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
