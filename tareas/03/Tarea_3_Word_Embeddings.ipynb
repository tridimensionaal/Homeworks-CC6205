{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ckbt7VPDhBwb"
   },
   "source": [
    "# **Tarea 3 - Word Embeddings 📚**\n",
    "\n",
    "**Integrantes:** Nicolas Lemunñir y Matías Seda\n",
    "\n",
    "**Fecha límite de entrega 📆:** 3 de mayo.\n",
    "\n",
    "**Tiempo estimado de dedicación:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T18:30:18.109327Z",
     "start_time": "2020-03-19T18:30:18.103344Z"
    },
    "id": "q5CSRY4oNCHK"
   },
   "source": [
    "\n",
    "**Instrucciones:**\n",
    "- El ejercicio consiste en:\n",
    "    - Responder preguntas relativas a los contenidos vistos en los vídeos y slides de las clases. \n",
    "    - Entrenar Word2Vec y Word Context Matrix sobre un pequeño corpus.\n",
    "    - Evaluar los embeddings obtenidos en una tarea de clasificación.\n",
    "- La tarea se realiza en grupos de **máximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
    "- La entrega es a través de u-cursos a más tardar el día estipulado arriba. No se aceptan atrasos.\n",
    "- El formato de entrega es este mismo Jupyter Notebook.\n",
    "- Al momento de la revisión tu código será ejecutado. Por favor verifica que tu entrega no tenga errores de compilación. \n",
    "- En el horario de auxiliar pueden realizar consultas acerca de la tarea a través del canal de Discord del curso. \n",
    "\n",
    "\n",
    "**Referencias**\n",
    "\n",
    "Vídeos: \n",
    "\n",
    "- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n",
    "- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n",
    "- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4wYf0vgnbTv"
   },
   "source": [
    "## **Preguntas teóricas 📕 (2 puntos).** ##\n",
    "Para estas preguntas no es necesario implementar código, pero pueden utilizar pseudo código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5hUG6-8ngoK"
   },
   "source": [
    "### **Parte 1: Modelos Lineales (1 ptos)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yRvZbhsoi8f"
   },
   "source": [
    "Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categorías: política, deporte, negocios y otros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irsqBVmCnx3M"
   },
   "source": [
    "**Pregunta 1**: Diseñe un modelo lineal capaz de clasificar un documento según estas categorías donde el output sea un vector con una distribución de probabilidad con la pertenencia a cada clase. \n",
    "\n",
    "Especifique: representación de los documentos de entrada, parámetros del modelo, transformaciones necesarias para obtener la probabilidad de cada etiqueta y función de pérdida escogida. **(0.5 puntos)**\n",
    "\n",
    "**Respuesta**:\n",
    "\n",
    "En primer lugar, la representación de los documentos de entrada será mediante vectores *word count*, es decir, dado el vocabulario del modelo, la representación de un documento será un vector del largo del vocabulario donde cada columna representará una palabra del vocabulario y el valor en dicha columna será la cantidad de veces que aparece la palabra asociada a la columna en el documento del input.\n",
    "\n",
    "Los parámetros del modelo serán la matriz $W$ y el vector $\\vec{b}$. La matriz $W$ será una matriz $W \\in \\mathcal{R}^{N\\times4}$ donde $N$ es el tamaño del vocabulario y donde cada vector de una fila representará la *importancia* o *peso* de la palabra asociada a dicha fila para las clases asociadas a cada columna. El vector $\\vec{b}$ será una vector $\\vec{b} \\in \\mathbb{R}^{4}$. De esa forma, se podrá modelar el problema mediante una función linear de la siguiente forma.\n",
    "\\begin{equation}\n",
    "f(\\vec{x}) = \\vec{x} \\cdot W + \\vec{b}\n",
    "\\end{equation}\n",
    "\n",
    "Ahora, para que la función $f(x) = \\vec{\\hat{y}}$ represente una distribución de probabilidad que indique la probabilidad de pertenencia del documento asociado al vector $x$ a cada una de las 4 clases, se necesita aplicar *softmax* tal que:\n",
    "\\begin{equation}\n",
    "softmax(\\vec{x})_{[i]} = \\frac{e^{\\vec{x}_{[i]}}}{\\sum_{j}^{} e^{\\vec{x}_{[j]}}}\n",
    "\\end{equation}\n",
    "\n",
    "Así, el vector $\\vec{\\hat{y}}$ queda definido de la siguiente forma:\n",
    "\\begin{equation}\n",
    "\\vec{\\hat{y}} = softmax(\\vec{x} \\cdot W + \\vec{b})\n",
    "\\end{equation}\n",
    "\n",
    "Con *softmax*, el vector $\\vec{\\hat{y}}$ representa una distribución de probabilidad que indica la probabilidad de pertenencia del documento asociado al vector $x$ a cada una de las 4 clases, es decir, cada columna del vector $\\vec{\\hat{y}}$ representa la probabilidad de que el documento pertenezca a la clase asociada a dicha columna y, además, al ser un vector que representa una distribución de probabilidad, la suma de todas las columnas del vector deben dar 1.\n",
    "\n",
    "Finalmente, la función de pérdida a utilizar será la función *cross-entropy loss function*:\n",
    "\\begin{equation}\n",
    "L_{cross-entropy}(\\vec{\\hat{y}}, \\vec{y}) = - \\sum_{i}^{}\\vec{y}_{[i]}\\cdot \\log(\\vec{\\hat{y}}_{[i]})\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5FaWqBVvL90"
   },
   "source": [
    "**Pregunta 2**: Explique cómo funciona el proceso de entrenamiento en este tipo de modelos y su evaluación. **(0.5 puntos)**\n",
    "\n",
    "**Respuesta**: \n",
    "\n",
    "Siguiendo con el ejemplo de la pregunta anterior, el proceso de entrenamiento consiste en encontrar los parámetros $\\hat{W}$ y $\\hat{B}$ tales que la suma de las funciones de pérdida para los documentos del dataset y sus respectivas predicciones sea mínima.\n",
    "\n",
    "Es decir, en general, definiendo el conjunto de parámetros como $\\Theta$ y definiendo la función $\\mathcal{L}(\\Theta)$ de la forma:\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\Theta) = \\frac{1}{n} \\sum_{i=1}^{n} L(f(\\vec{x};\\Theta), y_{i})\n",
    "\\end{equation}\n",
    "donde $L$ es la función de pérdida escogida y $n$ es la cantidad de inputs en el dataset de entrenamiento, el proceso de entrenamiento del modelo consiste en encontrar $\\hat{\\Theta}$ tal que:\n",
    "\\begin{equation}\n",
    "\\hat{\\Theta} = argmin_{\\Theta} \\mathcal{L}(\\Theta) =  argmin_{\\Theta} \\frac{1}{n} \\sum_{i=1}^{n} L(f(\\vec{x};\\Theta), y_{i})\n",
    "\\end{equation}\n",
    "\n",
    "Luego, para evaluar el modelo, se puede particionar el dataset original en tres dataset distintos: un dataset de entrenamiento, un dataset de validación y un dataset de testing. \n",
    "- El dataset de entrenamiento se utiliza para entrenar el modelo de la forma explicada anteriormente.\n",
    "- El dataset de validación se utilza para *tunear* y *evaluar* el modelo generado comparando las predicciones realizadas por el modelo entrenado y las clases originales de los datos, es decir, dado el modelo *creado*, se *pasan* los datos por el modelo, se obtienen las predicciones y se comparan las predicciones con las *clases* originales de los datos.\n",
    "- El dataset de testeo se utiliza para evaluar la performance del modelo *final* (también comparando las predicciones realizadas por el modelo entrenado y las clases originales de los datos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkK7pc54njZq"
   },
   "source": [
    "### **Parte 2: Redes Neuronales (1 ptos)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUbJjlj_9AFC"
   },
   "source": [
    "Supongamos que tenemos la siguiente red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obUfuOYB_TOC"
   },
   "source": [
    "![image.png](https://drive.google.com/uc?export=view&id=1nV1G0dOeVGPn40qGcGF9l_pVEFNtLU-w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2z-8zKW0_6q"
   },
   "source": [
    "**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matemática. Dada la red neuronal anterior, defina la salida $\\vec{\\hat{y}}$ en función del vector $\\vec{x}$, pesos $W^i$, bias $b^i$ y funciones $g,f,h$. \n",
    "\n",
    "Adicionalmente liste y explicite las dimensiones de cada matriz y vector involucrado en la red neuronal. **(0.5 Puntos)**\n",
    "\n",
    "**Respuesta**: \n",
    "\n",
    "Formula:\n",
    "$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) = h(g(f(\\vec{x}W^{1} + \\vec{b^1})W^2 + \\vec{b^2})W^3 + \\vec{b^3})W^4 + \\vec{b^4}$\n",
    "\n",
    "Dimensiones:\n",
    "- $\\vec{x} \\in \\mathcal{R}^{3}$\n",
    "- $f: \\mathcal{R}^{2} \\rightarrow \\mathcal{R}^{2}$\n",
    "- $g: \\mathcal{R}^{3} \\rightarrow \\mathcal{R}^{3}$\n",
    "- $h: \\mathcal{R}^{1} \\rightarrow \\mathcal{R}^{1}$\n",
    "- $W^1 \\in \\mathcal{R}^{3\\times2} \\text{, } b^1 \\in \\mathcal{R}^{2} $\n",
    "- $W^2 \\in \\mathcal{R}^{2\\times3} \\text{, } b^2 \\in \\mathcal{R}^{3} $\n",
    "- $W^3 \\in \\mathcal{R}^{3\\times1} \\text{, } b^3 \\in \\mathcal{R}^{1} $\n",
    "- $W^4 \\in \\mathcal{R}^{1\\times4} \\text{, } b^4 \\in \\mathcal{R}^{4} $\n",
    "\n",
    "**Pregunta 2**: Explique qué es backpropagation. ¿Cuales serían los parámetros a evaluar en la red neuronal anterior durante backpropagation? **(0.25 puntos)**\n",
    "\n",
    "**Respuesta**:\n",
    "\n",
    "Backpropagation es un método para entrenar redes neuronales que permite calcular de manera eficiente el gradiente de una función de pérdida respecto a todos sus parámetros. Backpropagation es eficaz ya que calcula las derivadas parciales respecto a cada *peso* mediante la regla de la cadena, calculando los gradientes de manera secuencial para cada capa, partiendo desde las capas exteriores hacia las capas inferiores para evitar cálculos redundantes de términos intermedios en la regla de la cadena.\n",
    "\n",
    "En la red neuronal anterior los parámetros a evaluar durante backpropagation son $W^1$, $\\vec{b^1}$, $W^2$, $\\vec{b^2}$, $W^3$, $\\vec{b^3}$, $W^4$ y $\\vec{b^4}$. Se pueden *agregar* los parámetros $\\vec{b^1}$, $\\vec{b^2}$, $\\vec{b^3}$ y  $\\vec{b^4}$ dentro de sus *respectivas* matrices de modo que los parámetros del modelo sean solo las matrices $W^1$, $W^2$, $W^3$, $W^4$ (matrices que, implicitamente, incluyen a los vectores $\\vec{b^1}$, $\\vec{b^2}$, $\\vec{b^3}$ y  $\\vec{b^4}$)\n",
    "\n",
    "**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? **(0.25 puntos)**\n",
    "\n",
    "**Respuesta**:\n",
    "\n",
    "Los pasos de backpropagation son los siguientes:\n",
    "\n",
    "- Se calcularon los outputs de la red, es decir, desde la *input layer* se pasan los inputs a través de las *hidden layer* hasta la *ouput layer*.\n",
    "- Luego, se quiere calcular las siguientes derivadas parciales:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{l}_{[i,j]}} = \\vec{\\delta^{l}_{[j]}}\\times\\vec{z^{(l-1)}_{[i]}}\n",
    "\\end{equation}\n",
    "\n",
    "- Nótese que los valores $\\vec{z^{(l-1)}_{[i]}}$ fueron calculados inicialmente, por lo que es necesario calcular $\\vec{\\delta^{l}_{[j]}}$\n",
    "- $\\vec{\\delta^{l}_{[j]}}$ viene dado por:\n",
    "\\begin{equation}\n",
    "\\vec{\\delta^{l}_{[j]}} = g^{l}(h^{l}_{[j]}) \\times \\sum_{k}{}\\left(\\vec{\\delta^{l}_{[k]}} \\times W^{l+1}_{[j,k]} \\right)\n",
    "\\end{equation}\n",
    "- Como $\\delta^{l}_{[j]}$ depende los $\\delta^{l+1}_{[k]}$ se aplica la lógica de backpropagation: las derivadas parciales de la capa más superior son directas de calcular. Luego, con dichas derivadas se calculan las derivadas de las capas inferiores (que depende de una derivada de una capa superior que ya está calculada). Así, se calculan las derivadas de manera eficiente.\n",
    "\n",
    "En el ejemplo anterior, se tienen que calcular las derivadas parciales de la función de pérdida con respecto a los últimos nodos de la red y, con dichas derivadas, calcular las derivadas de los nodos internos. Así, solo se realizaron cálculos directos de cuatro derivadas (asociadas a los nodos de la capa superior) y luego, las derivadas de las capas inferiores son calculadas de forma *indirecta* utilizando las derivadas ya calculadas.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocS_vQhR1gcU"
   },
   "source": [
    "## **Preguntas prácticas 💻 (4 puntos).** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ol82nJ0FnmcP"
   },
   "source": [
    "### **Parte 3: Word Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Daw7Ee5cdQTb"
   },
   "source": [
    "En la auxiliar 2 se nombraron dos formas de crear word vectors:\n",
    "\n",
    "-  Distributional Vectors.\n",
    "-  Distributed Vectors.\n",
    "\n",
    "El objetivo de esta parte es comparar las dos embeddings obtenidos de estas dos estrategias en una tarea de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "E2G1qcb7AJqW"
   },
   "outputs": [],
   "source": [
    "import re  \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict \n",
    "import string \n",
    "import multiprocessing\n",
    "import os\n",
    "import gensim\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuEAv-whdMCG"
   },
   "source": [
    "#### **Parte A (1 punto)** \n",
    "\n",
    "En esta parte debe crear una matriz palabra contexto, para esto, complete el siguiente template (para esta parte puede utilizar las librerías ```numpy``` y/o ```scipy```). Hint: revise como utilizar matrices sparse de ```scipy```\n",
    "\n",
    "```python\n",
    "class WordContextMatrix:\n",
    "\n",
    "  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Utilice el constructor para definir los parametros.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # se sugiere agregar un una estructura de datos para guardar las\n",
    "    # palabras del vocab y para guardar el conteo de coocurrencia\n",
    "    ...\n",
    "    \n",
    "  def add_word_to_vocab(self, word):\n",
    "    \"\"\"\n",
    "    Utilice este método para agregar token\n",
    "    a sus vocabulario\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Le puede ser útil considerar un token unk al vocab\n",
    "    # para palabras fuera del vocab\n",
    "    ...\n",
    "  \n",
    "  def build_matrix(self):\n",
    "    \"\"\"\n",
    "    Utilice este método para crear la palabra contexto\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "  def matrix2dict(self):\n",
    "    \"\"\"\n",
    "    Utilice este método para convertir la matriz a un diccionario de embeddings, donde las llaves deben ser los token del vocabulario y los embeddings los valores obtenidos de la matriz. \n",
    "    \"\"\"\n",
    "\n",
    "    # se recomienda transformar la matrix a un diccionario de embedding.\n",
    "    # por ejemplo {palabra1:vec1, palabra2:vec2, ...}\n",
    "    ...\n",
    "\n",
    "```\n",
    "\n",
    "puede modificar los parámetros o métodos si lo considera necesario. Para probar la matrix puede utilizar el siguiente corpus.\n",
    "\n",
    "```python\n",
    "corpus = [\n",
    "  \"I like deep learning.\",\n",
    "  \"I like NLP.\",\n",
    "  \"I enjoy flying.\"\n",
    "]\n",
    "```\n",
    "\n",
    "Obteniendo una matriz parecia a esta:\n",
    "\n",
    "***Resultado esperado***: \n",
    "\n",
    "| counts   | I  | like | enjoy | deep | learning | NLP | flying | . |   \n",
    "|----------|---:|-----:|------:|-----:|---------:|----:|-------:|--:|\n",
    "| I        | 0  |  2   |  1    |    0 |  0       |   0 | 0      | 0|            \n",
    "| like     |  2 |    0 |  0    |    1 |  0       |   1 | 0      | 0 | \n",
    "| enjoy    |  1 |    0 |  0    |    0 |  0       |   0 | 1      | 0 |\n",
    "| deep     |  0 |    1 |  0    |    0 |  1       |   0 | 0      | 0 |  \n",
    "| learning |  0 |    0 |  0    |    1 |  0       |   0 | 0      | 1 |          \n",
    "| NLP      |  0 |    1 |  0    |    0 |  0       |   0 | 0      | 1 |\n",
    "| flying   |  0 |    0 |  1    |    0 |  0       |   0 | 0      | 1 | \n",
    "| .        |  0 |    0 |  0    |    0 |  1       |   1 | 1      | 0 | \n",
    "\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ur16vkyO37B5"
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gOI1FL8MlGZB"
   },
   "outputs": [],
   "source": [
    "class WordContextMatrix:\n",
    "    def __init__(self, vocab_size, window_size, dataset, tokenizer):\n",
    "        \"\"\"\n",
    "        Utilice el constructor para definir los parametros.\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n = 0\n",
    "        self.window_size = window_size\n",
    "        self.vocab = {}\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mat = np.zeros((self.vocab_size, self.vocab_size))\n",
    "\n",
    "        self.create_vocab()\n",
    "        self.build_matrix()\n",
    "\n",
    "    def create_vocab(self):\n",
    "        cleaned_content = list(map(self.tokenizer, self.dataset))\n",
    "        phrases = Phrases(cleaned_content, min_count=100, progress_per=5000) \n",
    "        bigram = Phraser(phrases)\n",
    "        self.dataset = bigram[cleaned_content]\n",
    "\n",
    "        all_words = []\n",
    "        for words in self.dataset:\n",
    "            for word in words:\n",
    "                all_words.append(word)\n",
    "\n",
    "        count = Counter(all_words)\n",
    "        count = count.most_common(self.vocab_size)\n",
    "\n",
    "        for (word, _) in count:\n",
    "            self.add_word_to_vocab(word)\n",
    "\n",
    "    def add_word_to_vocab(self, word):\n",
    "        \"\"\"\n",
    "        Utilice este método para agregar token\n",
    "        a sus vocabulario\n",
    "        \"\"\"\n",
    "        # Le puede ser útil considerar un token unk al vocab\n",
    "        # para palabras fuera del vocab\n",
    "\n",
    "        if self.n >= self.vocab_size:\n",
    "            return\n",
    "        else:\n",
    "            self.vocab[word] = self.n\n",
    "            self.n += 1\n",
    "\n",
    "    def build_matrix(self):\n",
    "        \"\"\"\n",
    "        Utilice este método para crear la palabra contexto\n",
    "        \"\"\"\n",
    "        for words in self.dataset:\n",
    "            for (i, word) in enumerate(words):\n",
    "                if word in self.vocab:\n",
    "                    before = i - self.window_size//2\n",
    "                    if before >= 0:\n",
    "                        for j in range(before, i):\n",
    "                            if words[j] in self.vocab:\n",
    "                                word_1_indx = self.vocab[word]\n",
    "                                word_2_indx = self.vocab[words[j]]\n",
    "                                self.mat[word_1_indx][word_2_indx] += 1\n",
    "                                self.mat[word_2_indx][word_1_indx] += 1\n",
    "\n",
    "    def matrix2dict(self):\n",
    "        \"\"\"\n",
    "        Utilice este método para convertir la matriz a un diccionario de\n",
    "        embeddings, donde las llaves deben ser los token del vocabulario y los\n",
    "        embeddings los valores obtenidos de la matriz.\n",
    "        \"\"\"\n",
    "        dic_matrix = {}\n",
    "        for key in self.vocab.keys():\n",
    "            dic_matrix[key] = self.mat[self.vocab[key]]\n",
    "        return dic_matrix\n",
    "\n",
    "        # se recomienda transformar la matrix a un diccionario de embedding.\n",
    "        # por ejemplo {palabra1:vec1, palabra2:vec2, ...}\n",
    "\n",
    "    def print_matrix(self):\n",
    "        hola = \"        \"\n",
    "        print(hola, end=\" \")\n",
    "        for key in self.vocab.keys():\n",
    "            print(key, end=\" \")\n",
    "        print(\"\")\n",
    "\n",
    "        for key in self.vocab.keys():\n",
    "            print(key + \" \"*(len(hola)-len(key)), end=\" \")\n",
    "            i = self.vocab[key]\n",
    "\n",
    "            for key in self.vocab.keys():\n",
    "                j = self.vocab[key]\n",
    "                print(self.mat[i][j], end=\" \")\n",
    "            print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgmeSFqKLpFL"
   },
   "source": [
    "#### **Parte B (1.5 puntos)**\n",
    "\n",
    "En esta parte es debe entrenar Word2Vec de gensim y construir la matriz palabra contexto utilizando el dataset de diálogos de los Simpson. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZgN06q4QPi3"
   },
   "source": [
    "Utilizando el dataset adjunto con la tarea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eY3kmg4onnsu",
    "outputId": "d3525a54-0c10-401e-b3e2-9c6e9e714a2c"
   },
   "outputs": [],
   "source": [
    "data_file = \"dialogue-lines-of-the-simpsons.zip\"\n",
    "df = pd.read_csv(data_file)\n",
    "df = df.dropna().reset_index(drop=True) # Quitar filas vacias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAg5a5bmWk3T"
   },
   "source": [
    "**Pregunta 1**: Ayudándose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec. **(0.75 punto)** (Hint, le puede servir explorar un poco los datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWw2fXFRXe5Y"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Bvwplz7yTNcr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nicolas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "2022-05-30 22:39:00,346 : INFO : collecting all words and their counts\n",
      "2022-05-30 22:39:00,346 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2022-05-30 22:39:00,387 : INFO : PROGRESS: at sentence #5000, processed 49035 words and 36026 word types\n",
      "2022-05-30 22:39:00,429 : INFO : PROGRESS: at sentence #10000, processed 96482 words and 62640 word types\n",
      "2022-05-30 22:39:00,468 : INFO : PROGRESS: at sentence #15000, processed 141538 words and 85150 word types\n",
      "2022-05-30 22:39:00,519 : INFO : PROGRESS: at sentence #20000, processed 195196 words and 109693 word types\n",
      "2022-05-30 22:39:00,569 : INFO : PROGRESS: at sentence #25000, processed 247786 words and 133380 word types\n",
      "2022-05-30 22:39:00,623 : INFO : PROGRESS: at sentence #30000, processed 303591 words and 157130 word types\n",
      "2022-05-30 22:39:00,673 : INFO : PROGRESS: at sentence #35000, processed 356281 words and 177689 word types\n",
      "2022-05-30 22:39:00,716 : INFO : PROGRESS: at sentence #40000, processed 403454 words and 195411 word types\n",
      "2022-05-30 22:39:00,759 : INFO : PROGRESS: at sentence #45000, processed 449880 words and 212946 word types\n",
      "2022-05-30 22:39:00,802 : INFO : PROGRESS: at sentence #50000, processed 495223 words and 229961 word types\n",
      "2022-05-30 22:39:00,842 : INFO : PROGRESS: at sentence #55000, processed 539312 words and 245935 word types\n",
      "2022-05-30 22:39:00,882 : INFO : PROGRESS: at sentence #60000, processed 580923 words and 260317 word types\n",
      "2022-05-30 22:39:00,925 : INFO : PROGRESS: at sentence #65000, processed 625928 words and 275730 word types\n",
      "2022-05-30 22:39:00,972 : INFO : PROGRESS: at sentence #70000, processed 676880 words and 294038 word types\n",
      "2022-05-30 22:39:01,020 : INFO : PROGRESS: at sentence #75000, processed 727546 words and 311277 word types\n",
      "2022-05-30 22:39:01,070 : INFO : PROGRESS: at sentence #80000, processed 778548 words and 328056 word types\n",
      "2022-05-30 22:39:01,121 : INFO : PROGRESS: at sentence #85000, processed 829240 words and 344507 word types\n",
      "2022-05-30 22:39:01,177 : INFO : PROGRESS: at sentence #90000, processed 877998 words and 360233 word types\n",
      "2022-05-30 22:39:01,227 : INFO : PROGRESS: at sentence #95000, processed 928648 words and 375729 word types\n",
      "2022-05-30 22:39:01,277 : INFO : PROGRESS: at sentence #100000, processed 978326 words and 391491 word types\n",
      "2022-05-30 22:39:01,329 : INFO : PROGRESS: at sentence #105000, processed 1029302 words and 407068 word types\n",
      "2022-05-30 22:39:01,383 : INFO : PROGRESS: at sentence #110000, processed 1080794 words and 423039 word types\n",
      "2022-05-30 22:39:01,433 : INFO : PROGRESS: at sentence #115000, processed 1130222 words and 437656 word types\n",
      "2022-05-30 22:39:01,485 : INFO : PROGRESS: at sentence #120000, processed 1180592 words and 452581 word types\n",
      "2022-05-30 22:39:01,540 : INFO : PROGRESS: at sentence #125000, processed 1230252 words and 466277 word types\n",
      "2022-05-30 22:39:01,589 : INFO : PROGRESS: at sentence #130000, processed 1279612 words and 478785 word types\n",
      "2022-05-30 22:39:01,609 : INFO : collected 483384 token types (unigram + bigrams) from a corpus of 1297999 words and 131853 sentences\n",
      "2022-05-30 22:39:01,609 : INFO : merged Phrases<483384 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000>\n",
      "2022-05-30 22:39:01,610 : INFO : Phrases lifecycle event {'msg': 'built Phrases<483384 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000> in 1.26s', 'datetime': '2022-05-30T22:39:01.610782', 'gensim': '4.2.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'created'}\n",
      "2022-05-30 22:39:01,610 : INFO : exporting phrases from Phrases<483384 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000>\n",
      "2022-05-30 22:39:02,323 : INFO : FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<70 phrases, min_count=100, threshold=10.0> from Phrases<483384 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000> in 0.71s', 'datetime': '2022-05-30T22:39:02.323067', 'gensim': '4.2.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "content = df[\"spoken_words\"]\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# limpiar puntuaciones y separar por tokens.\n",
    "punctuation = string.punctuation + \"«»“”‘’…—\"\n",
    "stopwords_lst = set(stopwords.words('english'))\n",
    "\n",
    "def simple_tokenizer(doc, lower=False, b_stopwords=False):\n",
    "    global stopwords_lst, punctuation\n",
    "    if lower:\n",
    "        tokenized_doc = doc.translate(str.maketrans(\n",
    "            '', '', punctuation)).lower().split()\n",
    "\n",
    "    tokenized_doc = doc.translate(str.maketrans('', '', punctuation)).split()\n",
    "    \n",
    "    if b_stopwords:\n",
    "        tokenized_doc = [\n",
    "            token for token in tokenized_doc if token.lower() not in stopwords_lst\n",
    "        ]\n",
    "    return tokenized_doc\n",
    "\n",
    "\n",
    "cleaned_content = [simple_tokenizer(doc) for doc in content.values]\n",
    "phrases = Phrases(cleaned_content, min_count=100, progress_per=5000) \n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[cleaned_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 22:39:02,328 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.03>', 'datetime': '2022-05-30T22:39:02.328687', 'gensim': '4.2.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "simpsons_w2v = Word2Vec(min_count=10,\n",
    "                        window=4,\n",
    "                        vector_size=300,\n",
    "                        sample=6e-5,\n",
    "                        alpha=0.03,\n",
    "                        min_alpha=0.0007,\n",
    "                        negative=20,\n",
    "                        workers=multiprocessing.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 22:39:02,338 : INFO : collecting all words and their counts\n",
      "2022-05-30 22:39:02,338 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-05-30 22:39:02,420 : INFO : PROGRESS: at sentence #10000, processed 94692 words, keeping 12113 word types\n",
      "2022-05-30 22:39:02,505 : INFO : PROGRESS: at sentence #20000, processed 191523 words, keeping 18839 word types\n",
      "2022-05-30 22:39:02,601 : INFO : PROGRESS: at sentence #30000, processed 297968 words, keeping 24927 word types\n",
      "2022-05-30 22:39:02,686 : INFO : PROGRESS: at sentence #40000, processed 396027 words, keeping 29195 word types\n",
      "2022-05-30 22:39:02,767 : INFO : PROGRESS: at sentence #50000, processed 486084 words, keeping 33175 word types\n",
      "2022-05-30 22:39:02,845 : INFO : PROGRESS: at sentence #60000, processed 570185 words, keeping 36616 word types\n",
      "2022-05-30 22:39:02,932 : INFO : PROGRESS: at sentence #70000, processed 664434 words, keeping 40330 word types\n",
      "2022-05-30 22:39:03,022 : INFO : PROGRESS: at sentence #80000, processed 764408 words, keeping 43973 word types\n",
      "2022-05-30 22:39:03,108 : INFO : PROGRESS: at sentence #90000, processed 862283 words, keeping 47270 word types\n",
      "2022-05-30 22:39:03,195 : INFO : PROGRESS: at sentence #100000, processed 960954 words, keeping 50292 word types\n",
      "2022-05-30 22:39:03,286 : INFO : PROGRESS: at sentence #110000, processed 1061796 words, keeping 53614 word types\n",
      "2022-05-30 22:39:03,375 : INFO : PROGRESS: at sentence #120000, processed 1159986 words, keeping 56466 word types\n",
      "2022-05-30 22:39:03,463 : INFO : PROGRESS: at sentence #130000, processed 1257082 words, keeping 58619 word types\n",
      "2022-05-30 22:39:03,480 : INFO : collected 58974 word types from a corpus of 1275090 raw words and 131853 sentences\n",
      "2022-05-30 22:39:03,480 : INFO : Creating a fresh vocabulary\n",
      "2022-05-30 22:39:03,516 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 7913 unique words (13.42% of original 58974, drops 51061)', 'datetime': '2022-05-30T22:39:03.516798', 'gensim': '4.2.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-05-30 22:39:03,517 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 1166803 word corpus (91.51% of original 1275090, drops 108287)', 'datetime': '2022-05-30T22:39:03.517799', 'gensim': '4.2.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-05-30 22:39:03,555 : INFO : deleting the raw counts dictionary of 58974 items\n",
      "2022-05-30 22:39:03,556 : INFO : sample=6e-05 downsamples 801 most-common words\n",
      "2022-05-30 22:39:03,556 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 487197.76373725804 word corpus (41.8%% of prior 1166803)', 'datetime': '2022-05-30T22:39:03.556813', 'gensim': '4.2.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-05-30 22:39:03,621 : INFO : estimated required memory for 7913 words and 300 dimensions: 22947700 bytes\n",
      "2022-05-30 22:39:03,622 : INFO : resetting layer weights\n",
      "2022-05-30 22:39:03,629 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-05-30T22:39:03.629730', 'gensim': '4.2.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'build_vocab'}\n"
     ]
    }
   ],
   "source": [
    "simpsons_w2v.build_vocab(sentences, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 22:39:03,635 : INFO : Word2Vec lifecycle event {'msg': 'training model with 12 workers on 7913 vocabulary and 300 features, using sg=0 hs=0 sample=6e-05 negative=20 window=4 shrink_windows=True', 'datetime': '2022-05-30T22:39:03.635732', 'gensim': '4.2.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2022-05-30 22:39:04,795 : INFO : EPOCH 0 - PROGRESS: at 49.68% examples, 206666 words/s, in_qsize 23, out_qsize 1\n",
      "2022-05-30 22:39:05,267 : INFO : EPOCH 0: training on 1275090 raw words (487178 effective words) took 1.6s, 300071 effective words/s\n",
      "2022-05-30 22:39:06,278 : INFO : EPOCH 1 - PROGRESS: at 38.28% examples, 186979 words/s, in_qsize 20, out_qsize 3\n",
      "2022-05-30 22:39:06,900 : INFO : EPOCH 1: training on 1275090 raw words (486637 effective words) took 1.6s, 299496 effective words/s\n",
      "2022-05-30 22:39:07,914 : INFO : EPOCH 2 - PROGRESS: at 46.33% examples, 221447 words/s, in_qsize 15, out_qsize 1\n",
      "2022-05-30 22:39:08,522 : INFO : EPOCH 2: training on 1275090 raw words (487626 effective words) took 1.6s, 302404 effective words/s\n",
      "2022-05-30 22:39:09,544 : INFO : EPOCH 3 - PROGRESS: at 38.28% examples, 185915 words/s, in_qsize 23, out_qsize 0\n",
      "2022-05-30 22:39:10,276 : INFO : EPOCH 3: training on 1275090 raw words (487556 effective words) took 1.7s, 279423 effective words/s\n",
      "2022-05-30 22:39:11,402 : INFO : EPOCH 4 - PROGRESS: at 47.20% examples, 202595 words/s, in_qsize 23, out_qsize 0\n",
      "2022-05-30 22:39:11,912 : INFO : EPOCH 4: training on 1275090 raw words (487357 effective words) took 1.6s, 299376 effective words/s\n",
      "2022-05-30 22:39:13,032 : INFO : EPOCH 5 - PROGRESS: at 48.07% examples, 206696 words/s, in_qsize 23, out_qsize 0\n",
      "2022-05-30 22:39:13,515 : INFO : EPOCH 5: training on 1275090 raw words (486800 effective words) took 1.6s, 305112 effective words/s\n",
      "2022-05-30 22:39:14,528 : INFO : EPOCH 6 - PROGRESS: at 37.44% examples, 182853 words/s, in_qsize 18, out_qsize 6\n",
      "2022-05-30 22:39:15,169 : INFO : EPOCH 6: training on 1275090 raw words (486984 effective words) took 1.6s, 295963 effective words/s\n",
      "2022-05-30 22:39:16,203 : INFO : EPOCH 7 - PROGRESS: at 37.39% examples, 179002 words/s, in_qsize 23, out_qsize 0\n",
      "2022-05-30 22:39:16,951 : INFO : EPOCH 7: training on 1275090 raw words (487018 effective words) took 1.8s, 274712 effective words/s\n",
      "2022-05-30 22:39:18,000 : INFO : EPOCH 8 - PROGRESS: at 48.07% examples, 221399 words/s, in_qsize 16, out_qsize 0\n",
      "2022-05-30 22:39:18,615 : INFO : EPOCH 8: training on 1275090 raw words (486616 effective words) took 1.7s, 294004 effective words/s\n",
      "2022-05-30 22:39:19,639 : INFO : EPOCH 9 - PROGRESS: at 39.10% examples, 188725 words/s, in_qsize 19, out_qsize 5\n",
      "2022-05-30 22:39:20,256 : INFO : EPOCH 9: training on 1275090 raw words (487544 effective words) took 1.6s, 298851 effective words/s\n",
      "2022-05-30 22:39:21,275 : INFO : EPOCH 10 - PROGRESS: at 39.07% examples, 189887 words/s, in_qsize 15, out_qsize 9\n",
      "2022-05-30 22:39:21,900 : INFO : EPOCH 10: training on 1275090 raw words (486700 effective words) took 1.6s, 297715 effective words/s\n",
      "2022-05-30 22:39:23,030 : INFO : EPOCH 11 - PROGRESS: at 46.33% examples, 198371 words/s, in_qsize 23, out_qsize 0\n",
      "2022-05-30 22:39:23,652 : INFO : EPOCH 11: training on 1275090 raw words (487008 effective words) took 1.7s, 279532 effective words/s\n",
      "2022-05-30 22:39:24,685 : INFO : EPOCH 12 - PROGRESS: at 46.33% examples, 217490 words/s, in_qsize 16, out_qsize 0\n",
      "2022-05-30 22:39:25,315 : INFO : EPOCH 12: training on 1275090 raw words (488450 effective words) took 1.7s, 295140 effective words/s\n",
      "2022-05-30 22:39:26,344 : INFO : EPOCH 13 - PROGRESS: at 38.31% examples, 184096 words/s, in_qsize 23, out_qsize 0\n",
      "2022-05-30 22:39:26,990 : INFO : EPOCH 13: training on 1275090 raw words (487033 effective words) took 1.7s, 292481 effective words/s\n",
      "2022-05-30 22:39:28,004 : INFO : EPOCH 14 - PROGRESS: at 37.42% examples, 183311 words/s, in_qsize 14, out_qsize 10\n",
      "2022-05-30 22:39:28,631 : INFO : EPOCH 14: training on 1275090 raw words (487569 effective words) took 1.6s, 298814 effective words/s\n",
      "2022-05-30 22:39:28,631 : INFO : Word2Vec lifecycle event {'msg': 'training on 19126350 raw words (7308076 effective words) took 25.0s, 292371 effective words/s', 'datetime': '2022-05-30T22:39:28.631995', 'gensim': '4.2.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.42 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "simpsons_w2v.train(sentences, total_examples=simpsons_w2v.corpus_count, epochs=15, report_delay=10)\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicolas\\AppData\\Local\\Temp\\ipykernel_10256\\358243844.py:1: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  simpsons_w2v.init_sims(replace=True)\n",
      "2022-05-30 22:39:28,640 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    }
   ],
   "source": [
    "simpsons_w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vBkF3hreGjg"
   },
   "source": [
    "**Pregunta 2**: Cree una matriz palabra contexto usando el mismo dataset. Configure el largo del vocabulario a 1000 o 2000 tokens, puede agregar valores mayores pero tenga en cuenta que la construcción de la matriz puede tomar varios minutos. Puede que esto tarde un poco. **(0.75 punto)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzLuH6MneWIY"
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9gPyW8fMeXNX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 22:39:29,342 : INFO : collecting all words and their counts\n",
      "2022-05-30 22:39:29,343 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2022-05-30 22:39:29,384 : INFO : PROGRESS: at sentence #5000, processed 49035 words and 36026 word types\n",
      "2022-05-30 22:39:29,428 : INFO : PROGRESS: at sentence #10000, processed 96482 words and 62640 word types\n",
      "2022-05-30 22:39:29,470 : INFO : PROGRESS: at sentence #15000, processed 141538 words and 85150 word types\n",
      "2022-05-30 22:39:29,521 : INFO : PROGRESS: at sentence #20000, processed 195196 words and 109693 word types\n",
      "2022-05-30 22:39:29,569 : INFO : PROGRESS: at sentence #25000, processed 247786 words and 133380 word types\n",
      "2022-05-30 22:39:29,622 : INFO : PROGRESS: at sentence #30000, processed 303591 words and 157130 word types\n",
      "2022-05-30 22:39:29,678 : INFO : PROGRESS: at sentence #35000, processed 356281 words and 177689 word types\n",
      "2022-05-30 22:39:29,722 : INFO : PROGRESS: at sentence #40000, processed 403454 words and 195411 word types\n",
      "2022-05-30 22:39:29,766 : INFO : PROGRESS: at sentence #45000, processed 449880 words and 212946 word types\n",
      "2022-05-30 22:39:29,810 : INFO : PROGRESS: at sentence #50000, processed 495223 words and 229961 word types\n",
      "2022-05-30 22:39:29,854 : INFO : PROGRESS: at sentence #55000, processed 539312 words and 245935 word types\n",
      "2022-05-30 22:39:29,895 : INFO : PROGRESS: at sentence #60000, processed 580923 words and 260317 word types\n",
      "2022-05-30 22:39:29,939 : INFO : PROGRESS: at sentence #65000, processed 625928 words and 275730 word types\n",
      "2022-05-30 22:39:29,987 : INFO : PROGRESS: at sentence #70000, processed 676880 words and 294038 word types\n",
      "2022-05-30 22:39:30,036 : INFO : PROGRESS: at sentence #75000, processed 727546 words and 311277 word types\n",
      "2022-05-30 22:39:30,089 : INFO : PROGRESS: at sentence #80000, processed 778548 words and 328056 word types\n",
      "2022-05-30 22:39:30,141 : INFO : PROGRESS: at sentence #85000, processed 829240 words and 344507 word types\n",
      "2022-05-30 22:39:30,197 : INFO : PROGRESS: at sentence #90000, processed 877998 words and 360233 word types\n",
      "2022-05-30 22:39:30,245 : INFO : PROGRESS: at sentence #95000, processed 928648 words and 375729 word types\n",
      "2022-05-30 22:39:30,293 : INFO : PROGRESS: at sentence #100000, processed 978326 words and 391491 word types\n",
      "2022-05-30 22:39:30,345 : INFO : PROGRESS: at sentence #105000, processed 1029302 words and 407068 word types\n",
      "2022-05-30 22:39:30,398 : INFO : PROGRESS: at sentence #110000, processed 1080794 words and 423039 word types\n",
      "2022-05-30 22:39:30,446 : INFO : PROGRESS: at sentence #115000, processed 1130222 words and 437656 word types\n",
      "2022-05-30 22:39:30,498 : INFO : PROGRESS: at sentence #120000, processed 1180592 words and 452581 word types\n",
      "2022-05-30 22:39:30,551 : INFO : PROGRESS: at sentence #125000, processed 1230252 words and 466277 word types\n",
      "2022-05-30 22:39:30,600 : INFO : PROGRESS: at sentence #130000, processed 1279612 words and 478785 word types\n",
      "2022-05-30 22:39:30,618 : INFO : collected 483384 token types (unigram + bigrams) from a corpus of 1297999 words and 131853 sentences\n",
      "2022-05-30 22:39:30,619 : INFO : merged Phrases<483384 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000>\n",
      "2022-05-30 22:39:30,619 : INFO : Phrases lifecycle event {'msg': 'built Phrases<483384 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000> in 1.28s', 'datetime': '2022-05-30T22:39:30.619658', 'gensim': '4.2.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'created'}\n",
      "2022-05-30 22:39:30,620 : INFO : exporting phrases from Phrases<483384 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000>\n",
      "2022-05-30 22:39:31,322 : INFO : FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<70 phrases, min_count=100, threshold=10.0> from Phrases<483384 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000> in 0.70s', 'datetime': '2022-05-30T22:39:31.322469', 'gensim': '4.2.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "simpsons_wcm = WordContextMatrix(6000, 4, content, simple_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRCB-jqgTNcs"
   },
   "source": [
    "#### **Parte C (1.5 puntos): Aplicar embeddings para clasificar**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlqzlJRSTNcs"
   },
   "source": [
    "Ahora utilizaremos los embeddings que acabamos de calcular para clasificar palabras basadas en su polaridad (positivas o negativas). \n",
    "\n",
    "Para esto ocuparemos el lexicón AFINN incluido en la tarea, que incluye una lista de palabras y un 1 si su connotación es positiva y un -1 si es negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CMskFDmHTNcs"
   },
   "outputs": [],
   "source": [
    "AFINN = 'AFINN_full.csv'\n",
    "df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaKl8hsCTNcs"
   },
   "source": [
    "Hint: Para w2v y la wcm son esperables KeyErrors debido a que no todas las palabras del corpus de los simpsons tendrán una representación en AFINN. Para el caso de la matriz palabra contexto se recomienda convertir su matrix a un diccionario. Pueden utilizar esta función auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tWSSuctiTNcs"
   },
   "outputs": [],
   "source": [
    "def try_apply(model,word):\n",
    "    try:\n",
    "        aux = model[word]\n",
    "        return True\n",
    "    except KeyError:\n",
    "        #logger.error('Word {} not in dictionary'.format(word))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrVPeEzgTNcs"
   },
   "source": [
    "**Pregunta 1**: Transforme las palabras del corpus de AFINN a la representación en embedding que acabamos de calcular (con ambos modelos). \n",
    "\n",
    "Su dataframe final debe ser del estilo [embedding, sentimiento], donde los embeddings corresponden a $X$ y el sentimiento asociado con el embedding a $y$ (positivo/negativo, 1/-1). \n",
    "\n",
    "Para ambos modelos, separar train y test de acuerdo a la siguiente función. **(0.5 puntos)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_Y(model_dic, df_afinn):\n",
    "    model_df = []\n",
    "    for _, row in df_afinn.iterrows():\n",
    "        if try_apply(model_dic, row[0]):\n",
    "            model_df.append([model_dic[row[0]], row[1]])\n",
    "        else:\n",
    "            model_df.append([model_dic[\"hello\"]-model_dic[\"hello\"], row[1]])\n",
    "            \n",
    "    model_df = pd.DataFrame (model_df, columns = ['embedding', 'sentimiento'])\n",
    "    return train_test_split(model_df.embedding, model_df.sentimiento, random_state=0, test_size=0.1, stratify=model_df.sentimiento)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "0Bkt26BwTNcs"
   },
   "outputs": [],
   "source": [
    "wcm_dic = simpsons_wcm.matrix2dict()\n",
    "X_train_wcm, X_test_wcm, y_train_wcm, y_test_wcm = get_X_Y(wcm_dic, df_afinn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_dic = simpsons_w2v.wv\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = get_X_Y(w2v_dic, df_afinn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDKe4gA3TNct"
   },
   "source": [
    "**Pregunta 2**: Entrenar una regresión logística (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qué se obtienen estos resultados? Cómo los mejorarías? Como podrías mejorar los resultados de la matriz palabra contexto? es equivalente al modelo word2vec? **(1 punto)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJMzq_dETNct"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseFeature(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(BaseFeature):\n",
    "    def transform(self, X, y=None):\n",
    "        lst = []\n",
    "        for x in X:\n",
    "            lst.append(x)\n",
    "        return np.array(lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predic(X_train, y_train, X_test):\n",
    "    clf = LogisticRegression(max_iter=1000000)\n",
    "    model_transformer = Transformer()\n",
    "    pipeline = Pipeline([('doc2vec', model_transformer), ('clf', clf)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    return pipeline.predict(X_test)\n",
    "                         \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de confusión y accuracy, precision, recall, f1 para el modelo WCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_wcm = get_predic(X_train_wcm, y_train_wcm, X_test_wcm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[214   7]\n",
      " [ 95  23]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test_wcm, y_pred_wcm)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.97      0.81       221\n",
      "           1       0.77      0.19      0.31       118\n",
      "\n",
      "    accuracy                           0.70       339\n",
      "   macro avg       0.73      0.58      0.56       339\n",
      "weighted avg       0.72      0.70      0.63       339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_wcm, y_pred_wcm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de confusión y accuracy, precision, recall, f1 para el modelo W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_w2v = get_predic(X_train_w2v, y_train_w2v, X_test_w2v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[212   9]\n",
      " [ 96  22]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test_w2v, y_pred_w2v)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.96      0.80       221\n",
      "           1       0.71      0.19      0.30       118\n",
      "\n",
      "    accuracy                           0.69       339\n",
      "   macro avg       0.70      0.57      0.55       339\n",
      "weighted avg       0.70      0.69      0.63       339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_w2v, y_pred_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nótese que ambos modelos tienen resultados muy similares y lo anterior tiene sentido ya que se cumple la correspondencia entre modelos distribuidos y modelos distribucionales: ambos modelos están basados en la hipótesis distribucional, es decir, ambos capturan la similitud de palabras basándose en la similutd entre los contextos en que ocurren las palabras.\n",
    "\n",
    "Ambos modelos podrían ser mejorados agregando las palabras del dataset AFINN que no se encuentran en los vocabularios de los modelos. Teniendo un vocabulario completo, es decir, un vocabulario que incluya todas las palabras del dataset AFINN se esperaría que los resultados mejorasen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izppruGQTNct"
   },
   "source": [
    "# Bonus: +0.25 puntos en cualquier pregunta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YW0aeK2KTNct"
   },
   "source": [
    "**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset más grande y obtener mejores resultados. Les puede servir [ésta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim **(0.25 puntos)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvHcVS3sTNct"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "MSc8p-T8TNcu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 23:04:52,759 : INFO : loading projection weights from C:\\Users\\Nicolas/gensim-data\\glove-twitter-100\\glove-twitter-100.gz\n",
      "2022-05-30 23:06:13,995 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (1193514, 100) matrix of type float32 from C:\\\\Users\\\\Nicolas/gensim-data\\\\glove-twitter-100\\\\glove-twitter-100.gz', 'binary': False, 'encoding': 'utf8', 'datetime': '2022-05-30T23:06:13.995117', 'gensim': '4.2.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load(\"glove-twitter-100\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"num_records\": 1193514,\n",
      "    \"file_size\": 405932991,\n",
      "    \"base_dataset\": \"Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\",\n",
      "    \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py\",\n",
      "    \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
      "    \"parameters\": {\n",
      "        \"dimension\": 100\n",
      "    },\n",
      "    \"description\": \"Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)\",\n",
      "    \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt`.\",\n",
      "    \"read_more\": [\n",
      "        \"https://nlp.stanford.edu/projects/glove/\",\n",
      "        \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
      "    ],\n",
      "    \"checksum\": \"b04f7bed38756d64cf55b58ce7e97b15\",\n",
      "    \"file_name\": \"glove-twitter-100.gz\",\n",
      "    \"parts\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(api.info('glove-twitter-100'), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove, X_test_glove, y_train_glove, y_test_glove = get_X_Y(model, df_afinn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_glove = get_predic(X_train_glove, y_train_glove, X_test_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[210  11]\n",
      " [ 21  97]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test_glove, y_pred_glove)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.91      0.95      0.93       221\n",
      "           1       0.90      0.82      0.86       118\n",
      "\n",
      "    accuracy                           0.91       339\n",
      "   macro avg       0.90      0.89      0.89       339\n",
      "weighted avg       0.91      0.91      0.90       339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_glove, y_pred_glove))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tarea_3_Word_Embeddings.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
