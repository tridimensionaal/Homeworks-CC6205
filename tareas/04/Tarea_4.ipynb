{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwaDuQqCOyLJ"
   },
   "source": [
    "# **Tarea 4 - CC6205 Natural Language Processing üìö**\n",
    "\n",
    "**Integrantes:**\n",
    "\n",
    "**Fecha l√≠mite de entrega üìÜ:** Martes 07 de junio.\n",
    "\n",
    "**Tiempo estimado de dedicaci√≥n:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4lL5hGw07yP"
   },
   "source": [
    "Bienvenid@s a la cuarta tarea del curso de Natural Language Processing (NLP). \n",
    "En esta tarea estaremos tratando el problema de **tagging** (generaci√≥n de secuencias de etiquetas del mismo largo que la secuencia de input), el uso de **Convolutional Neural Networks** y **Recurrent Neural Networks**, e implementaremos una red usando PyTorch. \n",
    "\n",
    "Usen $\\LaTeX$ para las f√≥rmulas matem√°ticas. En la parte de programaci√≥n pueden usar lo que quieran, pero la [Auxiliar 3](https://youtu.be/36WTXvg3zh0) les puede ser de *gran ayuda*.\n",
    "\n",
    "**Instrucciones:**\n",
    "- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
    "- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n",
    "- El formato de entrega es este mismo Jupyter Notebook.\n",
    "- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n.\n",
    "- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav√©s del canal de Discord del curso.\n",
    "\n",
    "Si a√∫n no han visto las clases, se recomienda visitar los links de las referencias.\n",
    "\n",
    "**Referencias:**\n",
    "\n",
    "- [Tagging, and Hidden Markov Models ](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf) (slides by Michael Collins), [notes](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf), [video 1](https://youtu.be/-ngfOZz8yK0), [video 2](https://youtu.be/Tjgb-yQOg54), [video 3](https://youtu.be/aaa5Qoi8Vco), [video 4](https://youtu.be/4pKWIDkF_6Y)       \n",
    "- [MEMMs and CRFs](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-CRF.pdf): [notes 1](http://www.cs.columbia.edu/~mcollins/crf.pdf), [notes 2](http://www.cs.columbia.edu/~mcollins/fb.pdf), [video 1](https://youtu.be/qlI-4lSUDkg), [video 2](https://youtu.be/PLoLKQwkONw), [video 3](https://youtu.be/ZpUwDy6o28Y)\n",
    "- [Convolutional Neural Networks](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-CNN.pdf): [video](https://youtu.be/lLZW5Fn40r8)\n",
    "- [Recurrent Neural Networks](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-RNN.pdf): [video 1](https://youtu.be/BmhjUkzz3nk), [video 2](https://youtu.be/z43YFR1iIvk), [video 3](https://youtu.be/7L5JxQdwNJk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANqzQ3G9WNw3"
   },
   "source": [
    "# Hidden Markov Models (HMM), Maximum Entropy Markov Models (MEMM) and Conditional Random Field(CRF) (1,5 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWXD3D7RYKJ-"
   },
   "source": [
    "### Pregunta 1 (1 pt)\n",
    "Para un problema de POS tagging se define el conjunto de etiquetas $S = \\{ \\text{DET}, \\text{NOUN}, \\text{VERB}, \\text{ADP} \\}$ y se tiene un Hidden Markov Model con los siguientes par√°metros estimados a partir de un corpus de entrenamiento:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "q(\\text{NOUN}| \\text{ VERB}, \\text{DET}) &= 0.3 \\\\\n",
    "q(\\text{NOUN}|\\ w, \\text{DET}) &= 0 \\qquad \\forall w \\in S, w \\neq \\text{VERB} \\\\\n",
    "q(\\text{DET}| \\text{ VERB}, \\text{NOUN}) &= 0.4 \\\\\n",
    "q(\\text{DET}|\\ w, \\text{NOUN}) &= 0 \\qquad \\forall w \\in S, w \\neq \\text{VERB} \\\\\n",
    "e(the|\\text{ DET}) &= 0.5 \\\\\n",
    "e(pasta|\\text{ NOUN}) &= 0.6\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Luego para la oraci√≥n: `the man is pouring sauce on the pasta`, se tiene una tabla de programaci√≥n din√°mica con los siguientes valores:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\pi(7,\\text{DET},\\text{DET})&=0.1\\\\\n",
    "\\pi(7,\\text{NOUN},\\text{DET})&=0.2\\\\\n",
    "\\pi(7,\\text{VERB},\\text{DET})&=0.01\\\\\n",
    "\\pi(7,\\text{ADP},\\text{DET})&=0.5\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Con esta informaci√≥n, calcule el valor de $\\pi(8,\\text{DET},\\text{NOUN})$. Puede dejar el resultado expresado como una fracci√≥n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EzgysW9kGi-"
   },
   "source": [
    "**Respuesta**\n",
    "\n",
    "Por definici√≥n, se tiene que $\\pi(8,\\text{DET},\\text{NOUN})$ viene dado por:\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi(8,\\text{DET},\\text{NOUN}) = \\max_{w \\in S_{6}} (\\pi(7,w,\\text{DET}) \\cdot q(\\text{NOUN} | w, \\text{DET}) \\cdot e(x_{8} | \\text{NOUN}))\n",
    "\\end{equation}\n",
    "\n",
    "donde $x_{8}$ es la octava palabra de la oraci√≥n `the man is pouring sauce on the pasta`, es decir, $x_{8} = \\text{pasta}$ y $S_{6} = \\{ \\text{DET}, \\text{NOUN}, \\text{VERB},  \\text{ADP} \\}$.\n",
    "\n",
    "Ahora, los valores para la expresi√≥n $\\pi(7,w,\\text{DET}) \\cdot q(\\text{NOUN} | w, \\text{DET}) \\cdot e(x_{8} | \\text{NOUN})$ en funci√≥n de los posibles valores para $w$ son los siguientes:\n",
    "\n",
    "- $w=\\text{DET}$: \n",
    "\\begin{equation}\n",
    "\\pi(7,\\text{DET},\\text{DET}) \\cdot q(\\text{NOUN} | \\text{DET}, \\text{DET}) \\cdot e(\\text{pasta} | \\text{NOUN}) = 0.1 \\cdot 0 \\cdot 0.6 = 0\n",
    "\\end{equation}\n",
    "\n",
    "- $w=\\text{NOUN}$: \n",
    "\\begin{equation}\n",
    "\\pi(7,\\text{NOUN},\\text{DET}) \\cdot q(\\text{NOUN} | \\text{NOUN}, \\text{DET}) \\cdot e(\\text{pasta} | \\text{NOUN}) = 0.2 \\cdot 0 \\cdot 0.6 = 0\n",
    "\\end{equation}\n",
    "\n",
    "- $w=\\text{VERB}$: \n",
    "\\begin{equation} \n",
    "\\pi(7,\\text{VERB},\\text{DET}) \\cdot q(\\text{NOUN} | \\text{VERB}, \\text{DET}) \\cdot e(\\text{pasta} | \\text{NOUN}) = 0.01 \\cdot 0.3 \\cdot 0.6 = 0.018\n",
    "\\end{equation}\n",
    "\n",
    "- $w=\\text{ADP}$: \n",
    "\\begin{equation} \n",
    "\\pi(7,\\text{ADP},\\text{DET}) \\cdot q(\\text{NOUN} | \\text{ADP}, \\text{DET}) \\cdot e(\\text{pasta} | \\text{NOUN}) = 0.5 \\cdot 0 \\cdot 0.6 = 0\n",
    "\\end{equation}\n",
    "\n",
    "Luego, se tiene que \n",
    "\\begin{equation}\n",
    "\\pi(8,\\text{DET},\\text{NOUN}) = 0.018\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiwJb_vmkKLZ"
   },
   "source": [
    "### Pregunta 2 (0.5 pts)\n",
    "Comente  sobre las similitudes o diferencias entre los HMMs, MEMMs y CRFs. Para esto, responda las siguientes preguntas.\n",
    "\n",
    "#### 2.1. ¬øPara qu√© tipo de tarea sirven? D√© dos ejemplo de este tipo de tarea y descr√≠balos brevemente. (0.1 pts)\n",
    "\n",
    "**Respuesta:** \n",
    "\n",
    "Los modelos HMMS, MEMMs y CRFs sirven para etiquetar textos o, m√°s general, asignar etiquetas discretas a elementos discretos. Por ejemplo, tareas de etiquetado como *part-of-speech tagging (POS)* o *Named Entity Recognition (NER)* son tareas que pueden realizar los modelos HMMS, MEMMs y CRF. En particular, respecto al objetivo de las tareas mencionadas, se tiene que *part-of-speech tagging (POS)* busca determinar a que categor√≠a gramatical pertenece cada palabra en un texto y *Named Entity Recognition (NER)* que busca reconcer *entidades* dentro de una oraci√≥n.\n",
    "\n",
    "#### 2.2. ¬øQu√© modelos usan features? ¬øQu√© ventajas conlleva esto? (0.1 pts)\n",
    "\n",
    "**Respuesta:** \n",
    "\n",
    "Los modelos MEMMs y CRFs usan features. La ventaja de los vectores features es que permiten realizar una representaci√≥n m√°s completa del texto ya que los vectores features permiten *agregar* conocimiento experto, es decir, los vectores features pertimen codificar *conocimiento* que no est√° presente en el texto.\n",
    "\n",
    "\n",
    "#### 2.3. ¬øC√≥mo maneja cada uno de los modelos las palabras con baja frecuencia en el set de train? (0.1 pts)\n",
    "\n",
    "**Respuesta:** \n",
    "\n",
    "`Escriba su respuesta aqu√≠`\n",
    "\n",
    "#### 2.4. ¬øQu√© le permite a los CRF realizar decisiones globales? ¬øQu√© diferencia con respecto a los MEMMs permite lograr esto? ¬øPor qu√© los HMMs tampoco son capaces de tomar decisiones globales? (0.1 pts)\n",
    "\n",
    "**Respuesta:** \n",
    "\n",
    "La probabilidad estimada por el modelo MEMM viene dada por:\n",
    "\\begin{equation}\n",
    "P(s_{i} | s_{i-1}, x_{1}, ..., x_{m} ; \\vec{W}) = \\frac{exp(\\vec{W} \\cdot \\vec{\\Phi}(x_{1}, ..., x_{m}, i, s_{i-1}, s_{i}))}{\\sum_{\\hat{s} \\in S} exp(\\vec{W} \\cdot \\vec{\\Phi}(x_{1}, ..., x_{m}, i, s_{i-1}, \\hat{s}))}\n",
    "\\end{equation}\n",
    "\n",
    "Por el otro lado, la probabilidad estimada por el modelo CRF viene dada por:\n",
    "\\begin{equation}\n",
    "P(s_{1:m} | x_{1:m} ; \\vec{W}) = \\frac{exp(\\vec{W} \\cdot \\vec{\\Phi}(x_{1:m}, s_{1:m}))}{\\sum_{\\hat{s_{1:m}} \\in S^{m}} exp(\\vec{W} \\cdot \\vec{\\Phi}(x_{1:m}, \\hat{s_{1:m}}))}\n",
    "\\end{equation}\n",
    "\n",
    "N√≥tese que el denominador de la expresi√≥n asociada al modelo MEMM es local (la suma presente solo considera una etiquetas *anteriror* a la etiqueta acutal). En cambio, el denominador de la expresi√≥n asociada al modelo CRF es global (la suma presente considera todas las posibles sequencias de etiquetas $S_{m}$). Dicha diferencia es la que permite al modelo CRF tomar decisiones globales.\n",
    "\n",
    "\n",
    "Los modelos HMMs no son capaces de tomar decisiones globales ya que realizan un supuesto *markoviano* de segundo orden.\n",
    "#### 2.5 Dado una secuencia de $x_1, ..., x_m$ ¬øCu√°ntas posibles secuencias de etiquetas se pueden generar para un conjunto de etiquetas $S$ con $|S|=k$ ? ¬øAnalizarlas todas ser√≠a computacionalmente tratable? (0.1 pts)\n",
    "\n",
    "**Respuesta:**\n",
    "\n",
    "Existen $k^{m}$ posibles secuencias dado una secuencia $x_1, ..., x_m$ y un conjunto de etiquetas $S$ con $|S|=k$. N√≥tese que el problema es de tiempo exponencial y, por tanto, analizar todas las posibles secuencias de etiquetas no es tratable computacionalmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44ACHHZIWGF1"
   },
   "source": [
    "# Convolutional Neural Networks (0,5 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClRAHR95Y8aB"
   },
   "source": [
    "### Pregunta 3 (0,5 puntos)\n",
    "\n",
    "Considere la frase $w_{1..7}=$ `El agua moja y el fuego quema` $=[El, agua, moja, y, el, fuego, quema]$.\n",
    "\n",
    "La siguiente matriz de embeddings, donde la i-√©sima fila corresponde al vector de embedding de la i-√©sima palabra, ordenadas seg√∫n aparecen en la frase. (vectores de largo 2).\n",
    "\\begin{equation}\n",
    "E = \\begin{pmatrix}\n",
    "2 & 2\\\\\n",
    "0 & -2\\\\\n",
    "0 & 1\\\\\n",
    "-2 & 1\\\\\n",
    "1 & 0\\\\\n",
    "-1 & 1\\\\\n",
    "1 & 1\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Los siguientes 3 filtros\n",
    "\\begin{equation}\n",
    "U = \\begin{pmatrix}\n",
    "-1 & 1 & 0\\\\\n",
    "1 & 1 & 0\\\\\n",
    "0 & 0 & -1\\\\\n",
    "1 & -1 & -1\\\\\n",
    "-1 & -1 & 1\\\\\n",
    "1 & 0 & -1\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Y la funci√≥n de activaci√≥n\n",
    "\\begin{equation}\n",
    "tanh = \\frac{e^{2x} - 1}{e^{2x} + 1}\n",
    "\\end{equation}\n",
    "\n",
    "Usando estos param√°tros escriba los pasos para calcular la representaci√≥n (vector) resultante de aplicar la operaci√≥n de convoluci√≥n (sin padding) + max pooling. ¬øDe qu√© tama√±o ser√≠a la ventana que debemos usar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlQ30Arkq0u4"
   },
   "source": [
    "**Respuesta**\n",
    "\n",
    "- Dada la matriz $E$, se tiene que crear los vectores embedding $\\vec{v}$ para las siguientes frases:\n",
    " - el agua moja\n",
    " - agua moja y\n",
    " - moja y el \n",
    " - y el fuego\n",
    " - el fuego quema\n",
    "- Por ejemplo, para la frase `el agua moja`, su vector embedding asociado se crea concatenando los embedding para las palabras: `el`: $(2, 2)$, `agua`: $(0, -2)$ y `moja`: $(0, 1)$. As√≠, se tiene que el vector embedding $\\vec{v}$ para esta frase es: $(2, 2, 0, -2, 0, 1)^{T}$\n",
    "- Con los vectores embedding creados, para cada vector se debe multiplicar dicho vector por la matriz $U$ y luego aplicar la funci√≥n de activaci√≥n, es decir, se debe realizar la siguiente operacion:\n",
    "\\begin{equation}\n",
    "\\vec{p} = tanh(\\vec{v}^{T} \\cdot U)\n",
    "\\end{equation}\n",
    "- Luego, con todos los vectores $\\vec{p}$ asociados a cada frase, se debe elegir el vector m√°ximo, es decir, para cada posici√≥n (dimensi√≥n) de los vectores $\\vec{p}$, se debe escoger valor m√°ximo de todos los vectores $\\vec{p}$ para la posici√≥n (dimensi√≥n) mencionada.\n",
    "\n",
    "N√≥tese que, en este caso, el tama√±o de la ventana a usar a 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0rCwen3WREC"
   },
   "source": [
    "# Recurrent Neural Networks (1 punto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0et78Z4oKIq"
   },
   "source": [
    "### Pregunta 4 (0,5 puntos)\n",
    "Usando los embeddings de dos dimensiones de la pregunta anteror, la oraci√≥n `el fuego quema` la podemos representar por una secuencia de vectores $(\\vec{x}_1,\\vec{x}_2,\\vec{x}_3)$, con $\\vec{x}_i \\in \\mathbb{R}^{d_x}$ y $d_x=2$.\n",
    "\n",
    "Tenemos una red recurrente *Elman* definidad como: \n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\vec{s}_i &= R_{SRNN}\\left (\\vec{x}_i, \\vec{s}_{i-1}\\right ) = g \\left (\\vec{s}_{i-1}W^s + \\vec{x}_i W^x + \\vec{b}\\right ) \\\\\n",
    "\\vec{y}_i &= O_{SRNN}\\left(\\vec{s}_i\\right) = \\vec{s}_i \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "donde\n",
    "\\begin{equation}\n",
    "\\vec{s}_i, \\vec{y}_i \\in \\mathbb{R}^{d_s}, \\quad W^x \\in \\mathbb{R}^{d_x \\times d_s}, \\quad W^s \\in \\mathbb{R}^{d_s \\times d_s}, \\quad \\vec{b} \\in \\mathbb{R}^{d_s},\n",
    "\\end{equation}\n",
    "y los vectores de estado $s_i$ son de tres dimensiones, $ds= 3$.\n",
    "\n",
    "Sea\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\vec{s}_0 &= [0,0,0]\\\\\n",
    "W^x &= \\begin{pmatrix}\n",
    "0 &  0 & 1\\\\\n",
    "1 & -1 & 0\n",
    "\\end{pmatrix} \\\\\n",
    "W^s &= \\begin{pmatrix}\n",
    "1 & 0 &  1\\\\\n",
    "0 & 1 & -1\\\\\n",
    "1 & 1 &  1\n",
    "\\end{pmatrix} \\\\\n",
    "\\vec{b} &= [0, 0, 0] \\\\\n",
    "g(x) &= ReLu(x) = max(0, x)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "Calcule manualmente los valores de los vectores $\\vec{s}_1, \\vec{s}_2,\\vec{s}_3$ y de $\\vec{y}_1, \\vec{y}_2,\\vec{y}_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fim2W8JioPhL"
   },
   "source": [
    "**Respuesta**\n",
    "\n",
    "El embedding para la frase `el fuego quema` es $(1, 0, -1, 1, 1, 1)$, es decir, se tiene que $\\vec{x}_1 = (1, 0)$, $\\vec{x}_2 = (-1, 1)$ y $\\vec{x}_3 = (1, 1)$. Luego, se tiene que los vectores $\\vec{s}_1, \\vec{s}_2,\\vec{s}_3$ vienen dados por:\n",
    "- $\\vec{s}_1 = g(\\vec{s}_0 W^{s} + \\vec{x}_{1}W^{x} + \\vec{b}) = g((0,0,0) + (0, 0, 1)) = g((0,0,1))= (0,0,1) $\n",
    "- $\\vec{s}_2 = g(\\vec{s}_1 W^{s} + \\vec{x}_{2}W^{x} + \\vec{b}) = g((1,1,1) + (1,-1,-1)) = g((2,0,0)) = (2,0,0)$\n",
    "- $\\vec{s}_3 = g(\\vec{s}_2 W^{s} + \\vec{x}_{3}W^{x} + \\vec{b}) = g((2,0,2) + (1,-1,1)) = g((2,-1,2)) = (2,0,2)$\n",
    "\n",
    "Ahora, los vectores $\\vec{y}_1, \\vec{y}_2,\\vec{y}_3$ vienen dados por:\n",
    "- $\\vec{y}_1 = O_{SRNN}\\left(\\vec{s}_1\\right) = \\vec{s}_1 = (0,0,1)$\n",
    "- $\\vec{y}_2 = O_{SRNN}\\left(\\vec{s}_2\\right) = \\vec{s}_2 = (2,0,0)$\n",
    "- $\\vec{y}_3 = O_{SRNN}\\left(\\vec{s}_3\\right) = \\vec{s}_3 = (2,0,2)$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4rAT6ELxRZW"
   },
   "source": [
    "### Pregunta 5 (0.5 puntos)\n",
    "¬øDe qu√© forma las RNN y las CNN logran aprender representaciones espec√≠ficas\n",
    "para la tarea objetivo? Compare la forma en que las RNN y las CNN aprenden con los modelos que usan *features* dise√±adas manualmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6AXbQSgA_t8"
   },
   "source": [
    "**Respuesta**\n",
    "\n",
    "`Escriba su respuesta aqu√≠`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRJkBpjWyHnb"
   },
   "source": [
    "# Pregunta 6: Redes Neuronales con Pytorch (3 puntos) üí¨\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.anda.cl/wp-content/uploads/2021/03/0_5vNAtimPjYQr4W72.gif\" alt=\"chatbot\" width=\"400\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEla92bUymrQ"
   },
   "source": [
    "En esta secci√≥n de la tarea deber√°n implementar un Chatbot que sea capaz de generar una conversaci√≥n *‚Äúb√°sica‚Äù* utilizando un dataset de *Star Wars*. **El objetivo** de esta pregunta es que puedan aplicar lo aprendido sobre redes neuronales utilizando Pytorch en un ejemplo pr√°ctico.  Durante el desarrollo, se espera que puedan dise√±ar un bot (que tendr√° por atr√°s un clasificador) que sea capaz de clasificar diferentes etiquetas, cosa que una vez identificada la etiqueta entregue una respuesta acorde a lo preguntado.\n",
    "\n",
    "**Aviso:** Antes de comenzar con una descripci√≥n mas profunda de esta secci√≥n, les recomendamos que visualicen y se familiaricen con el dataset entregado, de esta forma comprender√°n mejor la descripci√≥n del enunciado (aqu√≠ una peque√±a ayudita üÜò)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0eKOGlMs3Dx-",
    "outputId": "376ced9d-b74d-499a-feb1-67e9574e77ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tags:  16\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "example_data = pd.read_json('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/star_wars_chatbot.json')\n",
    "print(\"Cantidad de tags: \", example_data['intents'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-6fCE5fHkNS"
   },
   "source": [
    "A continuaci√≥n, ejemplos del contenido del primer registro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axsi27BpHGOx",
    "outputId": "883da5bd-7f10-4a3c-da8d-6e50357413ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Hey',\n",
       " 'How are you',\n",
       " 'Is anyone there?',\n",
       " 'Hello',\n",
       " 'Good day',\n",
       " \"What's up\",\n",
       " 'Yo!',\n",
       " 'Howdy',\n",
       " 'Nice to meet you.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data['intents'][0]['patterns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OV0vGdwoHeg3",
    "outputId": "327fc95d-7dd8-4fa2-a410-1a2225661bcb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey',\n",
       " 'Hello, thanks for visiting.',\n",
       " 'Hi there, what can I do for you?',\n",
       " 'Hi there, how can I help?',\n",
       " 'Hello, there.',\n",
       " 'Hello Dear',\n",
       " 'Ooooo Hello, looking for someone or something?',\n",
       " 'Yes, I am here.',\n",
       " 'Listening carefully.',\n",
       " 'Ok, I am with you.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data['intents'][0]['responses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "0BnYez1oGtx3",
    "outputId": "dd0e649f-cdad-4f57-f848-384c688d38fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'greeting'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data['intents'][0]['tag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6BvAWCw3zPM"
   },
   "source": [
    "Del dataset cargado podemos notar que este viene en un formato `JSON`, por lo que sus datos est√°n almacenados en diccionarios. Las llaves de los diccionarios no son aleatorias y estos nos sirven para identificar puntos relevantes en el desarrollo del bot. A continuaci√≥n, se realiza una peque√±a descripci√≥n de las llaves:\n",
    "\n",
    "- `patterns`: Almacena los patrones con los que entrenaremos el modelo üòÆ, en otras palabras, es el corpus de entrenamiento que contiene solo preguntas o expresiones que deber√° responder el bot.\n",
    "- `responses`: Son las respuestas üôã relacionadas a los `patterns`, estas las utilizaremos en una etapa posterior a la clasificaci√≥n, para dar una respuesta aleator√≠a al usuario.\n",
    "- `tag`: Son las labels con las que entrenaremos nuestro modelo üíª. \n",
    "\n",
    "En s√≠ntesis, las `keys` relevantes para el entrenamiento de nuestra red neuronal ser√°n `patterns` (corpus) y `tag` (etiquetas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlOAdMjSSzNN"
   },
   "source": [
    "#### Explicaci√≥n de la tarea a realizar:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yGApnWVI4cO"
   },
   "source": [
    "**Explicaci√≥n de la tarea a realizar**: Implemente una Class llamada `CNNClassifier` que sea capaz de entrenar un modelo de texto a trav√©s de una red neuronal Feed Forward y una arquitectura convolucional (CNN 1D) [`torch.nn.Conv1d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#conv1d) . Para el dise√±o de las redes tienen completa libertad, pero se le aconseja que se gu√≠en de la √∫ltima auxiliar para la construcci√≥n. Es **important√≠simo** que el modelo a crear posea una capa de `Embedding` que se genere en base al entrenamiento del modelo. Creado el modelo, construya una funci√≥n batch para cargar los datos de entrenamiento del modelo.\n",
    "\n",
    "Construido el modelo, compare los resultados obtenidos para una red feed forward y una cnn. Para la comprobaci√≥n de sus resultados ejecute el chatbot y pruebelo, ¬øqu√© configuraci√≥n tiene mejores resultados?, ¬øa qu√© se deberan estos resultados?\n",
    "\n",
    "Ojo que un ejemplo de prueba con el chatbot puede ser (agregue mas preguntas ud):\n",
    "\n",
    "```\n",
    "Let's chat! (type 'finish_chat' to finish the chat)\n",
    "You: hi\n",
    "GA-97: Yes, I am here.\n",
    "You: can you tell me a joke?\n",
    "GA-97: Have you tried the gluten-free Wookiee treats? No, but I heard they are a little Chewy.\n",
    "```\n",
    "\n",
    "El resto del c√≥digo referido a la ejecuci√≥n del chat se los entregamos, por lo que no deber√≠an tener mayores problemas üò∏ (en caso de tener problemas con su c√≥digo, puede modificar cualquier parte sugerida siempre y cuando cumpla lo solicitado).\n",
    "\n",
    "**Igual [mucho texto](https://i0.wp.com/elgeneracionalpost.com/wp-content/uploads/2020/07/mucho-texto.jpg?fit=1280%2C720&ssl=1).... En resumen, ¬øQu√© se solicita?:**\n",
    "\n",
    "- [ ] Dise√±ar una red neuronal Feed Forward.\n",
    "- [ ] Dise√±ar un red convolucional.\n",
    "- [ ] Utilizar una capa de embeddings para generar representaciones vectoriales del corpus.\n",
    "- [ ] Crear el m√©todo forward de la clase `CNNClassifier`.\n",
    "- [ ] Crear la funci√≥n BATCH.\n",
    "- [ ] Probar el modelo y comparar los resultados obtenidos con la red Feed Forward y la red CNN. Comente sus resultados de forma cualitativa, se√±alando con qu√© tipo de red obtuvo mejores resultados con el chatbot.\n",
    "\n",
    "**Nota-1:** El modelo creado debe tener la opci√≥n de entrenar a traves de una feed forward y una CNN. Esto no significa que entrenar√° una FF y una CN, el modelo deber√° recibir un booleano que especifique que tipo de red utilizar√°.\n",
    "\n",
    "**Nota-2:** El dataset se descargar√° autom√°ticamente en la secci√≥n `Carga de Dataset üìö`, no os preocup√©is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4bKfAdEy3oD"
   },
   "source": [
    "#### Pasemos al C√≥digo ü¶æ\n",
    "\n",
    "Esqueleto propuesto (se **RECOMIENDA** que cambien **SOLO** la red neuronal y la funci√≥n Batch) ü¶¥:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUwxivx2MpMV"
   },
   "source": [
    "##### Instalamos librerias necesarias e importamos üòÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TjSZkBsk1H4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.8.0+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.0%2Bcu111-cp38-cp38-linux_x86_64.whl (1982.2 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1855.4 MB 8.4 MB/s eta 0:00:168  |‚ñå                               | 33.0 MB 6.5 MB/s eta 0:05:02     |‚ñà‚ñã                              | 99.9 MB 3.7 MB/s eta 0:08:23     |‚ñà‚ñà‚ñé                             | 140.4 MB 1.1 MB/s eta 0:28:06     |‚ñà‚ñà‚ñâ                             | 174.9 MB 6.1 MB/s eta 0:04:57     |‚ñà‚ñà‚ñà‚ñé                            | 203.4 MB 6.9 MB/s eta 0:04:17     |‚ñà‚ñà‚ñà‚ñç                            | 213.0 MB 4.1 MB/s eta 0:07:08     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 334.8 MB 4.1 MB/s eta 0:06:45     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 337.2 MB 5.6 MB/s eta 0:04:56     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 350.2 MB 6.8 MB/s eta 0:04:01     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 361.1 MB 6.3 MB/s eta 0:04:20     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 361.6 MB 6.3 MB/s eta 0:04:19     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 388.8 MB 6.8 MB/s eta 0:03:55     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 409.5 MB 3.0 MB/s eta 0:08:47     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 580.6 MB 2.9 MB/s eta 0:08:10     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 588.6 MB 473 kB/s eta 0:49:05     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 589.9 MB 473 kB/s eta 0:49:02     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 639.2 MB 7.4 MB/s eta 0:03:02     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 662.4 MB 6.8 MB/s eta 0:03:15     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 707.5 MB 5.1 MB/s eta 0:04:12     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 708.3 MB 5.4 MB/s eta 0:03:55     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 731.1 MB 3.2 MB/s eta 0:06:32     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 734.5 MB 2.5 MB/s eta 0:08:26     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 738.0 MB 2.5 MB/s eta 0:08:24     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 775.5 MB 6.1 MB/s eta 0:03:20     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 916.2 MB 4.3 MB/s eta 0:04:07     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 921.6 MB 4.0 MB/s eta 0:04:25     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 1028.1 MB 3.3 MB/s eta 0:04:52     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 1032.5 MB 6.5 MB/s eta 0:02:27     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 1052.7 MB 1.3 MB/s eta 0:11:57     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 1053.6 MB 1.3 MB/s eta 0:11:56     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 1058.8 MB 2.8 MB/s eta 0:05:35     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 1107.6 MB 5.8 MB/s eta 0:02:30     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 1167.5 MB 15.2 MB/s eta 0:00:54     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 1169.4 MB 8.4 MB/s eta 0:01:37     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 1322.3 MB 7.9 MB/s eta 0:01:24     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 1410.7 MB 4.5 MB/s eta 0:02:09     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 1433.9 MB 2.2 MB/s eta 0:04:09     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 1466.8 MB 5.0 MB/s eta 0:01:44     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 1519.1 MB 6.9 MB/s eta 0:01:08     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 1576.1 MB 3.7 MB/s eta 0:01:51     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 1576.8 MB 3.7 MB/s eta 0:01:51     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1661.9 MB 2.9 MB/s eta 0:01:52"
     ]
    }
   ],
   "source": [
    "# Esto toma su tiempo en ejecutarse\n",
    "!pip install torch==1.8.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install torchtext==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RfZ6SL-Q1Kwd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tridimensional/u/dcc/noveno/nlp/nlp/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "from random import choice\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from itertools import zip_longest\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Habemus GPU? False\n"
     ]
    }
   ],
   "source": [
    "# Verificar si cuda esta disponible en el entorno\n",
    "print(\"Habemus GPU?\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available(): # Usar esto para codigo agnostico\n",
    "    print(\"Cuantas GPUs me regala Google?\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oj-Epe7XJLrL"
   },
   "source": [
    "##### Carga de Dataset üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvlLqYRrVN6l",
    "outputId": "e6817cb7-ac85-471a-f426-f07537f357b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-06-20 02:52:32--  https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/star_wars_chatbot.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14469 (14K) [text/plain]\n",
      "Saving to: ‚Äòstar_wars_chatbot.json‚Äô\n",
      "\n",
      "star_wars_chatbot.j 100%[===================>]  14,13K  --.-KB/s    in 0,005s  \n",
      "\n",
      "2022-06-20 02:52:32 (2,65 MB/s) - ‚Äòstar_wars_chatbot.json‚Äô saved [14469/14469]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we obtain the dataset\n",
    "!wget 'https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/star_wars_chatbot.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MbbIsFUG1TXW",
    "outputId": "7ad12487-4295-4f5f-a3b2-c9e50729f680"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97lines [00:00, 42583.99lines/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset using json\n",
    "with open('star_wars_chatbot.json', 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Create a vocab with the dataset and get the number of classes that have\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "vocab = build_vocab_from_iterator(tokenizer(x) for list_words in dataset['intents'] for x in list_words['patterns'])\n",
    "num_classes = len(dataset['intents'])\n",
    "\n",
    "# Define a list with the labels\n",
    "labels = sorted(set([tag for tag in [intents['tag'] for intents in dataset['intents']]]))\n",
    "# Define a train_list where we can find the info in the format: [(tag_0, text_0)...,(tag_n-1, text_n-1)]\n",
    "train_list = [(labels.index(intents['tag']), text) for intents in dataset['intents'] for text in intents['patterns']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a52SUNKPJQxi"
   },
   "source": [
    "##### Creaci√≥n del modelo (2 puntos en total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "n-vQ24tMJG5H"
   },
   "outputs": [],
   "source": [
    "# Construya el modelo\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=32, num_classes=10, \n",
    "                 use_cnn=False, cnn_pool_channels=24, cnn_kernel_size=3):\n",
    "      super().__init__()\n",
    "    \n",
    "      self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "      self.use_cnn = use_cnn\n",
    "\n",
    "      if self.use_cnn:\n",
    "        # capa de convoluci√≥n\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=1,\n",
    "            out_channels=cnn_pool_channels,\n",
    "            kernel_size=cnn_kernel_size * embed_dim,\n",
    "            stride=embed_dim,\n",
    "            )\n",
    "        fc_in_size = cnn_pool_channels\n",
    "        \n",
    "        # capa lineal\n",
    "        self.fc = nn.Linear(fc_in_size, num_classes)\n",
    "        self.init_weights()\n",
    "      else:\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def init_weights(self):\n",
    "      # Esto puede ser util para inicializar los pesos\n",
    "      initrange = 0.5\n",
    "      self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "      self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "      self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "      text = torch.tensor(\n",
    "          list(\n",
    "              zip(\n",
    "                  *zip_longest(\n",
    "                      *([text[o:offsets[i+1]] for i, o in enumerate(offsets[:-1])] + [text[offsets[-1]:len(texts)]]), \n",
    "                      fillvalue=vocab[\"<pad>\"]\n",
    "                  )\n",
    "              )\n",
    "          )\n",
    "      ).to(text.device)\n",
    "\n",
    "      h = self.embedding(text)\n",
    "      if self.use_cnn:\n",
    "        # (N x longest_text x embed_dim)\n",
    "\n",
    "\n",
    "        # (N x pool_channels)\n",
    "        h = h.view(h.size(0), 1, -1)\n",
    "        h = torch.relu(self.conv(h))\n",
    "        h = h.mean(dim=2)\n",
    "\n",
    "        # (N x num_classes)\n",
    "        return self.fc(h)\n",
    "      else:\n",
    "        h = h.mean(dim=1)\n",
    "        return self.fc(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGN-T0JoJtmS"
   },
   "source": [
    "##### Funci√≥n Batch üë∑ (0,5 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "K1AZpXc7JxTa"
   },
   "outputs": [],
   "source": [
    "# Defina su funci√≥n de BATCH\n",
    "def generate_batch(batch):\n",
    "  label = torch.tensor([entry[0] for entry in batch])\n",
    "  texts = [tokenizer(entry[1]) for entry in batch]\n",
    "  offsets = [0] + [len(text) for text in texts]\n",
    "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "  big_text = torch.cat([torch.tensor([vocab.stoi[t] for t in text]) for text in texts])\n",
    "  return big_text, offsets, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YChwpNrrNRBe"
   },
   "source": [
    "##### Entrenamiento ü•ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5eRWRD_J0Km",
    "outputId": "89f923fd-8f1d-4258-ad74-4e5915c3d53a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is avaible: cpu\n",
      "train: 97 elements\n",
      "Epoch: 1000 \t iter-Loss: 0.001final loss: 0.0012\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"GPU is avaible: {device}\")\n",
    "\n",
    "# Define the different inputs in our model\n",
    "num_epochs = 1000\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-1\n",
    "INPUT_SIZE = len(vocab)\n",
    "OUTPUT_SIZE = num_classes\n",
    "USE_CNN = False\n",
    "\n",
    "# Define model, optimizer, loss and scheduler (Q: ¬øWhat is it?)\n",
    "model = CNNClassifier(INPUT_SIZE, num_classes=OUTPUT_SIZE, use_cnn=USE_CNN).to(device)\n",
    "optimizer = SGD(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda epoch: .9 ** (epoch // 10)])\n",
    "\n",
    "print(f'train: {len(train_list)} elements')\n",
    "\n",
    "# We train the model using the intents\n",
    "loss_list= []\n",
    "for epoch in range(1, num_epochs):\n",
    "  train_loader = DataLoader(train_list, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  for i, (texts, offsets, cls) in enumerate(train_loader):\n",
    "    texts = texts.to(device)\n",
    "    offsets = offsets.to(device)\n",
    "    cls = cls.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(texts, offsets)\n",
    "    loss = criterion(output, cls)\n",
    "    total_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  loss_list.append(loss.item())\n",
    "  sys.stdout.write('\\rEpoch: {0:03d} \\t iter-Loss: {1:.3f}'.format(epoch+1, loss.item()))\n",
    "\n",
    "print(f'final loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dlS4_X-L3DN"
   },
   "source": [
    "##### A probar! üß™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "6IhhAKFXL3eH",
    "outputId": "6a28628b-d2d0-4bf0-9479-ccabbef085e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'funny'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is working?, Try the next example!\n",
    "qText = \"'Do you know any joke?'\" # this must classify the label \"funny\"\n",
    "\n",
    "X = torch.tensor([vocab.stoi[t] for t in tokenizer(qText)]).to(device)\n",
    "\n",
    "model.eval()\n",
    "output = model(X, torch.tensor([0], dtype=torch.long).to(device))\n",
    "_, predicted = torch.max(output, dim=1)\n",
    "labels[predicted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udemze3zL549"
   },
   "source": [
    "Ya pero prometiste hacer un chatbot, no una simple clasificaci√≥n.... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpSYGx2tL0tC"
   },
   "source": [
    "##### Guardamos modelo ü¶∫ (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZBC4TyiqLzDv",
    "outputId": "0a743164-69aa-4b6e-a219-37da8ea699bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete. file saved to data.pth\n"
     ]
    }
   ],
   "source": [
    "# We save de model using pytorch (this is optional, just to learn how to do this in pytorch)\n",
    "data = {\n",
    "\"model_state\": model.state_dict(),\n",
    "\"input_size\": INPUT_SIZE,\n",
    "\"output_size\": OUTPUT_SIZE,\n",
    "\"use_cnn\": USE_CNN,\n",
    "\"labels\": labels\n",
    "        }\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYClbTtsMCjE"
   },
   "source": [
    "##### Chatbot üí¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c249zUwiMBxb",
    "outputId": "03d5c400-9860-4ea1-b8c6-f711067d2727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's chat! (type 'finish_chat' to finish the chat)\n",
      "You: Do you know any joke?\n",
      "GA-97: What do you need to reroute droids? R2-Detour.\n",
      "You: Do you know any joke?\n",
      "GA-97: What‚Äôs Yoda‚Äôs advice for going to the bathroom? Doo-doo or doo-doo-not-do.\n",
      "You: Do you know any joke?\n",
      "GA-97: Which Jedi became a rock star? Bon Jovi-Wan Kenobi.\n",
      "You: Do you know any joke?\n",
      "GA-97: What‚Äôs Yoda‚Äôs advice for going to the bathroom? Doo-doo or doo-doo-not-do.\n",
      "You: Do you know any joke?\n",
      "GA-97: How does Darth Vader like his toast? On the Dark Side.\n",
      "You: Do you know any joke?\n",
      "GA-97: What did Han Solo say to the waiter who recommended the haddock?, Never sell me the cods!\n",
      "You: Do you know any joke?\n",
      "GA-97: Why was the droid angry? Because people kept pushing its buttons.\n",
      "You: Do you know any joke?\n",
      "GA-97: Which program do Jedi use to open PDF files? Adobe-Wan Kenobi\n",
      "You: Do you know any joke?\n",
      "GA-97: What do you call a rebel princess who only shops at Whole Foods? Leia Organic.\n",
      "You: Do you know any joke?\n",
      "GA-97: Which Jedi became a rock star? Bon Jovi-Wan Kenobi.\n",
      "You: Do you know any joke?\n",
      "GA-97: Which program do Jedi use to open PDF files? Adobe-Wan Kenobi\n",
      "You: Do you know any joke?\n",
      "GA-97: What did Han Solo say to the waiter who recommended the haddock?, Never sell me the cods!\n",
      "You: finish_chat\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open('star_wars_chatbot.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "INPUT_SIZE = data[\"input_size\"]\n",
    "OUTPUT_SIZE = data[\"output_size\"]\n",
    "USE_CNN = data[\"use_cnn\"]\n",
    "labels = data['labels']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "model = CNNClassifier(INPUT_SIZE, num_classes=OUTPUT_SIZE, use_cnn=USE_CNN).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "\n",
    "# Dictionary with the answers\n",
    "responses = {key['tag']: key['responses'] for key in dataset['intents']}\n",
    "\n",
    "bot_name = \"GA-97\"\n",
    "print(\"Let's chat! (type 'finish_chat' to finish the chat)\")\n",
    "while True:\n",
    "    q_text = input(\"You: \")\n",
    "    q_text = q_text\n",
    "    if q_text == 'finish_chat':\n",
    "        break\n",
    "\n",
    "    X = torch.tensor([vocab.stoi[t] for t in tokenizer(q_text)]).to(device)\n",
    "    output = model(X, torch.tensor([0], dtype=torch.long).to(device))\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = labels[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "    if prob.item() > 0.50:\n",
    "      print(f\"{bot_name}: {random.choice(responses[tag])}\")\n",
    "    else:\n",
    "      print(f\"{bot_name}: My model can't understand you...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Hu2QTuSURCt"
   },
   "source": [
    "#### Comente los resultados aqu√≠ (0,5 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdFV63WVUX32"
   },
   "source": [
    "``Comente los resultados aqu√≠``"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tarea 4 - enunciado.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
